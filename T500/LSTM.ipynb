{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cec3755-8638-4fe7-a881-1c31bf1482a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ✅ Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63841af8-2705-4fac-bb4d-6db74d9958bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset: [10000 10000]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Prepare Data\n",
    "df = pd.read_csv(\"IL_T500_cleaned.csv\")\n",
    "\n",
    "# Check and drop NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Separate features (X) and label (y)\n",
    "X = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Balance the dataset with RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "print(f\"Balanced dataset: {np.bincount(y_resampled)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab88326-a184-4f85-840b-527bf58e35b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence X_seq shape: (19971, 30, 19)\n",
      "Sequence y_seq shape: (19971,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create Sequences for LSTM\n",
    "timesteps = 30  # Adjusted sequence length\n",
    "\n",
    "def create_sequences(data, labels, seq_len=30):\n",
    "    seqs, labs = [], []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seqs.append(data[i : i + seq_len])\n",
    "        labs.append(labels[i + seq_len - 1])\n",
    "    return np.array(seqs), np.array(labs)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_resampled, seq_len=timesteps)\n",
    "\n",
    "print(\"Sequence X_seq shape:\", X_seq.shape)\n",
    "print(\"Sequence y_seq shape:\", y_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310f5ca2-0034-480c-99af-ee6359cfee89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (16643, 30, 19) X_test shape: (3328, 30, 19)\n",
      "y_train shape: (16643,) y_test shape: (3328,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: TimeSeriesSplit for Train-Test Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X_seq):\n",
    "    X_train, X_test = X_seq[train_index], X_seq[test_index]\n",
    "    y_train, y_test = y_seq[train_index], y_seq[test_index]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c2c25a-63db-4651-bc56-7bb3f0d1e4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Prepare Tensors and DataLoader\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32).to(device)\n",
    "y_test_tensor  = torch.tensor(y_test,  dtype=torch.long).to(device)\n",
    "\n",
    "batch_size = 32  # Reduced batch size for better gradient updates\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor,  y_test_tensor)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a658c76-a287-4620-b7bd-acc193f3232b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6: Define LSTM Model\n",
    "class DeeperLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.5):\n",
    "        super(DeeperLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        x = self.fc1(last_step)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "num_features = X.shape[1]\n",
    "model = DeeperLSTM(input_dim=num_features).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a53be502-b790-4a35-ad0f-dabcabcff04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define Weighted Loss and Optimizer\n",
    "class_weights = torch.tensor([0.5, 1.5]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)  # Added L2 regularization\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a86be2-5e47-42af-8262-cd41bc7418af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 0.2078 | Acc: 89.55%\n",
      "Epoch 2/100 | Loss: 0.1196 | Acc: 97.76%\n",
      "Epoch 3/100 | Loss: 0.1198 | Acc: 97.77%\n",
      "Epoch 4/100 | Loss: 0.1205 | Acc: 97.78%\n",
      "Epoch 5/100 | Loss: 0.1191 | Acc: 97.78%\n",
      "Epoch 6/100 | Loss: 0.1193 | Acc: 97.77%\n",
      "Epoch 7/100 | Loss: 0.1210 | Acc: 97.79%\n",
      "Epoch 8/100 | Loss: 0.1191 | Acc: 97.78%\n",
      "Epoch 9/100 | Loss: 0.1190 | Acc: 97.76%\n",
      "Epoch 10/100 | Loss: 0.1184 | Acc: 97.77%\n",
      "Epoch 11/100 | Loss: 0.1193 | Acc: 97.78%\n",
      "Epoch 12/100 | Loss: 0.1188 | Acc: 97.78%\n",
      "Epoch 13/100 | Loss: 0.1184 | Acc: 97.78%\n",
      "Epoch 14/100 | Loss: 0.1183 | Acc: 97.78%\n",
      "Epoch 15/100 | Loss: 0.1188 | Acc: 97.78%\n",
      "Epoch 16/100 | Loss: 0.1183 | Acc: 97.78%\n",
      "Epoch 17/100 | Loss: 0.1184 | Acc: 97.79%\n",
      "Epoch 18/100 | Loss: 0.1182 | Acc: 97.79%\n",
      "Epoch 19/100 | Loss: 0.1181 | Acc: 97.79%\n",
      "Epoch 20/100 | Loss: 0.1186 | Acc: 97.78%\n",
      "Epoch 21/100 | Loss: 0.1184 | Acc: 97.78%\n",
      "Epoch 22/100 | Loss: 0.1185 | Acc: 97.78%\n",
      "Epoch 23/100 | Loss: 0.1182 | Acc: 97.78%\n",
      "Epoch 24/100 | Loss: 0.1185 | Acc: 97.79%\n",
      "Epoch 25/100 | Loss: 0.1181 | Acc: 97.78%\n",
      "Epoch 26/100 | Loss: 0.1180 | Acc: 97.78%\n",
      "Epoch 27/100 | Loss: 0.1178 | Acc: 97.79%\n",
      "Epoch 28/100 | Loss: 0.1181 | Acc: 97.79%\n",
      "Epoch 29/100 | Loss: 0.1177 | Acc: 97.79%\n",
      "Epoch 30/100 | Loss: 0.1182 | Acc: 97.78%\n",
      "Epoch 31/100 | Loss: 0.1178 | Acc: 97.79%\n",
      "Epoch 32/100 | Loss: 0.1178 | Acc: 97.79%\n",
      "Epoch 33/100 | Loss: 0.1182 | Acc: 97.78%\n",
      "Epoch 34/100 | Loss: 0.1176 | Acc: 97.79%\n",
      "Epoch 35/100 | Loss: 0.1183 | Acc: 97.79%\n",
      "Epoch 36/100 | Loss: 0.1178 | Acc: 97.78%\n",
      "Epoch 37/100 | Loss: 0.1182 | Acc: 97.77%\n",
      "Epoch 38/100 | Loss: 0.1183 | Acc: 97.78%\n",
      "Epoch 39/100 | Loss: 0.1177 | Acc: 97.78%\n",
      "Epoch 40/100 | Loss: 0.1200 | Acc: 97.79%\n",
      "Epoch 41/100 | Loss: 0.1174 | Acc: 97.79%\n",
      "Epoch 42/100 | Loss: 0.1177 | Acc: 97.80%\n",
      "Epoch 43/100 | Loss: 0.1178 | Acc: 97.79%\n",
      "Epoch 44/100 | Loss: 0.1176 | Acc: 97.79%\n",
      "Epoch 45/100 | Loss: 0.1174 | Acc: 97.78%\n",
      "Epoch 46/100 | Loss: 0.1170 | Acc: 97.78%\n",
      "Epoch 47/100 | Loss: 0.1156 | Acc: 97.78%\n",
      "Epoch 48/100 | Loss: 0.1146 | Acc: 97.78%\n",
      "Epoch 49/100 | Loss: 0.1141 | Acc: 97.79%\n",
      "Epoch 50/100 | Loss: 0.1133 | Acc: 97.78%\n",
      "Epoch 51/100 | Loss: 0.1077 | Acc: 97.76%\n",
      "Epoch 52/100 | Loss: 0.1099 | Acc: 97.79%\n",
      "Epoch 53/100 | Loss: 0.1114 | Acc: 97.78%\n",
      "Epoch 54/100 | Loss: 0.1073 | Acc: 97.59%\n",
      "Epoch 55/100 | Loss: 0.1054 | Acc: 97.39%\n",
      "Epoch 56/100 | Loss: 0.1071 | Acc: 97.48%\n",
      "Epoch 57/100 | Loss: 0.1062 | Acc: 97.51%\n",
      "Epoch 58/100 | Loss: 0.1093 | Acc: 97.79%\n",
      "Epoch 59/100 | Loss: 0.1121 | Acc: 97.53%\n",
      "Epoch 60/100 | Loss: 0.1079 | Acc: 97.55%\n",
      "Epoch 61/100 | Loss: 0.1130 | Acc: 97.37%\n",
      "Epoch 62/100 | Loss: 0.1070 | Acc: 97.40%\n",
      "Epoch 63/100 | Loss: 0.1072 | Acc: 97.52%\n",
      "Epoch 64/100 | Loss: 0.1048 | Acc: 97.42%\n",
      "Epoch 65/100 | Loss: 0.1111 | Acc: 97.60%\n",
      "Epoch 66/100 | Loss: 0.1033 | Acc: 97.52%\n",
      "Epoch 67/100 | Loss: 0.1139 | Acc: 97.39%\n",
      "Epoch 68/100 | Loss: 0.1029 | Acc: 97.61%\n",
      "Epoch 69/100 | Loss: 0.1068 | Acc: 97.43%\n",
      "Epoch 70/100 | Loss: 0.1039 | Acc: 97.51%\n",
      "Epoch 71/100 | Loss: 0.1042 | Acc: 97.73%\n",
      "Epoch 72/100 | Loss: 0.1025 | Acc: 97.59%\n",
      "Epoch 73/100 | Loss: 0.1007 | Acc: 97.57%\n",
      "Epoch 74/100 | Loss: 0.1079 | Acc: 97.41%\n",
      "Epoch 75/100 | Loss: 0.0983 | Acc: 97.52%\n",
      "Epoch 76/100 | Loss: 0.0962 | Acc: 97.63%\n",
      "Epoch 77/100 | Loss: 0.1023 | Acc: 97.76%\n",
      "Epoch 78/100 | Loss: 0.0967 | Acc: 97.57%\n",
      "Epoch 79/100 | Loss: 0.1030 | Acc: 97.70%\n",
      "Epoch 80/100 | Loss: 0.1076 | Acc: 97.47%\n",
      "Epoch 81/100 | Loss: 0.1019 | Acc: 97.63%\n",
      "Epoch 82/100 | Loss: 0.1017 | Acc: 97.62%\n",
      "Epoch 83/100 | Loss: 0.1023 | Acc: 97.63%\n",
      "Epoch 84/100 | Loss: 0.1003 | Acc: 97.69%\n",
      "Epoch 85/100 | Loss: 0.0991 | Acc: 97.64%\n",
      "Epoch 86/100 | Loss: 0.1000 | Acc: 97.69%\n",
      "Epoch 87/100 | Loss: 0.0981 | Acc: 97.64%\n",
      "Epoch 88/100 | Loss: 0.0915 | Acc: 97.92%\n",
      "Epoch 89/100 | Loss: 0.0953 | Acc: 97.58%\n",
      "Epoch 90/100 | Loss: 0.1000 | Acc: 97.46%\n",
      "Epoch 91/100 | Loss: 0.1013 | Acc: 97.82%\n",
      "Epoch 92/100 | Loss: 0.0960 | Acc: 97.52%\n",
      "Epoch 93/100 | Loss: 0.1008 | Acc: 97.73%\n",
      "Epoch 94/100 | Loss: 0.1002 | Acc: 97.43%\n",
      "Epoch 95/100 | Loss: 0.0948 | Acc: 97.90%\n",
      "Epoch 96/100 | Loss: 0.0981 | Acc: 97.72%\n",
      "Epoch 97/100 | Loss: 0.0925 | Acc: 97.40%\n",
      "Epoch 98/100 | Loss: 0.0883 | Acc: 97.82%\n",
      "Epoch 99/100 | Loss: 0.0925 | Acc: 97.82%\n",
      "Epoch 100/100 | Loss: 0.0951 | Acc: 97.98%\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train the Model\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += yb.size(0)\n",
    "        correct += (predicted == yb).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc  = 100.0 * correct / total\n",
    "    print(f\"Epoch {epoch}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d326f31b-433c-43ff-b8d1-494b0b9e3fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Test Results:\n",
      "Accuracy:   100.00%\n",
      "Precision:  100.00%\n",
      "Recall:     100.00%\n",
      "F1-score:   100.00%\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Evaluate the Model\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        logits = model(Xb)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "acc  = accuracy_score(all_true, all_preds) * 100\n",
    "prec = precision_score(all_true, all_preds, pos_label=1, zero_division=0) * 100\n",
    "rec  = recall_score(all_true, all_preds, pos_label=1, zero_division=0) * 100\n",
    "f1   = f1_score(all_true, all_preds, pos_label=1, zero_division=0) * 100\n",
    "\n",
    "print(\"\\n✅ Final Test Results:\")\n",
    "print(f\"Accuracy:   {acc:.2f}%\")\n",
    "print(f\"Precision:  {prec:.2f}%\")\n",
    "print(f\"Recall:     {rec:.2f}%\")\n",
    "print(f\"F1-score:   {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c972e32d-9368-4c4d-b224-ea007c0788f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced and Augmented dataset: [10000 20000]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"IL_T500_cleaned.csv\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Balance the dataset with RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Data Augmentation for anomalies\n",
    "anomalies = X_resampled[y_resampled == 1]\n",
    "augmented_anomalies = anomalies + np.random.normal(0, 0.01, anomalies.shape)\n",
    "X_resampled = np.vstack((X_resampled, augmented_anomalies))\n",
    "y_resampled = np.hstack((y_resampled, np.ones(len(augmented_anomalies), dtype=int)))\n",
    "y_resampled = y_resampled.astype(int)  # Ensure all labels are integers\n",
    "\n",
    "print(f\"Balanced and Augmented dataset: {np.bincount(y_resampled)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a8ddaa-bb1b-4376-a31d-d919a70369a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "\n",
    "def create_sequences(data, labels, seq_len=30):\n",
    "    seqs, labs = [], []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seqs.append(data[i : i + seq_len])\n",
    "        labs.append(labels[i + seq_len - 1])\n",
    "    return np.array(seqs), np.array(labs)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_resampled, seq_len=timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdb3901a-f3fe-4afe-959c-fee1f2e1c04f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (24976, 30, 19) X_test shape: (4995, 30, 19)\n",
      "y_train shape: (24976,) y_test shape: (4995,)\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X_seq):\n",
    "    X_train, X_test = X_seq[train_index], X_seq[test_index]\n",
    "    y_train, y_test = y_seq[train_index], y_seq[test_index]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b5df70-9f6a-4e95-a86f-a7d066f3cc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32).to(device)\n",
    "y_test_tensor  = torch.tensor(y_test,  dtype=torch.long).to(device)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor,  y_test_tensor)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "805ac51f-71b1-4f09-a9e4-fd108115ec1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeeperLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.5):\n",
    "        super(DeeperLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        x = self.fc1(last_step)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "num_features = X.shape[1]\n",
    "model = DeeperLSTM(input_dim=num_features).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392c64fd-fd8f-44b3-8af0-fb6d6c976d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spuspa/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69b2f62-4b1b-4eeb-b31a-4d4a7180f82c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d770429-a99f-45bd-a951-9ac3c40193f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 0.0105 | Acc: 94.61%\n",
      "Epoch 2/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 3/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 4/100 | Loss: 0.0074 | Acc: 97.11%\n",
      "Epoch 5/100 | Loss: 0.0073 | Acc: 97.08%\n",
      "Epoch 6/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 7/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 8/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 9/100 | Loss: 0.0074 | Acc: 97.09%\n",
      "Epoch 10/100 | Loss: 0.0073 | Acc: 97.08%\n",
      "Epoch 11/100 | Loss: 0.0073 | Acc: 97.09%\n",
      "Epoch 12/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 13/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 14/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 15/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 16/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 17/100 | Loss: 0.0073 | Acc: 97.10%\n",
      "Epoch 18/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 19/100 | Loss: 0.0073 | Acc: 97.10%\n",
      "Epoch 20/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 21/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 22/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 23/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 24/100 | Loss: 0.0073 | Acc: 97.10%\n",
      "Epoch 25/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 26/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 27/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 28/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 29/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 30/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 31/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 32/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 33/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 34/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 35/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 36/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 37/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 38/100 | Loss: 0.0073 | Acc: 97.10%\n",
      "Epoch 39/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 40/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 41/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 42/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 43/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 44/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 45/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 46/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 47/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 48/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 49/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 50/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 51/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 52/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 53/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 54/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 55/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 56/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 57/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 58/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 59/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 60/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 61/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 62/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 63/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 64/100 | Loss: 0.0073 | Acc: 97.10%\n",
      "Epoch 65/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 66/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 67/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 68/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 69/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 70/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 71/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 72/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 73/100 | Loss: 0.0073 | Acc: 97.13%\n",
      "Epoch 74/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 75/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 76/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 77/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 78/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 79/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 80/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 81/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 82/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 83/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 84/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 85/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 86/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 87/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 88/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 89/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 90/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 91/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 92/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 93/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 94/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 95/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 96/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 97/100 | Loss: 0.0073 | Acc: 97.12%\n",
      "Epoch 98/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 99/100 | Loss: 0.0073 | Acc: 97.11%\n",
      "Epoch 100/100 | Loss: 0.0073 | Acc: 97.11%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += yb.size(0)\n",
    "        correct += (predicted == yb).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc  = 100.0 * correct / total\n",
    "    scheduler.step(epoch_loss)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b541e384-ff14-42c8-a840-14e58f11c77f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Test Results:\n",
      "Accuracy:   100.00%\n",
      "Precision:  100.00%\n",
      "Recall:     100.00%\n",
      "F1-score:   100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        logits = model(Xb)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "acc  = accuracy_score(all_true, all_preds) * 100\n",
    "prec = precision_score(all_true, all_preds, pos_label=1) * 100\n",
    "rec  = recall_score(all_true, all_preds, pos_label=1) * 100\n",
    "f1   = f1_score(all_true, all_preds, pos_label=1) * 100\n",
    "\n",
    "print(\"\\n✅ Final Test Results:\")\n",
    "print(f\"Accuracy:   {acc:.2f}%\")\n",
    "print(f\"Precision:  {prec:.2f}%\")\n",
    "print(f\"Recall:     {rec:.2f}%\")\n",
    "print(f\"F1-score:   {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537036a-bb35-4ad5-9160-7261e9a44337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
