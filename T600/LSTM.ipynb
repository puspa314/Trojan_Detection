{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e773eb7-4ea2-4389-8ab2-77fa17347825",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ✅ Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "125911bb-a4fa-4c60-80e8-58825b674152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count before fixing:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "\n",
      "NaN count after filling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv(\"IL_T600_cleaned.csv\")  # Replace with the correct filename if needed\n",
    "\n",
    "# Check the initial NaN count per column\n",
    "print(\"NaN count before fixing:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN in 'spectral_contrast' with the column mean (example approach)\n",
    "#mean_val = df['spectral_contrast'].mean()\n",
    "#df['spectral_contrast'].fillna(mean_val, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Double-check NaNs are resolved\n",
    "print(\"\\nNaN count after filling:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4e0f41-58f2-4173-9070-797426f12121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (19999, 19), Label shape: (19999,)\n",
      "Unique labels: [0 1]\n",
      "Sequence X_seq shape: (19980, 20, 19)\n",
      "Sequence y_seq shape: (19980,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and label (y)\n",
    "X = df.drop(columns=['label']).values  # all columns except 'label'\n",
    "y = df['label'].values                 # label column\n",
    "\n",
    "print(f\"Features shape: {X.shape}, Label shape: {y.shape}\")\n",
    "print(\"Unique labels:\", np.unique(y))\n",
    "\n",
    "# Scale features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define sequence length for LSTM\n",
    "timesteps = 20\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, labels, seq_len=5):\n",
    "    seqs, labs = [], []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        seqs.append(data[i : i + seq_len])\n",
    "        labs.append(labels[i + seq_len - 1])\n",
    "    return np.array(seqs), np.array(labs)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, seq_len=timesteps)\n",
    "\n",
    "print(\"Sequence X_seq shape:\", X_seq.shape)\n",
    "print(\"Sequence y_seq shape:\", y_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683f51ea-1df7-4a38-8d94-69d74ec6a2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (15984, 20, 19) X_test shape: (3996, 20, 19)\n",
      "y_train shape: (15984,) y_test shape: (3996,)\n",
      "Any NaNs in X_train_tensor? False\n",
      "Any NaNs in y_train_tensor? False\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_seq\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape, \"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32).to(device)\n",
    "y_test_tensor  = torch.tensor(y_test,  dtype=torch.long).to(device)\n",
    "\n",
    "# Check for any NaNs in final tensors\n",
    "print(\"Any NaNs in X_train_tensor?\", torch.isnan(X_train_tensor).any().item())\n",
    "print(\"Any NaNs in y_train_tensor?\", torch.isnan(y_train_tensor.float()).any().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e006382-8089-4baa-9b4b-548c62a6fb6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 250\n",
      "Test batches: 63\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor,  y_test_tensor)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d613e9-c881-44cb-95e0-8e5ba83a1cef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperLSTM(\n",
      "  (lstm): LSTM(19, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DeeperLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.3):\n",
    "        super(DeeperLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout  # dropout applied between LSTM layers (only if num_layers > 1)\n",
    "        )\n",
    "        # Optional: A simple MLP head for extra capacity\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, timesteps, features)\n",
    "        lstm_out, _ = self.lstm(x)      # shape: (batch, timesteps, hidden_dim)\n",
    "        last_step = lstm_out[:, -1, :]  # use the final timestep: shape (batch, hidden_dim)\n",
    "        x = self.fc1(last_step)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out  # raw logits\n",
    "\n",
    "num_features = X.shape[1]  # should be 19\n",
    "model = DeeperLSTM(input_dim=num_features, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.3).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c898f1-4df4-4b25-a39e-10fa3d7f9c5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)  # a relatively small LR\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d322b935-6268-45a1-9e2e-d62b285fd027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training started...\n",
      "Epoch 1/100 | Loss: 0.6969 | Acc: 49.95%\n",
      "Epoch 2/100 | Loss: 0.6943 | Acc: 49.95%\n",
      "Epoch 3/100 | Loss: 0.6930 | Acc: 49.95%\n",
      "Epoch 4/100 | Loss: 0.6926 | Acc: 51.32%\n",
      "Epoch 5/100 | Loss: 0.6925 | Acc: 53.65%\n",
      "Epoch 6/100 | Loss: 0.6924 | Acc: 51.95%\n",
      "Epoch 7/100 | Loss: 0.6922 | Acc: 53.15%\n",
      "Epoch 8/100 | Loss: 0.6919 | Acc: 51.65%\n",
      "Epoch 9/100 | Loss: 0.6915 | Acc: 51.68%\n",
      "Epoch 10/100 | Loss: 0.6909 | Acc: 57.58%\n",
      "Epoch 11/100 | Loss: 0.6897 | Acc: 51.56%\n",
      "Epoch 12/100 | Loss: 0.6862 | Acc: 54.64%\n",
      "Epoch 13/100 | Loss: 0.6687 | Acc: 58.35%\n",
      "Epoch 14/100 | Loss: 0.6236 | Acc: 66.70%\n",
      "Epoch 15/100 | Loss: 0.6093 | Acc: 67.57%\n",
      "Epoch 16/100 | Loss: 0.6031 | Acc: 67.63%\n",
      "Epoch 17/100 | Loss: 0.5995 | Acc: 68.36%\n",
      "Epoch 18/100 | Loss: 0.5916 | Acc: 68.86%\n",
      "Epoch 19/100 | Loss: 0.5884 | Acc: 68.99%\n",
      "Epoch 20/100 | Loss: 0.5868 | Acc: 69.58%\n",
      "Epoch 21/100 | Loss: 0.5812 | Acc: 69.54%\n",
      "Epoch 22/100 | Loss: 0.5780 | Acc: 69.87%\n",
      "Epoch 23/100 | Loss: 0.5768 | Acc: 69.96%\n",
      "Epoch 24/100 | Loss: 0.5720 | Acc: 70.35%\n",
      "Epoch 25/100 | Loss: 0.5707 | Acc: 70.69%\n",
      "Epoch 26/100 | Loss: 0.5672 | Acc: 70.88%\n",
      "Epoch 27/100 | Loss: 0.5648 | Acc: 71.10%\n",
      "Epoch 28/100 | Loss: 0.5607 | Acc: 71.16%\n",
      "Epoch 29/100 | Loss: 0.5589 | Acc: 71.77%\n",
      "Epoch 30/100 | Loss: 0.5566 | Acc: 71.44%\n",
      "Epoch 31/100 | Loss: 0.5542 | Acc: 71.82%\n",
      "Epoch 32/100 | Loss: 0.5508 | Acc: 71.82%\n",
      "Epoch 33/100 | Loss: 0.5503 | Acc: 72.13%\n",
      "Epoch 34/100 | Loss: 0.5440 | Acc: 72.77%\n",
      "Epoch 35/100 | Loss: 0.5452 | Acc: 72.46%\n",
      "Epoch 36/100 | Loss: 0.5417 | Acc: 72.69%\n",
      "Epoch 37/100 | Loss: 0.5391 | Acc: 73.02%\n",
      "Epoch 38/100 | Loss: 0.5415 | Acc: 72.84%\n",
      "Epoch 39/100 | Loss: 0.5361 | Acc: 73.30%\n",
      "Epoch 40/100 | Loss: 0.5336 | Acc: 73.24%\n",
      "Epoch 41/100 | Loss: 0.5324 | Acc: 73.77%\n",
      "Epoch 42/100 | Loss: 0.5320 | Acc: 73.67%\n",
      "Epoch 43/100 | Loss: 0.5305 | Acc: 73.45%\n",
      "Epoch 44/100 | Loss: 0.5257 | Acc: 74.27%\n",
      "Epoch 45/100 | Loss: 0.5254 | Acc: 73.91%\n",
      "Epoch 46/100 | Loss: 0.5265 | Acc: 73.89%\n",
      "Epoch 47/100 | Loss: 0.5242 | Acc: 74.15%\n",
      "Epoch 48/100 | Loss: 0.5237 | Acc: 74.26%\n",
      "Epoch 49/100 | Loss: 0.5208 | Acc: 74.44%\n",
      "Epoch 50/100 | Loss: 0.5217 | Acc: 74.44%\n",
      "Epoch 51/100 | Loss: 0.5190 | Acc: 74.54%\n",
      "Epoch 52/100 | Loss: 0.5182 | Acc: 74.63%\n",
      "Epoch 53/100 | Loss: 0.5172 | Acc: 74.51%\n",
      "Epoch 54/100 | Loss: 0.5135 | Acc: 74.81%\n",
      "Epoch 55/100 | Loss: 0.5160 | Acc: 74.93%\n",
      "Epoch 56/100 | Loss: 0.5143 | Acc: 75.10%\n",
      "Epoch 57/100 | Loss: 0.5113 | Acc: 75.09%\n",
      "Epoch 58/100 | Loss: 0.5121 | Acc: 75.38%\n",
      "Epoch 59/100 | Loss: 0.5113 | Acc: 75.24%\n",
      "Epoch 60/100 | Loss: 0.5097 | Acc: 75.23%\n",
      "Epoch 61/100 | Loss: 0.5101 | Acc: 75.29%\n",
      "Epoch 62/100 | Loss: 0.5081 | Acc: 75.51%\n",
      "Epoch 63/100 | Loss: 0.5079 | Acc: 75.57%\n",
      "Epoch 64/100 | Loss: 0.5087 | Acc: 75.27%\n",
      "Epoch 65/100 | Loss: 0.5055 | Acc: 75.74%\n",
      "Epoch 66/100 | Loss: 0.5075 | Acc: 75.29%\n",
      "Epoch 67/100 | Loss: 0.5054 | Acc: 75.74%\n",
      "Epoch 68/100 | Loss: 0.5050 | Acc: 75.88%\n",
      "Epoch 69/100 | Loss: 0.5059 | Acc: 75.39%\n",
      "Epoch 70/100 | Loss: 0.5050 | Acc: 75.69%\n",
      "Epoch 71/100 | Loss: 0.5050 | Acc: 75.52%\n",
      "Epoch 72/100 | Loss: 0.5017 | Acc: 75.85%\n",
      "Epoch 73/100 | Loss: 0.4997 | Acc: 76.19%\n",
      "Epoch 74/100 | Loss: 0.4996 | Acc: 75.89%\n",
      "Epoch 75/100 | Loss: 0.4984 | Acc: 76.28%\n",
      "Epoch 76/100 | Loss: 0.4989 | Acc: 76.24%\n",
      "Epoch 77/100 | Loss: 0.4986 | Acc: 76.06%\n",
      "Epoch 78/100 | Loss: 0.4977 | Acc: 76.27%\n",
      "Epoch 79/100 | Loss: 0.4995 | Acc: 76.13%\n",
      "Epoch 80/100 | Loss: 0.4962 | Acc: 76.16%\n",
      "Epoch 81/100 | Loss: 0.4939 | Acc: 76.46%\n",
      "Epoch 82/100 | Loss: 0.4988 | Acc: 76.11%\n",
      "Epoch 83/100 | Loss: 0.4967 | Acc: 76.20%\n",
      "Epoch 84/100 | Loss: 0.4943 | Acc: 76.35%\n",
      "Epoch 85/100 | Loss: 0.4963 | Acc: 76.38%\n",
      "Epoch 86/100 | Loss: 0.4990 | Acc: 76.14%\n",
      "Epoch 87/100 | Loss: 0.4944 | Acc: 76.31%\n",
      "Epoch 88/100 | Loss: 0.4924 | Acc: 76.55%\n",
      "Epoch 89/100 | Loss: 0.4906 | Acc: 76.73%\n",
      "Epoch 90/100 | Loss: 0.4898 | Acc: 76.78%\n",
      "Epoch 91/100 | Loss: 0.4929 | Acc: 76.31%\n",
      "Epoch 92/100 | Loss: 0.4911 | Acc: 76.53%\n",
      "Epoch 93/100 | Loss: 0.4910 | Acc: 76.68%\n",
      "Epoch 94/100 | Loss: 0.4916 | Acc: 76.73%\n",
      "Epoch 95/100 | Loss: 0.4893 | Acc: 76.59%\n",
      "Epoch 96/100 | Loss: 0.4893 | Acc: 76.47%\n",
      "Epoch 97/100 | Loss: 0.4838 | Acc: 77.16%\n",
      "Epoch 98/100 | Loss: 0.4869 | Acc: 76.88%\n",
      "Epoch 99/100 | Loss: 0.4877 | Acc: 77.01%\n",
      "Epoch 100/100 | Loss: 0.4854 | Acc: 76.94%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "print(\"\\nTraining started...\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(Xb)          # shape: (batch, 2)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += yb.size(0)\n",
    "        correct += (predicted == yb).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc  = 100.0 * correct / total\n",
    "    print(f\"Epoch {epoch}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f189bc-47c3-4505-87e0-b1d5b3f50a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Test Results:\n",
      "AUC:        84.10%\n",
      "Accuracy:   75.93%\n",
      "Precision:  78.83%\n",
      "Recall:     70.95%\n",
      "F1-score:   74.68%\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1615  381]\n",
      " [ 581 1419]]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 9) Evaluate on Test Set  +  Confusion Matrix  +  ROC-AUC\n",
    "# ---------------------------------------------------------------\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_true, all_scores = [], [], []          # ← new list for AUC scores\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        logits = model(Xb)                            # shape: [batch, 2]\n",
    "\n",
    "        # ── probability that each sample is class-1 (Trojan) ──\n",
    "        prob_pos = F.softmax(logits, dim=1)[:, 1]     # if model uses softmax\n",
    "        # prob_pos = torch.sigmoid(logits).squeeze()  # use this instead if model ends with a single-logit sigmoid layer\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "\n",
    "        all_scores.append(prob_pos.cpu().numpy())     # ← collect scores\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_scores = np.concatenate(all_scores)               # ← for roc_auc_score\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_true   = np.concatenate(all_true)\n",
    "\n",
    "# Metrics\n",
    "acc  = accuracy_score(all_true, all_preds)  * 100\n",
    "prec = precision_score(all_true, all_preds, zero_division=0) * 100\n",
    "rec  = recall_score(all_true, all_preds, zero_division=0) * 100\n",
    "f1   = f1_score(all_true, all_preds, zero_division=0) * 100\n",
    "auc  = roc_auc_score(all_true, all_scores) * 100       # ← NEW\n",
    "\n",
    "# Display\n",
    "print(\"\\n✅ Final Test Results:\")\n",
    "print(f\"AUC:        {auc:.2f}%\")\n",
    "print(f\"Accuracy:   {acc:.2f}%\")\n",
    "print(f\"Precision:  {prec:.2f}%\")\n",
    "print(f\"Recall:     {rec:.2f}%\")\n",
    "print(f\"F1-score:   {f1:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1363b5cd-9c8f-4f3e-a827-790048375d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
