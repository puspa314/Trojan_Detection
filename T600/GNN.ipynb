{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1020b613-49a3-4cf6-b81a-606e304d90a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch-geometric\n",
      "  Obtaining dependency information for torch-geometric from https://files.pythonhosted.org/packages/03/9f/157e913626c1acfb3b19ce000b1a6e4e4fb177c0bc0ea0c67ca5bd714b5a/torch_geometric-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-scatter\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch-spline-conv\n",
      "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.8.5)\n",
      "Requirement already satisfied: fsspec in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (2023.4.0)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (4.65.0)\n",
      "Requirement already satisfied: scipy in /home/spuspa/.local/lib/python3.11/site-packages (from torch-sparse) (1.14.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (2023.7.22)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
      "  Building wheel for torch-scatter (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[127 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/placeholder.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/scatter.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_coo.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_csr.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/testing.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/utils.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/logsumexp.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/softmax.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/std.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_scatter.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_scatter.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_scatter.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_scatter.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_scatter._scatter_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -DWITH_PYTHON -Icsrc -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/TH -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/THC -I/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include/python3.11 -c csrc/cpu/scatter_cpu.cpp -o build/temp.linux-x86_64-cpython-311/csrc/cpu/scatter_cpu.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_scatter_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/scatter_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/scatter_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      "  \u001b[31m   \u001b[0m  #error \\\n",
      "  \u001b[31m   \u001b[0m   ^~~~~\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/Backend.h:5,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/Layout.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:12,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/scatter_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/scatter_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cpu_ks = DispatchKeySet(DispatchKey::AutogradCPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradIPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_ipu_ks = DispatchKeySet(DispatchKey::AutogradIPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xpu_ks = DispatchKeySet(DispatchKey::AutogradXPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCUDA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cuda_ks = DispatchKeySet(DispatchKey::AutogradCUDA);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXLA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xla_ks = DispatchKeySet(DispatchKey::AutogradXLA);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradLazy)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_lazy_ks = DispatchKeySet(DispatchKey::AutogradLazy);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMeta)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_meta_ks = DispatchKeySet(DispatchKey::AutogradMeta);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMPS)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_mps_ks = DispatchKeySet(DispatchKey::AutogradMPS);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradHPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_hpu_ks = DispatchKeySet(DispatchKey::AutogradHPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse1)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse1);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse2)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse2);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse3)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse3);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-scatter\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-scatter\n",
      "  Building wheel for torch-sparse (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[136 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/add.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/bandwidth.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/cat.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/coalesce.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/convert.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/diag.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/eye.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/index_select.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/masked_select.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/matmul.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/metis.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/mul.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/narrow.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/permute.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/reduce.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/rw.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/saint.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/sample.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/select.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spadd.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spmm.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/spspmm.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/storage.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/tensor.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/testing.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/transpose.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/typing.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m copying torch_sparse/utils.py -> build/lib.linux-x86_64-cpython-311/torch_sparse\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_sparse.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_sparse.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_sparse.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_sparse.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/css'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/html'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/tests'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/examples'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'third_party/parallel-hashmap/benchmark'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'benchmark'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_sparse._convert_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -DWITH_PYTHON -Icsrc -I/local_scratch/slurm.2158609/pip-install-lvq64rsl/torch-sparse_52bbb233644b49a6bda6314e2de27aa4/third_party/parallel-hashmap -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/TH -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/THC -I/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include/python3.11 -c csrc/convert.cpp -o build/temp.linux-x86_64-cpython-311/csrc/convert.o -O3 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_convert_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/CallOnce.h:8,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Context.h:22,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/ATen.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/script.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/convert.cpp:4:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      "  \u001b[31m   \u001b[0m  #error \\\n",
      "  \u001b[31m   \u001b[0m   ^~~~~\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Generator.h:11,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/CPUGeneratorImpl.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Context.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/ATen.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/script.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/convert.cpp:4:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cpu_ks = DispatchKeySet(DispatchKey::AutogradCPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradIPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_ipu_ks = DispatchKeySet(DispatchKey::AutogradIPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xpu_ks = DispatchKeySet(DispatchKey::AutogradXPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCUDA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cuda_ks = DispatchKeySet(DispatchKey::AutogradCUDA);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXLA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xla_ks = DispatchKeySet(DispatchKey::AutogradXLA);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradLazy)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_lazy_ks = DispatchKeySet(DispatchKey::AutogradLazy);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMeta)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_meta_ks = DispatchKeySet(DispatchKey::AutogradMeta);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMPS)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_mps_ks = DispatchKeySet(DispatchKey::AutogradMPS);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradHPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_hpu_ks = DispatchKeySet(DispatchKey::AutogradHPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse1)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse1);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse2)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse2);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse3)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse3);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-sparse\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-sparse\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[126 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/fps.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/graclus.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/grid.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/knn.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/nearest.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/radius.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/rw.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/sampler.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/testing.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m copying torch_cluster/typing.py -> build/lib.linux-x86_64-cpython-311/torch_cluster\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_cluster.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_cluster.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_cluster.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_cluster.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_cluster._fps_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -DWITH_PYTHON -Icsrc -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/TH -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/THC -I/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include/python3.11 -c csrc/cpu/fps_cpu.cpp -o build/temp.linux-x86_64-cpython-311/csrc/cpu/fps_cpu.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_fps_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBase.h:14,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:38,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/fps_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/fps_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      "  \u001b[31m   \u001b[0m  #error \\\n",
      "  \u001b[31m   \u001b[0m   ^~~~~\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/Backend.h:5,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/Layout.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/TensorBody.h:12,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/../extensions.h:2,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/fps_cpu.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/cpu/fps_cpu.cpp:1:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cpu_ks = DispatchKeySet(DispatchKey::AutogradCPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradIPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_ipu_ks = DispatchKeySet(DispatchKey::AutogradIPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xpu_ks = DispatchKeySet(DispatchKey::AutogradXPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCUDA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cuda_ks = DispatchKeySet(DispatchKey::AutogradCUDA);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXLA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xla_ks = DispatchKeySet(DispatchKey::AutogradXLA);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradLazy)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_lazy_ks = DispatchKeySet(DispatchKey::AutogradLazy);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMeta)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_meta_ks = DispatchKeySet(DispatchKey::AutogradMeta);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMPS)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_mps_ks = DispatchKeySet(DispatchKey::AutogradMPS);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradHPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_hpu_ks = DispatchKeySet(DispatchKey::AutogradHPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse1)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse1);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse2)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse2);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse3)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse3);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-cluster\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-cluster\n",
      "  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[104 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m copying torch_spline_conv/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m copying torch_spline_conv/basis.py -> build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m copying torch_spline_conv/conv.py -> build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m copying torch_spline_conv/testing.py -> build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m copying torch_spline_conv/weighting.py -> build/lib.linux-x86_64-cpython-311/torch_spline_conv\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_spline_conv.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_spline_conv.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_spline_conv.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_spline_conv.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_spline_conv.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_spline_conv.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'torch_spline_conv._basis_cpu' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-311/csrc/cpu\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -O2 -isystem /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include -fPIC -Icsrc -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/TH -I/home/spuspa/.local/lib/python3.11/site-packages/torch/include/THC -I/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/include/python3.11 -c csrc/basis.cpp -o build/temp.linux-x86_64-cpython-311/csrc/basis.o -O2 -Wno-sign-compare -DAT_PARALLEL_OPENMP -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=_basis_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/CallOnce.h:8,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Context.h:22,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/ATen.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/script.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/basis.cpp:2:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/util/C++17.h:13:2: error: #error \"You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later.\"\n",
      "  \u001b[31m   \u001b[0m  #error \\\n",
      "  \u001b[31m   \u001b[0m   ^~~~~\n",
      "  \u001b[31m   \u001b[0m In file included from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/core/Generator.h:11,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/CPUGeneratorImpl.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/Context.h:4,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/ATen/ATen.h:7,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from /home/spuspa/.local/lib/python3.11/site-packages/torch/include/torch/script.h:3,\n",
      "  \u001b[31m   \u001b[0m                  from csrc/basis.cpp:2:\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:754:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cpu_ks = DispatchKeySet(DispatchKey::AutogradCPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradIPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:755:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_ipu_ks = DispatchKeySet(DispatchKey::AutogradIPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:756:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xpu_ks = DispatchKeySet(DispatchKey::AutogradXPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradCUDA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:757:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_cuda_ks = DispatchKeySet(DispatchKey::AutogradCUDA);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradXLA)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:758:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_xla_ks = DispatchKeySet(DispatchKey::AutogradXLA);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradLazy)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:759:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_lazy_ks = DispatchKeySet(DispatchKey::AutogradLazy);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMeta)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:760:75: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_meta_ks = DispatchKeySet(DispatchKey::AutogradMeta);\n",
      "  \u001b[31m   \u001b[0m                                                                            ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradMPS)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:761:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_mps_ks = DispatchKeySet(DispatchKey::AutogradMPS);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradHPU)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:762:73: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m  constexpr auto autograd_hpu_ks = DispatchKeySet(DispatchKey::AutogradHPU);\n",
      "  \u001b[31m   \u001b[0m                                                                          ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse1)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:764:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse1);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse2)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:766:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse2);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52:   in ‘constexpr’ expansion of ‘c10::DispatchKeySet(AutogradPrivateUse3)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:236:42:   in ‘constexpr’ expansion of ‘c10::toBackendComponent(k)’\n",
      "  \u001b[31m   \u001b[0m /home/spuspa/.local/lib/python3.11/site-packages/torch/include/c10/core/DispatchKeySet.h:768:52: error: overflow in constant expression [-fpermissive]\n",
      "  \u001b[31m   \u001b[0m      DispatchKeySet(DispatchKey::AutogradPrivateUse3);\n",
      "  \u001b[31m   \u001b[0m                                                     ^\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-spline-conv\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for torch-spline-conv\n",
      "Failed to build torch-scatter torch-sparse torch-cluster torch-spline-conv\n",
      "\u001b[31mERROR: Could not build wheels for torch-scatter, torch-sparse, torch-cluster, torch-spline-conv, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2657a76b-8096-4229-b10c-a8ed1a22d7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3555061209.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -c \"import torch; print(torch.__version__)\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import torch; print(torch.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa6661b-3e99-4bb8-8d94-38e8dd4fc222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "655cb1d1-7939-4868-81d7-3fe7586bdc36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/c9/bc/b7db44f5f39f9d0494071bddae6880eb645970366d0a200022a1a93d57f5/pip-25.0.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (0.38.4)\n",
      "Collecting wheel\n",
      "  Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel, pip\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-25.0.1 wheel-0.45.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/spuspa/.local/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/spuspa/.local/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /home/spuspa/.local/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/spuspa/.local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip wheel\n",
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381a2227-66c2-4b43-bd46-61f62c045212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_cluster-1.6.3%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/spuspa/.local/lib/python3.11/site-packages (from torch-sparse) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from scipy->torch-sparse) (1.24.3)\n",
      "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
      "Successfully installed torch-cluster-1.6.3+pt25cu124 torch-scatter-2.1.2+pt25cu124 torch-sparse-0.6.18+pt25cu124 torch-spline-conv-1.2.2+pt25cu124\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: aiohttp in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.8.5)\n",
      "Requirement already satisfied: fsspec in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (2023.4.0)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch-geometric) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from aiohttp->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->torch-geometric) (2023.7.22)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
    "!pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9674f049-5a74-4142-ab5c-c23812d06c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyG Installed Successfully!\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch_geometric; print('✅ PyG Installed Successfully!')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33bbee3b-0a97-42d6-b972-2ae6d6473813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ✅ Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fa8097-6eb8-47cf-8b3b-7cfcd65c4e22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features normalized.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load dataset\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "else:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "# ✅ Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# ✅ Separate features & labels\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan\n",
    "\n",
    "# ✅ Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351984cb-58e9-4350-9426-0613c27a37c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check for missing values\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ✅ Fill NaN values with the column mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# ✅ Drop rows with any remaining NaNs (optional but safer)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ✅ Verify no NaNs remain\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ✅ Now proceed with normalizing & creating KNN graph\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df.drop(columns=['label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e5ae34-0765-4df0-aa8e-2e27b684a4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph dataset created.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Convert dataset into a graph using KNN\n",
    "k_neighbors = 5  # Adjust based on dataset size\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "edge_index = torch.tensor(np.array(knn_graph.nonzero()), dtype=torch.long)\n",
    "\n",
    "# ✅ Convert to PyTorch Tensors\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# ✅ Create a PyTorch Geometric data object\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7904f07d-4992-41af-a21b-77d1e977cc78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Training samples: 15972\n",
      "✅ Testing samples: 4028\n"
     ]
    }
   ],
   "source": [
    "# ✅ Split into training & test sets (80% train, 20% test)\n",
    "train_mask = torch.rand(len(y)) < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f72213-243b-46ff-a510-753769123853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GNN model initialized.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Define Graph Neural Network Model\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ✅ Initialize model\n",
    "model = GNN(input_dim=X.shape[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✅ GNN model initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d9da28-e477-457d-9829-ec90f876ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GNN model initialized.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Define Graph Neural Network Model\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# ✅ Initialize model\n",
    "model = GNN(input_dim=X.shape[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✅ GNN model initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6bab47-d49e-4365-84e7-52daff2371b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Training Started...\n",
      "Epoch 1/2000, Loss: 0.6965, Train Accuracy: 48.14%\n",
      "Epoch 2/2000, Loss: 0.6971, Train Accuracy: 49.76%\n",
      "Epoch 3/2000, Loss: 0.6948, Train Accuracy: 48.62%\n",
      "Epoch 4/2000, Loss: 0.6953, Train Accuracy: 48.47%\n",
      "Epoch 5/2000, Loss: 0.6942, Train Accuracy: 48.45%\n",
      "Epoch 6/2000, Loss: 0.6930, Train Accuracy: 49.44%\n",
      "Epoch 7/2000, Loss: 0.6927, Train Accuracy: 52.83%\n",
      "Epoch 8/2000, Loss: 0.6924, Train Accuracy: 53.43%\n",
      "Epoch 9/2000, Loss: 0.6915, Train Accuracy: 53.59%\n",
      "Epoch 10/2000, Loss: 0.6908, Train Accuracy: 52.06%\n",
      "Epoch 11/2000, Loss: 0.6905, Train Accuracy: 50.85%\n",
      "Epoch 12/2000, Loss: 0.6903, Train Accuracy: 51.14%\n",
      "Epoch 13/2000, Loss: 0.6897, Train Accuracy: 52.05%\n",
      "Epoch 14/2000, Loss: 0.6891, Train Accuracy: 53.58%\n",
      "Epoch 15/2000, Loss: 0.6889, Train Accuracy: 54.50%\n",
      "Epoch 16/2000, Loss: 0.6887, Train Accuracy: 54.58%\n",
      "Epoch 17/2000, Loss: 0.6883, Train Accuracy: 54.64%\n",
      "Epoch 18/2000, Loss: 0.6879, Train Accuracy: 54.14%\n",
      "Epoch 19/2000, Loss: 0.6878, Train Accuracy: 53.83%\n",
      "Epoch 20/2000, Loss: 0.6876, Train Accuracy: 53.69%\n",
      "Epoch 21/2000, Loss: 0.6873, Train Accuracy: 54.31%\n",
      "Epoch 22/2000, Loss: 0.6870, Train Accuracy: 54.90%\n",
      "Epoch 23/2000, Loss: 0.6869, Train Accuracy: 55.16%\n",
      "Epoch 24/2000, Loss: 0.6867, Train Accuracy: 55.23%\n",
      "Epoch 25/2000, Loss: 0.6864, Train Accuracy: 54.98%\n",
      "Epoch 26/2000, Loss: 0.6863, Train Accuracy: 54.88%\n",
      "Epoch 27/2000, Loss: 0.6861, Train Accuracy: 54.80%\n",
      "Epoch 28/2000, Loss: 0.6859, Train Accuracy: 54.92%\n",
      "Epoch 29/2000, Loss: 0.6856, Train Accuracy: 55.15%\n",
      "Epoch 30/2000, Loss: 0.6855, Train Accuracy: 55.55%\n",
      "Epoch 31/2000, Loss: 0.6853, Train Accuracy: 55.62%\n",
      "Epoch 32/2000, Loss: 0.6851, Train Accuracy: 55.40%\n",
      "Epoch 33/2000, Loss: 0.6849, Train Accuracy: 55.27%\n",
      "Epoch 34/2000, Loss: 0.6847, Train Accuracy: 55.30%\n",
      "Epoch 35/2000, Loss: 0.6845, Train Accuracy: 55.31%\n",
      "Epoch 36/2000, Loss: 0.6843, Train Accuracy: 55.68%\n",
      "Epoch 37/2000, Loss: 0.6841, Train Accuracy: 55.78%\n",
      "Epoch 38/2000, Loss: 0.6839, Train Accuracy: 55.92%\n",
      "Epoch 39/2000, Loss: 0.6836, Train Accuracy: 55.65%\n",
      "Epoch 40/2000, Loss: 0.6835, Train Accuracy: 55.60%\n",
      "Epoch 41/2000, Loss: 0.6833, Train Accuracy: 55.67%\n",
      "Epoch 42/2000, Loss: 0.6830, Train Accuracy: 55.96%\n",
      "Epoch 43/2000, Loss: 0.6828, Train Accuracy: 56.16%\n",
      "Epoch 44/2000, Loss: 0.6826, Train Accuracy: 56.17%\n",
      "Epoch 45/2000, Loss: 0.6824, Train Accuracy: 56.00%\n",
      "Epoch 46/2000, Loss: 0.6822, Train Accuracy: 55.95%\n",
      "Epoch 47/2000, Loss: 0.6820, Train Accuracy: 55.95%\n",
      "Epoch 48/2000, Loss: 0.6818, Train Accuracy: 56.23%\n",
      "Epoch 49/2000, Loss: 0.6815, Train Accuracy: 56.31%\n",
      "Epoch 50/2000, Loss: 0.6813, Train Accuracy: 56.22%\n",
      "Epoch 51/2000, Loss: 0.6811, Train Accuracy: 56.19%\n",
      "Epoch 52/2000, Loss: 0.6808, Train Accuracy: 56.37%\n",
      "Epoch 53/2000, Loss: 0.6806, Train Accuracy: 56.35%\n",
      "Epoch 54/2000, Loss: 0.6804, Train Accuracy: 56.48%\n",
      "Epoch 55/2000, Loss: 0.6801, Train Accuracy: 56.50%\n",
      "Epoch 56/2000, Loss: 0.6799, Train Accuracy: 56.49%\n",
      "Epoch 57/2000, Loss: 0.6796, Train Accuracy: 56.58%\n",
      "Epoch 58/2000, Loss: 0.6794, Train Accuracy: 56.66%\n",
      "Epoch 59/2000, Loss: 0.6791, Train Accuracy: 56.81%\n",
      "Epoch 60/2000, Loss: 0.6788, Train Accuracy: 56.89%\n",
      "Epoch 61/2000, Loss: 0.6786, Train Accuracy: 56.97%\n",
      "Epoch 62/2000, Loss: 0.6783, Train Accuracy: 57.09%\n",
      "Epoch 63/2000, Loss: 0.6780, Train Accuracy: 57.12%\n",
      "Epoch 64/2000, Loss: 0.6778, Train Accuracy: 57.12%\n",
      "Epoch 65/2000, Loss: 0.6775, Train Accuracy: 57.23%\n",
      "Epoch 66/2000, Loss: 0.6772, Train Accuracy: 57.22%\n",
      "Epoch 67/2000, Loss: 0.6769, Train Accuracy: 57.32%\n",
      "Epoch 68/2000, Loss: 0.6767, Train Accuracy: 57.45%\n",
      "Epoch 69/2000, Loss: 0.6764, Train Accuracy: 57.43%\n",
      "Epoch 70/2000, Loss: 0.6761, Train Accuracy: 57.53%\n",
      "Epoch 71/2000, Loss: 0.6758, Train Accuracy: 57.54%\n",
      "Epoch 72/2000, Loss: 0.6755, Train Accuracy: 57.58%\n",
      "Epoch 73/2000, Loss: 0.6752, Train Accuracy: 57.73%\n",
      "Epoch 74/2000, Loss: 0.6750, Train Accuracy: 57.80%\n",
      "Epoch 75/2000, Loss: 0.6747, Train Accuracy: 57.77%\n",
      "Epoch 76/2000, Loss: 0.6744, Train Accuracy: 57.81%\n",
      "Epoch 77/2000, Loss: 0.6741, Train Accuracy: 57.84%\n",
      "Epoch 78/2000, Loss: 0.6738, Train Accuracy: 57.89%\n",
      "Epoch 79/2000, Loss: 0.6735, Train Accuracy: 57.91%\n",
      "Epoch 80/2000, Loss: 0.6732, Train Accuracy: 58.00%\n",
      "Epoch 81/2000, Loss: 0.6729, Train Accuracy: 58.00%\n",
      "Epoch 82/2000, Loss: 0.6726, Train Accuracy: 58.07%\n",
      "Epoch 83/2000, Loss: 0.6723, Train Accuracy: 58.12%\n",
      "Epoch 84/2000, Loss: 0.6721, Train Accuracy: 58.06%\n",
      "Epoch 85/2000, Loss: 0.6718, Train Accuracy: 58.06%\n",
      "Epoch 86/2000, Loss: 0.6715, Train Accuracy: 58.10%\n",
      "Epoch 87/2000, Loss: 0.6712, Train Accuracy: 58.13%\n",
      "Epoch 88/2000, Loss: 0.6710, Train Accuracy: 58.18%\n",
      "Epoch 89/2000, Loss: 0.6707, Train Accuracy: 58.26%\n",
      "Epoch 90/2000, Loss: 0.6704, Train Accuracy: 58.31%\n",
      "Epoch 91/2000, Loss: 0.6701, Train Accuracy: 58.38%\n",
      "Epoch 92/2000, Loss: 0.6699, Train Accuracy: 58.43%\n",
      "Epoch 93/2000, Loss: 0.6696, Train Accuracy: 58.46%\n",
      "Epoch 94/2000, Loss: 0.6693, Train Accuracy: 58.53%\n",
      "Epoch 95/2000, Loss: 0.6691, Train Accuracy: 58.55%\n",
      "Epoch 96/2000, Loss: 0.6688, Train Accuracy: 58.52%\n",
      "Epoch 97/2000, Loss: 0.6686, Train Accuracy: 58.60%\n",
      "Epoch 98/2000, Loss: 0.6683, Train Accuracy: 58.58%\n",
      "Epoch 99/2000, Loss: 0.6681, Train Accuracy: 58.68%\n",
      "Epoch 100/2000, Loss: 0.6678, Train Accuracy: 58.73%\n",
      "Epoch 101/2000, Loss: 0.6676, Train Accuracy: 58.72%\n",
      "Epoch 102/2000, Loss: 0.6674, Train Accuracy: 58.70%\n",
      "Epoch 103/2000, Loss: 0.6671, Train Accuracy: 58.75%\n",
      "Epoch 104/2000, Loss: 0.6669, Train Accuracy: 58.78%\n",
      "Epoch 105/2000, Loss: 0.6667, Train Accuracy: 58.84%\n",
      "Epoch 106/2000, Loss: 0.6665, Train Accuracy: 58.85%\n",
      "Epoch 107/2000, Loss: 0.6662, Train Accuracy: 58.88%\n",
      "Epoch 108/2000, Loss: 0.6660, Train Accuracy: 58.92%\n",
      "Epoch 109/2000, Loss: 0.6658, Train Accuracy: 58.93%\n",
      "Epoch 110/2000, Loss: 0.6656, Train Accuracy: 59.02%\n",
      "Epoch 111/2000, Loss: 0.6654, Train Accuracy: 59.05%\n",
      "Epoch 112/2000, Loss: 0.6652, Train Accuracy: 59.06%\n",
      "Epoch 113/2000, Loss: 0.6651, Train Accuracy: 59.07%\n",
      "Epoch 114/2000, Loss: 0.6649, Train Accuracy: 59.08%\n",
      "Epoch 115/2000, Loss: 0.6647, Train Accuracy: 59.20%\n",
      "Epoch 116/2000, Loss: 0.6645, Train Accuracy: 59.15%\n",
      "Epoch 117/2000, Loss: 0.6643, Train Accuracy: 59.15%\n",
      "Epoch 118/2000, Loss: 0.6642, Train Accuracy: 59.12%\n",
      "Epoch 119/2000, Loss: 0.6640, Train Accuracy: 59.16%\n",
      "Epoch 120/2000, Loss: 0.6639, Train Accuracy: 59.16%\n",
      "Epoch 121/2000, Loss: 0.6637, Train Accuracy: 59.10%\n",
      "Epoch 122/2000, Loss: 0.6636, Train Accuracy: 59.25%\n",
      "Epoch 123/2000, Loss: 0.6634, Train Accuracy: 58.98%\n",
      "Epoch 124/2000, Loss: 0.6633, Train Accuracy: 59.47%\n",
      "Epoch 125/2000, Loss: 0.6632, Train Accuracy: 59.12%\n",
      "Epoch 126/2000, Loss: 0.6630, Train Accuracy: 59.37%\n",
      "Epoch 127/2000, Loss: 0.6629, Train Accuracy: 59.09%\n",
      "Epoch 128/2000, Loss: 0.6628, Train Accuracy: 59.11%\n",
      "Epoch 129/2000, Loss: 0.6627, Train Accuracy: 59.43%\n",
      "Epoch 130/2000, Loss: 0.6626, Train Accuracy: 59.15%\n",
      "Epoch 131/2000, Loss: 0.6624, Train Accuracy: 59.27%\n",
      "Epoch 132/2000, Loss: 0.6623, Train Accuracy: 59.32%\n",
      "Epoch 133/2000, Loss: 0.6622, Train Accuracy: 59.13%\n",
      "Epoch 134/2000, Loss: 0.6621, Train Accuracy: 59.38%\n",
      "Epoch 135/2000, Loss: 0.6620, Train Accuracy: 59.25%\n",
      "Epoch 136/2000, Loss: 0.6619, Train Accuracy: 59.32%\n",
      "Epoch 137/2000, Loss: 0.6618, Train Accuracy: 59.37%\n",
      "Epoch 138/2000, Loss: 0.6617, Train Accuracy: 59.25%\n",
      "Epoch 139/2000, Loss: 0.6616, Train Accuracy: 59.39%\n",
      "Epoch 140/2000, Loss: 0.6615, Train Accuracy: 59.39%\n",
      "Epoch 141/2000, Loss: 0.6615, Train Accuracy: 59.38%\n",
      "Epoch 142/2000, Loss: 0.6614, Train Accuracy: 59.49%\n",
      "Epoch 143/2000, Loss: 0.6613, Train Accuracy: 59.37%\n",
      "Epoch 144/2000, Loss: 0.6612, Train Accuracy: 59.55%\n",
      "Epoch 145/2000, Loss: 0.6611, Train Accuracy: 59.50%\n",
      "Epoch 146/2000, Loss: 0.6610, Train Accuracy: 59.50%\n",
      "Epoch 147/2000, Loss: 0.6610, Train Accuracy: 59.53%\n",
      "Epoch 148/2000, Loss: 0.6609, Train Accuracy: 59.54%\n",
      "Epoch 149/2000, Loss: 0.6608, Train Accuracy: 59.60%\n",
      "Epoch 150/2000, Loss: 0.6608, Train Accuracy: 59.59%\n",
      "Epoch 151/2000, Loss: 0.6607, Train Accuracy: 59.66%\n",
      "Epoch 152/2000, Loss: 0.6606, Train Accuracy: 59.65%\n",
      "Epoch 153/2000, Loss: 0.6605, Train Accuracy: 59.67%\n",
      "Epoch 154/2000, Loss: 0.6605, Train Accuracy: 59.75%\n",
      "Epoch 155/2000, Loss: 0.6604, Train Accuracy: 59.82%\n",
      "Epoch 156/2000, Loss: 0.6603, Train Accuracy: 59.80%\n",
      "Epoch 157/2000, Loss: 0.6603, Train Accuracy: 59.73%\n",
      "Epoch 158/2000, Loss: 0.6602, Train Accuracy: 59.85%\n",
      "Epoch 159/2000, Loss: 0.6601, Train Accuracy: 59.79%\n",
      "Epoch 160/2000, Loss: 0.6601, Train Accuracy: 59.95%\n",
      "Epoch 161/2000, Loss: 0.6600, Train Accuracy: 59.71%\n",
      "Epoch 162/2000, Loss: 0.6600, Train Accuracy: 59.97%\n",
      "Epoch 163/2000, Loss: 0.6599, Train Accuracy: 59.70%\n",
      "Epoch 164/2000, Loss: 0.6599, Train Accuracy: 60.01%\n",
      "Epoch 165/2000, Loss: 0.6598, Train Accuracy: 59.89%\n",
      "Epoch 166/2000, Loss: 0.6597, Train Accuracy: 59.93%\n",
      "Epoch 167/2000, Loss: 0.6597, Train Accuracy: 59.99%\n",
      "Epoch 168/2000, Loss: 0.6596, Train Accuracy: 59.84%\n",
      "Epoch 169/2000, Loss: 0.6596, Train Accuracy: 60.06%\n",
      "Epoch 170/2000, Loss: 0.6595, Train Accuracy: 59.84%\n",
      "Epoch 171/2000, Loss: 0.6594, Train Accuracy: 59.98%\n",
      "Epoch 172/2000, Loss: 0.6594, Train Accuracy: 59.99%\n",
      "Epoch 173/2000, Loss: 0.6593, Train Accuracy: 59.97%\n",
      "Epoch 174/2000, Loss: 0.6593, Train Accuracy: 60.04%\n",
      "Epoch 175/2000, Loss: 0.6592, Train Accuracy: 59.90%\n",
      "Epoch 176/2000, Loss: 0.6592, Train Accuracy: 60.09%\n",
      "Epoch 177/2000, Loss: 0.6591, Train Accuracy: 59.97%\n",
      "Epoch 178/2000, Loss: 0.6591, Train Accuracy: 60.07%\n",
      "Epoch 179/2000, Loss: 0.6590, Train Accuracy: 59.97%\n",
      "Epoch 180/2000, Loss: 0.6589, Train Accuracy: 59.98%\n",
      "Epoch 181/2000, Loss: 0.6589, Train Accuracy: 60.15%\n",
      "Epoch 182/2000, Loss: 0.6589, Train Accuracy: 60.04%\n",
      "Epoch 183/2000, Loss: 0.6588, Train Accuracy: 60.15%\n",
      "Epoch 184/2000, Loss: 0.6588, Train Accuracy: 60.03%\n",
      "Epoch 185/2000, Loss: 0.6587, Train Accuracy: 60.23%\n",
      "Epoch 186/2000, Loss: 0.6587, Train Accuracy: 60.02%\n",
      "Epoch 187/2000, Loss: 0.6586, Train Accuracy: 60.26%\n",
      "Epoch 188/2000, Loss: 0.6586, Train Accuracy: 59.99%\n",
      "Epoch 189/2000, Loss: 0.6585, Train Accuracy: 60.19%\n",
      "Epoch 190/2000, Loss: 0.6584, Train Accuracy: 60.07%\n",
      "Epoch 191/2000, Loss: 0.6584, Train Accuracy: 60.17%\n",
      "Epoch 192/2000, Loss: 0.6584, Train Accuracy: 60.19%\n",
      "Epoch 193/2000, Loss: 0.6583, Train Accuracy: 60.09%\n",
      "Epoch 194/2000, Loss: 0.6583, Train Accuracy: 60.29%\n",
      "Epoch 195/2000, Loss: 0.6582, Train Accuracy: 60.10%\n",
      "Epoch 196/2000, Loss: 0.6582, Train Accuracy: 60.30%\n",
      "Epoch 197/2000, Loss: 0.6581, Train Accuracy: 59.97%\n",
      "Epoch 198/2000, Loss: 0.6581, Train Accuracy: 60.35%\n",
      "Epoch 199/2000, Loss: 0.6581, Train Accuracy: 60.06%\n",
      "Epoch 200/2000, Loss: 0.6581, Train Accuracy: 60.43%\n",
      "Epoch 201/2000, Loss: 0.6580, Train Accuracy: 60.07%\n",
      "Epoch 202/2000, Loss: 0.6580, Train Accuracy: 60.46%\n",
      "Epoch 203/2000, Loss: 0.6579, Train Accuracy: 60.18%\n",
      "Epoch 204/2000, Loss: 0.6578, Train Accuracy: 60.26%\n",
      "Epoch 205/2000, Loss: 0.6578, Train Accuracy: 60.32%\n",
      "Epoch 206/2000, Loss: 0.6578, Train Accuracy: 60.11%\n",
      "Epoch 207/2000, Loss: 0.6577, Train Accuracy: 60.47%\n",
      "Epoch 208/2000, Loss: 0.6577, Train Accuracy: 60.22%\n",
      "Epoch 209/2000, Loss: 0.6576, Train Accuracy: 60.36%\n",
      "Epoch 210/2000, Loss: 0.6575, Train Accuracy: 60.38%\n",
      "Epoch 211/2000, Loss: 0.6575, Train Accuracy: 60.18%\n",
      "Epoch 212/2000, Loss: 0.6575, Train Accuracy: 60.46%\n",
      "Epoch 213/2000, Loss: 0.6575, Train Accuracy: 60.24%\n",
      "Epoch 214/2000, Loss: 0.6574, Train Accuracy: 60.42%\n",
      "Epoch 215/2000, Loss: 0.6573, Train Accuracy: 60.34%\n",
      "Epoch 216/2000, Loss: 0.6573, Train Accuracy: 60.29%\n",
      "Epoch 217/2000, Loss: 0.6573, Train Accuracy: 60.50%\n",
      "Epoch 218/2000, Loss: 0.6572, Train Accuracy: 60.32%\n",
      "Epoch 219/2000, Loss: 0.6572, Train Accuracy: 60.48%\n",
      "Epoch 220/2000, Loss: 0.6571, Train Accuracy: 60.27%\n",
      "Epoch 221/2000, Loss: 0.6571, Train Accuracy: 60.45%\n",
      "Epoch 222/2000, Loss: 0.6570, Train Accuracy: 60.43%\n",
      "Epoch 223/2000, Loss: 0.6570, Train Accuracy: 60.33%\n",
      "Epoch 224/2000, Loss: 0.6570, Train Accuracy: 60.51%\n",
      "Epoch 225/2000, Loss: 0.6569, Train Accuracy: 60.35%\n",
      "Epoch 226/2000, Loss: 0.6569, Train Accuracy: 60.54%\n",
      "Epoch 227/2000, Loss: 0.6568, Train Accuracy: 60.33%\n",
      "Epoch 228/2000, Loss: 0.6568, Train Accuracy: 60.51%\n",
      "Epoch 229/2000, Loss: 0.6568, Train Accuracy: 60.42%\n",
      "Epoch 230/2000, Loss: 0.6567, Train Accuracy: 60.41%\n",
      "Epoch 231/2000, Loss: 0.6567, Train Accuracy: 60.49%\n",
      "Epoch 232/2000, Loss: 0.6566, Train Accuracy: 60.40%\n",
      "Epoch 233/2000, Loss: 0.6566, Train Accuracy: 60.57%\n",
      "Epoch 234/2000, Loss: 0.6566, Train Accuracy: 60.36%\n",
      "Epoch 235/2000, Loss: 0.6565, Train Accuracy: 60.56%\n",
      "Epoch 236/2000, Loss: 0.6565, Train Accuracy: 60.36%\n",
      "Epoch 237/2000, Loss: 0.6564, Train Accuracy: 60.54%\n",
      "Epoch 238/2000, Loss: 0.6564, Train Accuracy: 60.44%\n",
      "Epoch 239/2000, Loss: 0.6564, Train Accuracy: 60.53%\n",
      "Epoch 240/2000, Loss: 0.6563, Train Accuracy: 60.51%\n",
      "Epoch 241/2000, Loss: 0.6563, Train Accuracy: 60.56%\n",
      "Epoch 242/2000, Loss: 0.6562, Train Accuracy: 60.54%\n",
      "Epoch 243/2000, Loss: 0.6562, Train Accuracy: 60.50%\n",
      "Epoch 244/2000, Loss: 0.6561, Train Accuracy: 60.54%\n",
      "Epoch 245/2000, Loss: 0.6561, Train Accuracy: 60.53%\n",
      "Epoch 246/2000, Loss: 0.6561, Train Accuracy: 60.52%\n",
      "Epoch 247/2000, Loss: 0.6560, Train Accuracy: 60.51%\n",
      "Epoch 248/2000, Loss: 0.6560, Train Accuracy: 60.52%\n",
      "Epoch 249/2000, Loss: 0.6559, Train Accuracy: 60.51%\n",
      "Epoch 250/2000, Loss: 0.6559, Train Accuracy: 60.51%\n",
      "Epoch 251/2000, Loss: 0.6559, Train Accuracy: 60.52%\n",
      "Epoch 252/2000, Loss: 0.6558, Train Accuracy: 60.54%\n",
      "Epoch 253/2000, Loss: 0.6558, Train Accuracy: 60.55%\n",
      "Epoch 254/2000, Loss: 0.6557, Train Accuracy: 60.54%\n",
      "Epoch 255/2000, Loss: 0.6557, Train Accuracy: 60.55%\n",
      "Epoch 256/2000, Loss: 0.6557, Train Accuracy: 60.54%\n",
      "Epoch 257/2000, Loss: 0.6556, Train Accuracy: 60.54%\n",
      "Epoch 258/2000, Loss: 0.6556, Train Accuracy: 60.61%\n",
      "Epoch 259/2000, Loss: 0.6556, Train Accuracy: 60.74%\n",
      "Epoch 260/2000, Loss: 0.6557, Train Accuracy: 60.50%\n",
      "Epoch 261/2000, Loss: 0.6559, Train Accuracy: 60.47%\n",
      "Epoch 262/2000, Loss: 0.6561, Train Accuracy: 60.36%\n",
      "Epoch 263/2000, Loss: 0.6559, Train Accuracy: 60.46%\n",
      "Epoch 264/2000, Loss: 0.6554, Train Accuracy: 60.64%\n",
      "Epoch 265/2000, Loss: 0.6554, Train Accuracy: 60.69%\n",
      "Epoch 266/2000, Loss: 0.6557, Train Accuracy: 60.56%\n",
      "Epoch 267/2000, Loss: 0.6555, Train Accuracy: 60.56%\n",
      "Epoch 268/2000, Loss: 0.6552, Train Accuracy: 60.59%\n",
      "Epoch 269/2000, Loss: 0.6554, Train Accuracy: 60.67%\n",
      "Epoch 270/2000, Loss: 0.6554, Train Accuracy: 60.54%\n",
      "Epoch 271/2000, Loss: 0.6551, Train Accuracy: 60.73%\n",
      "Epoch 272/2000, Loss: 0.6552, Train Accuracy: 60.83%\n",
      "Epoch 273/2000, Loss: 0.6552, Train Accuracy: 60.66%\n",
      "Epoch 274/2000, Loss: 0.6550, Train Accuracy: 60.78%\n",
      "Epoch 275/2000, Loss: 0.6550, Train Accuracy: 60.78%\n",
      "Epoch 276/2000, Loss: 0.6550, Train Accuracy: 60.66%\n",
      "Epoch 277/2000, Loss: 0.6549, Train Accuracy: 60.79%\n",
      "Epoch 278/2000, Loss: 0.6549, Train Accuracy: 60.79%\n",
      "Epoch 279/2000, Loss: 0.6549, Train Accuracy: 60.83%\n",
      "Epoch 280/2000, Loss: 0.6548, Train Accuracy: 60.75%\n",
      "Epoch 281/2000, Loss: 0.6547, Train Accuracy: 60.72%\n",
      "Epoch 282/2000, Loss: 0.6548, Train Accuracy: 60.79%\n",
      "Epoch 283/2000, Loss: 0.6547, Train Accuracy: 60.86%\n",
      "Epoch 284/2000, Loss: 0.6546, Train Accuracy: 60.71%\n",
      "Epoch 285/2000, Loss: 0.6546, Train Accuracy: 60.77%\n",
      "Epoch 286/2000, Loss: 0.6546, Train Accuracy: 60.87%\n",
      "Epoch 287/2000, Loss: 0.6545, Train Accuracy: 60.76%\n",
      "Epoch 288/2000, Loss: 0.6545, Train Accuracy: 60.72%\n",
      "Epoch 289/2000, Loss: 0.6545, Train Accuracy: 60.84%\n",
      "Epoch 290/2000, Loss: 0.6545, Train Accuracy: 60.75%\n",
      "Epoch 291/2000, Loss: 0.6544, Train Accuracy: 60.72%\n",
      "Epoch 292/2000, Loss: 0.6544, Train Accuracy: 60.78%\n",
      "Epoch 293/2000, Loss: 0.6544, Train Accuracy: 60.79%\n",
      "Epoch 294/2000, Loss: 0.6543, Train Accuracy: 60.77%\n",
      "Epoch 295/2000, Loss: 0.6543, Train Accuracy: 60.86%\n",
      "Epoch 296/2000, Loss: 0.6543, Train Accuracy: 60.82%\n",
      "Epoch 297/2000, Loss: 0.6542, Train Accuracy: 60.88%\n",
      "Epoch 298/2000, Loss: 0.6542, Train Accuracy: 60.86%\n",
      "Epoch 299/2000, Loss: 0.6542, Train Accuracy: 60.76%\n",
      "Epoch 300/2000, Loss: 0.6541, Train Accuracy: 60.90%\n",
      "Epoch 301/2000, Loss: 0.6541, Train Accuracy: 60.81%\n",
      "Epoch 302/2000, Loss: 0.6540, Train Accuracy: 60.78%\n",
      "Epoch 303/2000, Loss: 0.6540, Train Accuracy: 60.91%\n",
      "Epoch 304/2000, Loss: 0.6540, Train Accuracy: 60.79%\n",
      "Epoch 305/2000, Loss: 0.6539, Train Accuracy: 60.85%\n",
      "Epoch 306/2000, Loss: 0.6539, Train Accuracy: 60.89%\n",
      "Epoch 307/2000, Loss: 0.6539, Train Accuracy: 60.84%\n",
      "Epoch 308/2000, Loss: 0.6538, Train Accuracy: 60.89%\n",
      "Epoch 309/2000, Loss: 0.6538, Train Accuracy: 60.90%\n",
      "Epoch 310/2000, Loss: 0.6538, Train Accuracy: 60.85%\n",
      "Epoch 311/2000, Loss: 0.6538, Train Accuracy: 60.88%\n",
      "Epoch 312/2000, Loss: 0.6537, Train Accuracy: 60.89%\n",
      "Epoch 313/2000, Loss: 0.6537, Train Accuracy: 60.91%\n",
      "Epoch 314/2000, Loss: 0.6537, Train Accuracy: 60.92%\n",
      "Epoch 315/2000, Loss: 0.6536, Train Accuracy: 60.90%\n",
      "Epoch 316/2000, Loss: 0.6536, Train Accuracy: 60.90%\n",
      "Epoch 317/2000, Loss: 0.6536, Train Accuracy: 60.94%\n",
      "Epoch 318/2000, Loss: 0.6535, Train Accuracy: 60.98%\n",
      "Epoch 319/2000, Loss: 0.6535, Train Accuracy: 60.89%\n",
      "Epoch 320/2000, Loss: 0.6535, Train Accuracy: 60.95%\n",
      "Epoch 321/2000, Loss: 0.6534, Train Accuracy: 60.94%\n",
      "Epoch 322/2000, Loss: 0.6534, Train Accuracy: 60.94%\n",
      "Epoch 323/2000, Loss: 0.6534, Train Accuracy: 60.98%\n",
      "Epoch 324/2000, Loss: 0.6533, Train Accuracy: 60.91%\n",
      "Epoch 325/2000, Loss: 0.6533, Train Accuracy: 60.96%\n",
      "Epoch 326/2000, Loss: 0.6533, Train Accuracy: 60.97%\n",
      "Epoch 327/2000, Loss: 0.6532, Train Accuracy: 60.93%\n",
      "Epoch 328/2000, Loss: 0.6532, Train Accuracy: 60.95%\n",
      "Epoch 329/2000, Loss: 0.6531, Train Accuracy: 60.84%\n",
      "Epoch 330/2000, Loss: 0.6531, Train Accuracy: 60.93%\n",
      "Epoch 331/2000, Loss: 0.6530, Train Accuracy: 60.92%\n",
      "Epoch 332/2000, Loss: 0.6530, Train Accuracy: 60.89%\n",
      "Epoch 333/2000, Loss: 0.6530, Train Accuracy: 60.88%\n",
      "Epoch 334/2000, Loss: 0.6529, Train Accuracy: 60.90%\n",
      "Epoch 335/2000, Loss: 0.6529, Train Accuracy: 60.91%\n",
      "Epoch 336/2000, Loss: 0.6528, Train Accuracy: 61.03%\n",
      "Epoch 337/2000, Loss: 0.6528, Train Accuracy: 60.94%\n",
      "Epoch 338/2000, Loss: 0.6528, Train Accuracy: 60.99%\n",
      "Epoch 339/2000, Loss: 0.6527, Train Accuracy: 60.97%\n",
      "Epoch 340/2000, Loss: 0.6527, Train Accuracy: 61.01%\n",
      "Epoch 341/2000, Loss: 0.6526, Train Accuracy: 61.01%\n",
      "Epoch 342/2000, Loss: 0.6526, Train Accuracy: 61.01%\n",
      "Epoch 343/2000, Loss: 0.6526, Train Accuracy: 60.99%\n",
      "Epoch 344/2000, Loss: 0.6525, Train Accuracy: 60.98%\n",
      "Epoch 345/2000, Loss: 0.6525, Train Accuracy: 61.00%\n",
      "Epoch 346/2000, Loss: 0.6525, Train Accuracy: 61.02%\n",
      "Epoch 347/2000, Loss: 0.6524, Train Accuracy: 61.05%\n",
      "Epoch 348/2000, Loss: 0.6524, Train Accuracy: 61.08%\n",
      "Epoch 349/2000, Loss: 0.6523, Train Accuracy: 61.09%\n",
      "Epoch 350/2000, Loss: 0.6523, Train Accuracy: 61.06%\n",
      "Epoch 351/2000, Loss: 0.6523, Train Accuracy: 61.04%\n",
      "Epoch 352/2000, Loss: 0.6522, Train Accuracy: 61.05%\n",
      "Epoch 353/2000, Loss: 0.6522, Train Accuracy: 60.98%\n",
      "Epoch 354/2000, Loss: 0.6521, Train Accuracy: 61.05%\n",
      "Epoch 355/2000, Loss: 0.6521, Train Accuracy: 61.07%\n",
      "Epoch 356/2000, Loss: 0.6521, Train Accuracy: 61.11%\n",
      "Epoch 357/2000, Loss: 0.6520, Train Accuracy: 61.08%\n",
      "Epoch 358/2000, Loss: 0.6520, Train Accuracy: 61.19%\n",
      "Epoch 359/2000, Loss: 0.6520, Train Accuracy: 61.09%\n",
      "Epoch 360/2000, Loss: 0.6520, Train Accuracy: 61.15%\n",
      "Epoch 361/2000, Loss: 0.6520, Train Accuracy: 61.11%\n",
      "Epoch 362/2000, Loss: 0.6521, Train Accuracy: 61.46%\n",
      "Epoch 363/2000, Loss: 0.6522, Train Accuracy: 61.31%\n",
      "Epoch 364/2000, Loss: 0.6523, Train Accuracy: 61.28%\n",
      "Epoch 365/2000, Loss: 0.6521, Train Accuracy: 61.29%\n",
      "Epoch 366/2000, Loss: 0.6518, Train Accuracy: 61.35%\n",
      "Epoch 367/2000, Loss: 0.6517, Train Accuracy: 61.19%\n",
      "Epoch 368/2000, Loss: 0.6518, Train Accuracy: 61.24%\n",
      "Epoch 369/2000, Loss: 0.6519, Train Accuracy: 61.45%\n",
      "Epoch 370/2000, Loss: 0.6517, Train Accuracy: 61.25%\n",
      "Epoch 371/2000, Loss: 0.6516, Train Accuracy: 61.18%\n",
      "Epoch 372/2000, Loss: 0.6516, Train Accuracy: 61.38%\n",
      "Epoch 373/2000, Loss: 0.6517, Train Accuracy: 61.23%\n",
      "Epoch 374/2000, Loss: 0.6516, Train Accuracy: 61.48%\n",
      "Epoch 375/2000, Loss: 0.6515, Train Accuracy: 61.21%\n",
      "Epoch 376/2000, Loss: 0.6514, Train Accuracy: 61.24%\n",
      "Epoch 377/2000, Loss: 0.6515, Train Accuracy: 61.48%\n",
      "Epoch 378/2000, Loss: 0.6515, Train Accuracy: 61.22%\n",
      "Epoch 379/2000, Loss: 0.6513, Train Accuracy: 61.37%\n",
      "Epoch 380/2000, Loss: 0.6513, Train Accuracy: 61.29%\n",
      "Epoch 381/2000, Loss: 0.6513, Train Accuracy: 61.22%\n",
      "Epoch 382/2000, Loss: 0.6513, Train Accuracy: 61.39%\n",
      "Epoch 383/2000, Loss: 0.6512, Train Accuracy: 61.23%\n",
      "Epoch 384/2000, Loss: 0.6512, Train Accuracy: 61.23%\n",
      "Epoch 385/2000, Loss: 0.6512, Train Accuracy: 61.39%\n",
      "Epoch 386/2000, Loss: 0.6512, Train Accuracy: 61.24%\n",
      "Epoch 387/2000, Loss: 0.6511, Train Accuracy: 61.45%\n",
      "Epoch 388/2000, Loss: 0.6511, Train Accuracy: 61.21%\n",
      "Epoch 389/2000, Loss: 0.6510, Train Accuracy: 61.20%\n",
      "Epoch 390/2000, Loss: 0.6510, Train Accuracy: 61.45%\n",
      "Epoch 391/2000, Loss: 0.6510, Train Accuracy: 61.21%\n",
      "Epoch 392/2000, Loss: 0.6509, Train Accuracy: 61.45%\n",
      "Epoch 393/2000, Loss: 0.6509, Train Accuracy: 61.43%\n",
      "Epoch 394/2000, Loss: 0.6509, Train Accuracy: 61.26%\n",
      "Epoch 395/2000, Loss: 0.6509, Train Accuracy: 61.49%\n",
      "Epoch 396/2000, Loss: 0.6508, Train Accuracy: 61.23%\n",
      "Epoch 397/2000, Loss: 0.6508, Train Accuracy: 61.41%\n",
      "Epoch 398/2000, Loss: 0.6508, Train Accuracy: 61.48%\n",
      "Epoch 399/2000, Loss: 0.6508, Train Accuracy: 61.31%\n",
      "Epoch 400/2000, Loss: 0.6507, Train Accuracy: 61.53%\n",
      "Epoch 401/2000, Loss: 0.6507, Train Accuracy: 61.28%\n",
      "Epoch 402/2000, Loss: 0.6507, Train Accuracy: 61.41%\n",
      "Epoch 403/2000, Loss: 0.6506, Train Accuracy: 61.43%\n",
      "Epoch 404/2000, Loss: 0.6506, Train Accuracy: 61.33%\n",
      "Epoch 405/2000, Loss: 0.6506, Train Accuracy: 61.53%\n",
      "Epoch 406/2000, Loss: 0.6506, Train Accuracy: 61.36%\n",
      "Epoch 407/2000, Loss: 0.6505, Train Accuracy: 61.43%\n",
      "Epoch 408/2000, Loss: 0.6505, Train Accuracy: 61.36%\n",
      "Epoch 409/2000, Loss: 0.6505, Train Accuracy: 61.39%\n",
      "Epoch 410/2000, Loss: 0.6504, Train Accuracy: 61.50%\n",
      "Epoch 411/2000, Loss: 0.6504, Train Accuracy: 61.39%\n",
      "Epoch 412/2000, Loss: 0.6504, Train Accuracy: 61.53%\n",
      "Epoch 413/2000, Loss: 0.6504, Train Accuracy: 61.39%\n",
      "Epoch 414/2000, Loss: 0.6503, Train Accuracy: 61.43%\n",
      "Epoch 415/2000, Loss: 0.6503, Train Accuracy: 61.44%\n",
      "Epoch 416/2000, Loss: 0.6503, Train Accuracy: 61.40%\n",
      "Epoch 417/2000, Loss: 0.6503, Train Accuracy: 61.50%\n",
      "Epoch 418/2000, Loss: 0.6502, Train Accuracy: 61.43%\n",
      "Epoch 419/2000, Loss: 0.6502, Train Accuracy: 61.48%\n",
      "Epoch 420/2000, Loss: 0.6502, Train Accuracy: 61.44%\n",
      "Epoch 421/2000, Loss: 0.6502, Train Accuracy: 61.53%\n",
      "Epoch 422/2000, Loss: 0.6501, Train Accuracy: 61.55%\n",
      "Epoch 423/2000, Loss: 0.6501, Train Accuracy: 61.45%\n",
      "Epoch 424/2000, Loss: 0.6501, Train Accuracy: 61.55%\n",
      "Epoch 425/2000, Loss: 0.6501, Train Accuracy: 61.43%\n",
      "Epoch 426/2000, Loss: 0.6500, Train Accuracy: 61.53%\n",
      "Epoch 427/2000, Loss: 0.6500, Train Accuracy: 61.47%\n",
      "Epoch 428/2000, Loss: 0.6500, Train Accuracy: 61.53%\n",
      "Epoch 429/2000, Loss: 0.6500, Train Accuracy: 61.51%\n",
      "Epoch 430/2000, Loss: 0.6499, Train Accuracy: 61.57%\n",
      "Epoch 431/2000, Loss: 0.6499, Train Accuracy: 61.55%\n",
      "Epoch 432/2000, Loss: 0.6499, Train Accuracy: 61.56%\n",
      "Epoch 433/2000, Loss: 0.6498, Train Accuracy: 61.54%\n",
      "Epoch 434/2000, Loss: 0.6498, Train Accuracy: 61.55%\n",
      "Epoch 435/2000, Loss: 0.6498, Train Accuracy: 61.58%\n",
      "Epoch 436/2000, Loss: 0.6498, Train Accuracy: 61.56%\n",
      "Epoch 437/2000, Loss: 0.6497, Train Accuracy: 61.58%\n",
      "Epoch 438/2000, Loss: 0.6497, Train Accuracy: 61.56%\n",
      "Epoch 439/2000, Loss: 0.6497, Train Accuracy: 61.58%\n",
      "Epoch 440/2000, Loss: 0.6497, Train Accuracy: 61.59%\n",
      "Epoch 441/2000, Loss: 0.6497, Train Accuracy: 61.53%\n",
      "Epoch 442/2000, Loss: 0.6496, Train Accuracy: 61.60%\n",
      "Epoch 443/2000, Loss: 0.6496, Train Accuracy: 61.58%\n",
      "Epoch 444/2000, Loss: 0.6496, Train Accuracy: 61.53%\n",
      "Epoch 445/2000, Loss: 0.6496, Train Accuracy: 61.80%\n",
      "Epoch 446/2000, Loss: 0.6496, Train Accuracy: 61.42%\n",
      "Epoch 447/2000, Loss: 0.6496, Train Accuracy: 61.87%\n",
      "Epoch 448/2000, Loss: 0.6496, Train Accuracy: 61.34%\n",
      "Epoch 449/2000, Loss: 0.6497, Train Accuracy: 61.70%\n",
      "Epoch 450/2000, Loss: 0.6497, Train Accuracy: 61.47%\n",
      "Epoch 451/2000, Loss: 0.6497, Train Accuracy: 61.70%\n",
      "Epoch 452/2000, Loss: 0.6496, Train Accuracy: 61.39%\n",
      "Epoch 453/2000, Loss: 0.6495, Train Accuracy: 61.87%\n",
      "Epoch 454/2000, Loss: 0.6494, Train Accuracy: 61.56%\n",
      "Epoch 455/2000, Loss: 0.6493, Train Accuracy: 61.60%\n",
      "Epoch 456/2000, Loss: 0.6494, Train Accuracy: 61.86%\n",
      "Epoch 457/2000, Loss: 0.6494, Train Accuracy: 61.37%\n",
      "Epoch 458/2000, Loss: 0.6494, Train Accuracy: 61.80%\n",
      "Epoch 459/2000, Loss: 0.6493, Train Accuracy: 61.41%\n",
      "Epoch 460/2000, Loss: 0.6492, Train Accuracy: 61.84%\n",
      "Epoch 461/2000, Loss: 0.6492, Train Accuracy: 61.66%\n",
      "Epoch 462/2000, Loss: 0.6492, Train Accuracy: 61.60%\n",
      "Epoch 463/2000, Loss: 0.6492, Train Accuracy: 61.89%\n",
      "Epoch 464/2000, Loss: 0.6492, Train Accuracy: 61.51%\n",
      "Epoch 465/2000, Loss: 0.6492, Train Accuracy: 61.88%\n",
      "Epoch 466/2000, Loss: 0.6491, Train Accuracy: 61.59%\n",
      "Epoch 467/2000, Loss: 0.6491, Train Accuracy: 61.65%\n",
      "Epoch 468/2000, Loss: 0.6491, Train Accuracy: 61.78%\n",
      "Epoch 469/2000, Loss: 0.6491, Train Accuracy: 61.60%\n",
      "Epoch 470/2000, Loss: 0.6491, Train Accuracy: 61.88%\n",
      "Epoch 471/2000, Loss: 0.6490, Train Accuracy: 61.56%\n",
      "Epoch 472/2000, Loss: 0.6490, Train Accuracy: 61.91%\n",
      "Epoch 473/2000, Loss: 0.6489, Train Accuracy: 61.63%\n",
      "Epoch 474/2000, Loss: 0.6489, Train Accuracy: 61.61%\n",
      "Epoch 475/2000, Loss: 0.6489, Train Accuracy: 61.80%\n",
      "Epoch 476/2000, Loss: 0.6489, Train Accuracy: 61.65%\n",
      "Epoch 477/2000, Loss: 0.6489, Train Accuracy: 61.90%\n",
      "Epoch 478/2000, Loss: 0.6489, Train Accuracy: 61.64%\n",
      "Epoch 479/2000, Loss: 0.6488, Train Accuracy: 61.88%\n",
      "Epoch 480/2000, Loss: 0.6488, Train Accuracy: 61.64%\n",
      "Epoch 481/2000, Loss: 0.6488, Train Accuracy: 61.66%\n",
      "Epoch 482/2000, Loss: 0.6488, Train Accuracy: 61.81%\n",
      "Epoch 483/2000, Loss: 0.6488, Train Accuracy: 61.64%\n",
      "Epoch 484/2000, Loss: 0.6487, Train Accuracy: 61.88%\n",
      "Epoch 485/2000, Loss: 0.6487, Train Accuracy: 61.65%\n",
      "Epoch 486/2000, Loss: 0.6487, Train Accuracy: 61.85%\n",
      "Epoch 487/2000, Loss: 0.6487, Train Accuracy: 61.61%\n",
      "Epoch 488/2000, Loss: 0.6486, Train Accuracy: 61.86%\n",
      "Epoch 489/2000, Loss: 0.6486, Train Accuracy: 61.72%\n",
      "Epoch 490/2000, Loss: 0.6486, Train Accuracy: 61.73%\n",
      "Epoch 491/2000, Loss: 0.6486, Train Accuracy: 61.83%\n",
      "Epoch 492/2000, Loss: 0.6486, Train Accuracy: 61.71%\n",
      "Epoch 493/2000, Loss: 0.6486, Train Accuracy: 61.85%\n",
      "Epoch 494/2000, Loss: 0.6485, Train Accuracy: 61.66%\n",
      "Epoch 495/2000, Loss: 0.6485, Train Accuracy: 61.83%\n",
      "Epoch 496/2000, Loss: 0.6485, Train Accuracy: 61.71%\n",
      "Epoch 497/2000, Loss: 0.6485, Train Accuracy: 61.84%\n",
      "Epoch 498/2000, Loss: 0.6485, Train Accuracy: 61.75%\n",
      "Epoch 499/2000, Loss: 0.6484, Train Accuracy: 61.83%\n",
      "Epoch 500/2000, Loss: 0.6484, Train Accuracy: 61.76%\n",
      "Epoch 501/2000, Loss: 0.6484, Train Accuracy: 61.84%\n",
      "Epoch 502/2000, Loss: 0.6484, Train Accuracy: 61.72%\n",
      "Epoch 503/2000, Loss: 0.6484, Train Accuracy: 61.86%\n",
      "Epoch 504/2000, Loss: 0.6483, Train Accuracy: 61.76%\n",
      "Epoch 505/2000, Loss: 0.6483, Train Accuracy: 61.86%\n",
      "Epoch 506/2000, Loss: 0.6483, Train Accuracy: 61.75%\n",
      "Epoch 507/2000, Loss: 0.6483, Train Accuracy: 61.86%\n",
      "Epoch 508/2000, Loss: 0.6483, Train Accuracy: 61.74%\n",
      "Epoch 509/2000, Loss: 0.6483, Train Accuracy: 61.86%\n",
      "Epoch 510/2000, Loss: 0.6482, Train Accuracy: 61.76%\n",
      "Epoch 511/2000, Loss: 0.6482, Train Accuracy: 61.83%\n",
      "Epoch 512/2000, Loss: 0.6482, Train Accuracy: 61.74%\n",
      "Epoch 513/2000, Loss: 0.6482, Train Accuracy: 61.88%\n",
      "Epoch 514/2000, Loss: 0.6483, Train Accuracy: 61.64%\n",
      "Epoch 515/2000, Loss: 0.6483, Train Accuracy: 61.81%\n",
      "Epoch 516/2000, Loss: 0.6483, Train Accuracy: 61.53%\n",
      "Epoch 517/2000, Loss: 0.6484, Train Accuracy: 61.90%\n",
      "Epoch 518/2000, Loss: 0.6484, Train Accuracy: 61.60%\n",
      "Epoch 519/2000, Loss: 0.6483, Train Accuracy: 61.82%\n",
      "Epoch 520/2000, Loss: 0.6482, Train Accuracy: 61.67%\n",
      "Epoch 521/2000, Loss: 0.6481, Train Accuracy: 61.85%\n",
      "Epoch 522/2000, Loss: 0.6480, Train Accuracy: 61.85%\n",
      "Epoch 523/2000, Loss: 0.6480, Train Accuracy: 61.80%\n",
      "Epoch 524/2000, Loss: 0.6481, Train Accuracy: 61.81%\n",
      "Epoch 525/2000, Loss: 0.6481, Train Accuracy: 61.70%\n",
      "Epoch 526/2000, Loss: 0.6480, Train Accuracy: 61.81%\n",
      "Epoch 527/2000, Loss: 0.6480, Train Accuracy: 61.83%\n",
      "Epoch 528/2000, Loss: 0.6479, Train Accuracy: 61.82%\n",
      "Epoch 529/2000, Loss: 0.6479, Train Accuracy: 61.82%\n",
      "Epoch 530/2000, Loss: 0.6479, Train Accuracy: 61.85%\n",
      "Epoch 531/2000, Loss: 0.6479, Train Accuracy: 61.78%\n",
      "Epoch 532/2000, Loss: 0.6479, Train Accuracy: 61.80%\n",
      "Epoch 533/2000, Loss: 0.6479, Train Accuracy: 61.84%\n",
      "Epoch 534/2000, Loss: 0.6478, Train Accuracy: 61.88%\n",
      "Epoch 535/2000, Loss: 0.6478, Train Accuracy: 61.92%\n",
      "Epoch 536/2000, Loss: 0.6478, Train Accuracy: 61.82%\n",
      "Epoch 537/2000, Loss: 0.6478, Train Accuracy: 61.88%\n",
      "Epoch 538/2000, Loss: 0.6478, Train Accuracy: 61.92%\n",
      "Epoch 539/2000, Loss: 0.6478, Train Accuracy: 61.83%\n",
      "Epoch 540/2000, Loss: 0.6477, Train Accuracy: 61.92%\n",
      "Epoch 541/2000, Loss: 0.6477, Train Accuracy: 61.87%\n",
      "Epoch 542/2000, Loss: 0.6477, Train Accuracy: 61.83%\n",
      "Epoch 543/2000, Loss: 0.6477, Train Accuracy: 61.87%\n",
      "Epoch 544/2000, Loss: 0.6477, Train Accuracy: 61.90%\n",
      "Epoch 545/2000, Loss: 0.6477, Train Accuracy: 61.89%\n",
      "Epoch 546/2000, Loss: 0.6476, Train Accuracy: 61.90%\n",
      "Epoch 547/2000, Loss: 0.6476, Train Accuracy: 61.91%\n",
      "Epoch 548/2000, Loss: 0.6476, Train Accuracy: 61.93%\n",
      "Epoch 549/2000, Loss: 0.6476, Train Accuracy: 62.00%\n",
      "Epoch 550/2000, Loss: 0.6476, Train Accuracy: 61.88%\n",
      "Epoch 551/2000, Loss: 0.6475, Train Accuracy: 61.86%\n",
      "Epoch 552/2000, Loss: 0.6475, Train Accuracy: 61.85%\n",
      "Epoch 553/2000, Loss: 0.6475, Train Accuracy: 61.85%\n",
      "Epoch 554/2000, Loss: 0.6475, Train Accuracy: 61.87%\n",
      "Epoch 555/2000, Loss: 0.6475, Train Accuracy: 61.88%\n",
      "Epoch 556/2000, Loss: 0.6475, Train Accuracy: 61.95%\n",
      "Epoch 557/2000, Loss: 0.6475, Train Accuracy: 61.91%\n",
      "Epoch 558/2000, Loss: 0.6475, Train Accuracy: 61.96%\n",
      "Epoch 559/2000, Loss: 0.6474, Train Accuracy: 61.90%\n",
      "Epoch 560/2000, Loss: 0.6474, Train Accuracy: 61.98%\n",
      "Epoch 561/2000, Loss: 0.6474, Train Accuracy: 61.94%\n",
      "Epoch 562/2000, Loss: 0.6474, Train Accuracy: 61.98%\n",
      "Epoch 563/2000, Loss: 0.6474, Train Accuracy: 61.94%\n",
      "Epoch 564/2000, Loss: 0.6474, Train Accuracy: 62.00%\n",
      "Epoch 565/2000, Loss: 0.6474, Train Accuracy: 61.96%\n",
      "Epoch 566/2000, Loss: 0.6473, Train Accuracy: 61.99%\n",
      "Epoch 567/2000, Loss: 0.6473, Train Accuracy: 61.98%\n",
      "Epoch 568/2000, Loss: 0.6473, Train Accuracy: 61.98%\n",
      "Epoch 569/2000, Loss: 0.6473, Train Accuracy: 61.98%\n",
      "Epoch 570/2000, Loss: 0.6473, Train Accuracy: 61.96%\n",
      "Epoch 571/2000, Loss: 0.6473, Train Accuracy: 61.93%\n",
      "Epoch 572/2000, Loss: 0.6473, Train Accuracy: 61.94%\n",
      "Epoch 573/2000, Loss: 0.6473, Train Accuracy: 61.94%\n",
      "Epoch 574/2000, Loss: 0.6472, Train Accuracy: 61.95%\n",
      "Epoch 575/2000, Loss: 0.6472, Train Accuracy: 61.96%\n",
      "Epoch 576/2000, Loss: 0.6472, Train Accuracy: 61.94%\n",
      "Epoch 577/2000, Loss: 0.6472, Train Accuracy: 61.95%\n",
      "Epoch 578/2000, Loss: 0.6472, Train Accuracy: 61.95%\n",
      "Epoch 579/2000, Loss: 0.6472, Train Accuracy: 61.98%\n",
      "Epoch 580/2000, Loss: 0.6471, Train Accuracy: 61.95%\n",
      "Epoch 581/2000, Loss: 0.6471, Train Accuracy: 61.93%\n",
      "Epoch 582/2000, Loss: 0.6471, Train Accuracy: 61.98%\n",
      "Epoch 583/2000, Loss: 0.6471, Train Accuracy: 61.90%\n",
      "Epoch 584/2000, Loss: 0.6471, Train Accuracy: 61.99%\n",
      "Epoch 585/2000, Loss: 0.6470, Train Accuracy: 61.89%\n",
      "Epoch 586/2000, Loss: 0.6470, Train Accuracy: 61.97%\n",
      "Epoch 587/2000, Loss: 0.6470, Train Accuracy: 61.90%\n",
      "Epoch 588/2000, Loss: 0.6470, Train Accuracy: 61.99%\n",
      "Epoch 589/2000, Loss: 0.6470, Train Accuracy: 61.88%\n",
      "Epoch 590/2000, Loss: 0.6470, Train Accuracy: 61.98%\n",
      "Epoch 591/2000, Loss: 0.6470, Train Accuracy: 61.90%\n",
      "Epoch 592/2000, Loss: 0.6470, Train Accuracy: 61.96%\n",
      "Epoch 593/2000, Loss: 0.6470, Train Accuracy: 61.99%\n",
      "Epoch 594/2000, Loss: 0.6470, Train Accuracy: 62.03%\n",
      "Epoch 595/2000, Loss: 0.6470, Train Accuracy: 61.96%\n",
      "Epoch 596/2000, Loss: 0.6470, Train Accuracy: 62.16%\n",
      "Epoch 597/2000, Loss: 0.6471, Train Accuracy: 61.96%\n",
      "Epoch 598/2000, Loss: 0.6471, Train Accuracy: 62.05%\n",
      "Epoch 599/2000, Loss: 0.6471, Train Accuracy: 61.92%\n",
      "Epoch 600/2000, Loss: 0.6470, Train Accuracy: 62.08%\n",
      "Epoch 601/2000, Loss: 0.6469, Train Accuracy: 62.02%\n",
      "Epoch 602/2000, Loss: 0.6469, Train Accuracy: 62.04%\n",
      "Epoch 603/2000, Loss: 0.6468, Train Accuracy: 61.88%\n",
      "Epoch 604/2000, Loss: 0.6468, Train Accuracy: 61.88%\n",
      "Epoch 605/2000, Loss: 0.6468, Train Accuracy: 62.01%\n",
      "Epoch 606/2000, Loss: 0.6468, Train Accuracy: 61.98%\n",
      "Epoch 607/2000, Loss: 0.6468, Train Accuracy: 62.06%\n",
      "Epoch 608/2000, Loss: 0.6468, Train Accuracy: 62.03%\n",
      "Epoch 609/2000, Loss: 0.6468, Train Accuracy: 62.10%\n",
      "Epoch 610/2000, Loss: 0.6467, Train Accuracy: 61.92%\n",
      "Epoch 611/2000, Loss: 0.6467, Train Accuracy: 61.91%\n",
      "Epoch 612/2000, Loss: 0.6467, Train Accuracy: 61.95%\n",
      "Epoch 613/2000, Loss: 0.6467, Train Accuracy: 61.89%\n",
      "Epoch 614/2000, Loss: 0.6467, Train Accuracy: 62.01%\n",
      "Epoch 615/2000, Loss: 0.6467, Train Accuracy: 61.93%\n",
      "Epoch 616/2000, Loss: 0.6467, Train Accuracy: 62.08%\n",
      "Epoch 617/2000, Loss: 0.6467, Train Accuracy: 61.90%\n",
      "Epoch 618/2000, Loss: 0.6466, Train Accuracy: 62.02%\n",
      "Epoch 619/2000, Loss: 0.6466, Train Accuracy: 61.86%\n",
      "Epoch 620/2000, Loss: 0.6466, Train Accuracy: 61.86%\n",
      "Epoch 621/2000, Loss: 0.6466, Train Accuracy: 61.93%\n",
      "Epoch 622/2000, Loss: 0.6465, Train Accuracy: 61.90%\n",
      "Epoch 623/2000, Loss: 0.6465, Train Accuracy: 61.93%\n",
      "Epoch 624/2000, Loss: 0.6465, Train Accuracy: 61.85%\n",
      "Epoch 625/2000, Loss: 0.6465, Train Accuracy: 62.00%\n",
      "Epoch 626/2000, Loss: 0.6465, Train Accuracy: 61.86%\n",
      "Epoch 627/2000, Loss: 0.6465, Train Accuracy: 62.01%\n",
      "Epoch 628/2000, Loss: 0.6465, Train Accuracy: 61.86%\n",
      "Epoch 629/2000, Loss: 0.6465, Train Accuracy: 61.97%\n",
      "Epoch 630/2000, Loss: 0.6465, Train Accuracy: 61.91%\n",
      "Epoch 631/2000, Loss: 0.6464, Train Accuracy: 61.94%\n",
      "Epoch 632/2000, Loss: 0.6464, Train Accuracy: 61.95%\n",
      "Epoch 633/2000, Loss: 0.6464, Train Accuracy: 61.98%\n",
      "Epoch 634/2000, Loss: 0.6464, Train Accuracy: 61.96%\n",
      "Epoch 635/2000, Loss: 0.6464, Train Accuracy: 61.90%\n",
      "Epoch 636/2000, Loss: 0.6464, Train Accuracy: 61.95%\n",
      "Epoch 637/2000, Loss: 0.6464, Train Accuracy: 61.88%\n",
      "Epoch 638/2000, Loss: 0.6464, Train Accuracy: 61.99%\n",
      "Epoch 639/2000, Loss: 0.6464, Train Accuracy: 61.92%\n",
      "Epoch 640/2000, Loss: 0.6464, Train Accuracy: 62.02%\n",
      "Epoch 641/2000, Loss: 0.6464, Train Accuracy: 61.92%\n",
      "Epoch 642/2000, Loss: 0.6464, Train Accuracy: 62.08%\n",
      "Epoch 643/2000, Loss: 0.6464, Train Accuracy: 61.96%\n",
      "Epoch 644/2000, Loss: 0.6463, Train Accuracy: 62.15%\n",
      "Epoch 645/2000, Loss: 0.6463, Train Accuracy: 62.01%\n",
      "Epoch 646/2000, Loss: 0.6463, Train Accuracy: 62.17%\n",
      "Epoch 647/2000, Loss: 0.6463, Train Accuracy: 61.98%\n",
      "Epoch 648/2000, Loss: 0.6463, Train Accuracy: 62.14%\n",
      "Epoch 649/2000, Loss: 0.6463, Train Accuracy: 61.93%\n",
      "Epoch 650/2000, Loss: 0.6462, Train Accuracy: 62.07%\n",
      "Epoch 651/2000, Loss: 0.6462, Train Accuracy: 61.94%\n",
      "Epoch 652/2000, Loss: 0.6462, Train Accuracy: 61.94%\n",
      "Epoch 653/2000, Loss: 0.6462, Train Accuracy: 61.92%\n",
      "Epoch 654/2000, Loss: 0.6462, Train Accuracy: 61.99%\n",
      "Epoch 655/2000, Loss: 0.6461, Train Accuracy: 62.01%\n",
      "Epoch 656/2000, Loss: 0.6461, Train Accuracy: 62.00%\n",
      "Epoch 657/2000, Loss: 0.6461, Train Accuracy: 62.00%\n",
      "Epoch 658/2000, Loss: 0.6461, Train Accuracy: 61.96%\n",
      "Epoch 659/2000, Loss: 0.6461, Train Accuracy: 61.95%\n",
      "Epoch 660/2000, Loss: 0.6461, Train Accuracy: 61.95%\n",
      "Epoch 661/2000, Loss: 0.6461, Train Accuracy: 62.00%\n",
      "Epoch 662/2000, Loss: 0.6461, Train Accuracy: 61.91%\n",
      "Epoch 663/2000, Loss: 0.6461, Train Accuracy: 62.13%\n",
      "Epoch 664/2000, Loss: 0.6461, Train Accuracy: 62.08%\n",
      "Epoch 665/2000, Loss: 0.6461, Train Accuracy: 62.23%\n",
      "Epoch 666/2000, Loss: 0.6462, Train Accuracy: 62.09%\n",
      "Epoch 667/2000, Loss: 0.6462, Train Accuracy: 62.15%\n",
      "Epoch 668/2000, Loss: 0.6462, Train Accuracy: 62.06%\n",
      "Epoch 669/2000, Loss: 0.6461, Train Accuracy: 62.18%\n",
      "Epoch 670/2000, Loss: 0.6461, Train Accuracy: 62.10%\n",
      "Epoch 671/2000, Loss: 0.6460, Train Accuracy: 62.18%\n",
      "Epoch 672/2000, Loss: 0.6460, Train Accuracy: 61.95%\n",
      "Epoch 673/2000, Loss: 0.6459, Train Accuracy: 62.01%\n",
      "Epoch 674/2000, Loss: 0.6459, Train Accuracy: 62.06%\n",
      "Epoch 675/2000, Loss: 0.6459, Train Accuracy: 62.00%\n",
      "Epoch 676/2000, Loss: 0.6459, Train Accuracy: 62.03%\n",
      "Epoch 677/2000, Loss: 0.6460, Train Accuracy: 61.98%\n",
      "Epoch 678/2000, Loss: 0.6460, Train Accuracy: 62.18%\n",
      "Epoch 679/2000, Loss: 0.6460, Train Accuracy: 62.00%\n",
      "Epoch 680/2000, Loss: 0.6459, Train Accuracy: 62.17%\n",
      "Epoch 681/2000, Loss: 0.6459, Train Accuracy: 61.95%\n",
      "Epoch 682/2000, Loss: 0.6459, Train Accuracy: 62.13%\n",
      "Epoch 683/2000, Loss: 0.6459, Train Accuracy: 62.03%\n",
      "Epoch 684/2000, Loss: 0.6458, Train Accuracy: 62.00%\n",
      "Epoch 685/2000, Loss: 0.6458, Train Accuracy: 62.01%\n",
      "Epoch 686/2000, Loss: 0.6458, Train Accuracy: 61.99%\n",
      "Epoch 687/2000, Loss: 0.6458, Train Accuracy: 62.05%\n",
      "Epoch 688/2000, Loss: 0.6458, Train Accuracy: 62.01%\n",
      "Epoch 689/2000, Loss: 0.6458, Train Accuracy: 62.06%\n",
      "Epoch 690/2000, Loss: 0.6458, Train Accuracy: 62.02%\n",
      "Epoch 691/2000, Loss: 0.6458, Train Accuracy: 62.08%\n",
      "Epoch 692/2000, Loss: 0.6458, Train Accuracy: 61.99%\n",
      "Epoch 693/2000, Loss: 0.6458, Train Accuracy: 62.14%\n",
      "Epoch 694/2000, Loss: 0.6458, Train Accuracy: 61.99%\n",
      "Epoch 695/2000, Loss: 0.6458, Train Accuracy: 62.13%\n",
      "Epoch 696/2000, Loss: 0.6457, Train Accuracy: 62.02%\n",
      "Epoch 697/2000, Loss: 0.6457, Train Accuracy: 62.11%\n",
      "Epoch 698/2000, Loss: 0.6457, Train Accuracy: 62.05%\n",
      "Epoch 699/2000, Loss: 0.6457, Train Accuracy: 62.08%\n",
      "Epoch 700/2000, Loss: 0.6457, Train Accuracy: 62.04%\n",
      "Epoch 701/2000, Loss: 0.6456, Train Accuracy: 62.05%\n",
      "Epoch 702/2000, Loss: 0.6456, Train Accuracy: 62.03%\n",
      "Epoch 703/2000, Loss: 0.6456, Train Accuracy: 62.07%\n",
      "Epoch 704/2000, Loss: 0.6456, Train Accuracy: 62.05%\n",
      "Epoch 705/2000, Loss: 0.6456, Train Accuracy: 62.03%\n",
      "Epoch 706/2000, Loss: 0.6456, Train Accuracy: 62.10%\n",
      "Epoch 707/2000, Loss: 0.6456, Train Accuracy: 62.06%\n",
      "Epoch 708/2000, Loss: 0.6456, Train Accuracy: 62.02%\n",
      "Epoch 709/2000, Loss: 0.6456, Train Accuracy: 62.10%\n",
      "Epoch 710/2000, Loss: 0.6456, Train Accuracy: 62.10%\n",
      "Epoch 711/2000, Loss: 0.6456, Train Accuracy: 62.03%\n",
      "Epoch 712/2000, Loss: 0.6456, Train Accuracy: 62.18%\n",
      "Epoch 713/2000, Loss: 0.6456, Train Accuracy: 62.01%\n",
      "Epoch 714/2000, Loss: 0.6456, Train Accuracy: 62.17%\n",
      "Epoch 715/2000, Loss: 0.6457, Train Accuracy: 62.17%\n",
      "Epoch 716/2000, Loss: 0.6458, Train Accuracy: 62.30%\n",
      "Epoch 717/2000, Loss: 0.6458, Train Accuracy: 62.08%\n",
      "Epoch 718/2000, Loss: 0.6458, Train Accuracy: 62.25%\n",
      "Epoch 719/2000, Loss: 0.6457, Train Accuracy: 62.11%\n",
      "Epoch 720/2000, Loss: 0.6456, Train Accuracy: 62.22%\n",
      "Epoch 721/2000, Loss: 0.6455, Train Accuracy: 62.02%\n",
      "Epoch 722/2000, Loss: 0.6454, Train Accuracy: 62.10%\n",
      "Epoch 723/2000, Loss: 0.6454, Train Accuracy: 62.18%\n",
      "Epoch 724/2000, Loss: 0.6455, Train Accuracy: 62.05%\n",
      "Epoch 725/2000, Loss: 0.6455, Train Accuracy: 62.25%\n",
      "Epoch 726/2000, Loss: 0.6455, Train Accuracy: 62.10%\n",
      "Epoch 727/2000, Loss: 0.6455, Train Accuracy: 62.18%\n",
      "Epoch 728/2000, Loss: 0.6454, Train Accuracy: 62.06%\n",
      "Epoch 729/2000, Loss: 0.6454, Train Accuracy: 62.10%\n",
      "Epoch 730/2000, Loss: 0.6453, Train Accuracy: 62.09%\n",
      "Epoch 731/2000, Loss: 0.6454, Train Accuracy: 62.06%\n",
      "Epoch 732/2000, Loss: 0.6454, Train Accuracy: 62.20%\n",
      "Epoch 733/2000, Loss: 0.6454, Train Accuracy: 62.10%\n",
      "Epoch 734/2000, Loss: 0.6454, Train Accuracy: 62.20%\n",
      "Epoch 735/2000, Loss: 0.6453, Train Accuracy: 62.10%\n",
      "Epoch 736/2000, Loss: 0.6453, Train Accuracy: 62.14%\n",
      "Epoch 737/2000, Loss: 0.6453, Train Accuracy: 62.11%\n",
      "Epoch 738/2000, Loss: 0.6453, Train Accuracy: 62.10%\n",
      "Epoch 739/2000, Loss: 0.6453, Train Accuracy: 62.15%\n",
      "Epoch 740/2000, Loss: 0.6453, Train Accuracy: 62.13%\n",
      "Epoch 741/2000, Loss: 0.6453, Train Accuracy: 62.19%\n",
      "Epoch 742/2000, Loss: 0.6453, Train Accuracy: 62.08%\n",
      "Epoch 743/2000, Loss: 0.6452, Train Accuracy: 62.18%\n",
      "Epoch 744/2000, Loss: 0.6452, Train Accuracy: 62.05%\n",
      "Epoch 745/2000, Loss: 0.6452, Train Accuracy: 62.15%\n",
      "Epoch 746/2000, Loss: 0.6452, Train Accuracy: 62.16%\n",
      "Epoch 747/2000, Loss: 0.6452, Train Accuracy: 62.06%\n",
      "Epoch 748/2000, Loss: 0.6452, Train Accuracy: 62.13%\n",
      "Epoch 749/2000, Loss: 0.6452, Train Accuracy: 62.05%\n",
      "Epoch 750/2000, Loss: 0.6452, Train Accuracy: 62.17%\n",
      "Epoch 751/2000, Loss: 0.6452, Train Accuracy: 62.10%\n",
      "Epoch 752/2000, Loss: 0.6452, Train Accuracy: 62.20%\n",
      "Epoch 753/2000, Loss: 0.6451, Train Accuracy: 62.06%\n",
      "Epoch 754/2000, Loss: 0.6451, Train Accuracy: 62.15%\n",
      "Epoch 755/2000, Loss: 0.6451, Train Accuracy: 62.10%\n",
      "Epoch 756/2000, Loss: 0.6451, Train Accuracy: 62.16%\n",
      "Epoch 757/2000, Loss: 0.6451, Train Accuracy: 62.17%\n",
      "Epoch 758/2000, Loss: 0.6451, Train Accuracy: 62.16%\n",
      "Epoch 759/2000, Loss: 0.6451, Train Accuracy: 62.16%\n",
      "Epoch 760/2000, Loss: 0.6451, Train Accuracy: 62.11%\n",
      "Epoch 761/2000, Loss: 0.6451, Train Accuracy: 62.17%\n",
      "Epoch 762/2000, Loss: 0.6451, Train Accuracy: 62.08%\n",
      "Epoch 763/2000, Loss: 0.6450, Train Accuracy: 62.20%\n",
      "Epoch 764/2000, Loss: 0.6450, Train Accuracy: 62.08%\n",
      "Epoch 765/2000, Loss: 0.6450, Train Accuracy: 62.20%\n",
      "Epoch 766/2000, Loss: 0.6450, Train Accuracy: 62.09%\n",
      "Epoch 767/2000, Loss: 0.6450, Train Accuracy: 62.18%\n",
      "Epoch 768/2000, Loss: 0.6450, Train Accuracy: 62.09%\n",
      "Epoch 769/2000, Loss: 0.6450, Train Accuracy: 62.17%\n",
      "Epoch 770/2000, Loss: 0.6450, Train Accuracy: 62.09%\n",
      "Epoch 771/2000, Loss: 0.6450, Train Accuracy: 62.20%\n",
      "Epoch 772/2000, Loss: 0.6450, Train Accuracy: 62.08%\n",
      "Epoch 773/2000, Loss: 0.6450, Train Accuracy: 62.22%\n",
      "Epoch 774/2000, Loss: 0.6449, Train Accuracy: 62.10%\n",
      "Epoch 775/2000, Loss: 0.6449, Train Accuracy: 62.24%\n",
      "Epoch 776/2000, Loss: 0.6449, Train Accuracy: 62.10%\n",
      "Epoch 777/2000, Loss: 0.6449, Train Accuracy: 62.20%\n",
      "Epoch 778/2000, Loss: 0.6449, Train Accuracy: 62.09%\n",
      "Epoch 779/2000, Loss: 0.6449, Train Accuracy: 62.17%\n",
      "Epoch 780/2000, Loss: 0.6449, Train Accuracy: 62.10%\n",
      "Epoch 781/2000, Loss: 0.6449, Train Accuracy: 62.25%\n",
      "Epoch 782/2000, Loss: 0.6449, Train Accuracy: 62.06%\n",
      "Epoch 783/2000, Loss: 0.6449, Train Accuracy: 62.22%\n",
      "Epoch 784/2000, Loss: 0.6449, Train Accuracy: 62.16%\n",
      "Epoch 785/2000, Loss: 0.6449, Train Accuracy: 62.24%\n",
      "Epoch 786/2000, Loss: 0.6450, Train Accuracy: 62.15%\n",
      "Epoch 787/2000, Loss: 0.6450, Train Accuracy: 62.28%\n",
      "Epoch 788/2000, Loss: 0.6450, Train Accuracy: 62.16%\n",
      "Epoch 789/2000, Loss: 0.6450, Train Accuracy: 62.29%\n",
      "Epoch 790/2000, Loss: 0.6450, Train Accuracy: 62.18%\n",
      "Epoch 791/2000, Loss: 0.6449, Train Accuracy: 62.27%\n",
      "Epoch 792/2000, Loss: 0.6448, Train Accuracy: 62.14%\n",
      "Epoch 793/2000, Loss: 0.6448, Train Accuracy: 62.22%\n",
      "Epoch 794/2000, Loss: 0.6447, Train Accuracy: 62.20%\n",
      "Epoch 795/2000, Loss: 0.6447, Train Accuracy: 62.18%\n",
      "Epoch 796/2000, Loss: 0.6447, Train Accuracy: 62.28%\n",
      "Epoch 797/2000, Loss: 0.6447, Train Accuracy: 62.13%\n",
      "Epoch 798/2000, Loss: 0.6448, Train Accuracy: 62.25%\n",
      "Epoch 799/2000, Loss: 0.6448, Train Accuracy: 62.15%\n",
      "Epoch 800/2000, Loss: 0.6448, Train Accuracy: 62.22%\n",
      "Epoch 801/2000, Loss: 0.6448, Train Accuracy: 62.19%\n",
      "Epoch 802/2000, Loss: 0.6448, Train Accuracy: 62.18%\n",
      "Epoch 803/2000, Loss: 0.6447, Train Accuracy: 62.09%\n",
      "Epoch 804/2000, Loss: 0.6447, Train Accuracy: 62.23%\n",
      "Epoch 805/2000, Loss: 0.6447, Train Accuracy: 62.11%\n",
      "Epoch 806/2000, Loss: 0.6446, Train Accuracy: 62.28%\n",
      "Epoch 807/2000, Loss: 0.6446, Train Accuracy: 62.23%\n",
      "Epoch 808/2000, Loss: 0.6446, Train Accuracy: 62.24%\n",
      "Epoch 809/2000, Loss: 0.6446, Train Accuracy: 62.28%\n",
      "Epoch 810/2000, Loss: 0.6446, Train Accuracy: 62.17%\n",
      "Epoch 811/2000, Loss: 0.6446, Train Accuracy: 62.20%\n",
      "Epoch 812/2000, Loss: 0.6446, Train Accuracy: 62.09%\n",
      "Epoch 813/2000, Loss: 0.6446, Train Accuracy: 62.27%\n",
      "Epoch 814/2000, Loss: 0.6446, Train Accuracy: 62.09%\n",
      "Epoch 815/2000, Loss: 0.6446, Train Accuracy: 62.29%\n",
      "Epoch 816/2000, Loss: 0.6446, Train Accuracy: 62.09%\n",
      "Epoch 817/2000, Loss: 0.6446, Train Accuracy: 62.28%\n",
      "Epoch 818/2000, Loss: 0.6446, Train Accuracy: 62.10%\n",
      "Epoch 819/2000, Loss: 0.6446, Train Accuracy: 62.23%\n",
      "Epoch 820/2000, Loss: 0.6445, Train Accuracy: 62.15%\n",
      "Epoch 821/2000, Loss: 0.6445, Train Accuracy: 62.20%\n",
      "Epoch 822/2000, Loss: 0.6445, Train Accuracy: 62.22%\n",
      "Epoch 823/2000, Loss: 0.6445, Train Accuracy: 62.25%\n",
      "Epoch 824/2000, Loss: 0.6445, Train Accuracy: 62.22%\n",
      "Epoch 825/2000, Loss: 0.6445, Train Accuracy: 62.28%\n",
      "Epoch 826/2000, Loss: 0.6445, Train Accuracy: 62.22%\n",
      "Epoch 827/2000, Loss: 0.6444, Train Accuracy: 62.27%\n",
      "Epoch 828/2000, Loss: 0.6444, Train Accuracy: 62.23%\n",
      "Epoch 829/2000, Loss: 0.6444, Train Accuracy: 62.22%\n",
      "Epoch 830/2000, Loss: 0.6444, Train Accuracy: 62.25%\n",
      "Epoch 831/2000, Loss: 0.6444, Train Accuracy: 62.23%\n",
      "Epoch 832/2000, Loss: 0.6444, Train Accuracy: 62.31%\n",
      "Epoch 833/2000, Loss: 0.6444, Train Accuracy: 62.25%\n",
      "Epoch 834/2000, Loss: 0.6444, Train Accuracy: 62.28%\n",
      "Epoch 835/2000, Loss: 0.6444, Train Accuracy: 62.21%\n",
      "Epoch 836/2000, Loss: 0.6444, Train Accuracy: 62.23%\n",
      "Epoch 837/2000, Loss: 0.6444, Train Accuracy: 62.20%\n",
      "Epoch 838/2000, Loss: 0.6444, Train Accuracy: 62.24%\n",
      "Epoch 839/2000, Loss: 0.6444, Train Accuracy: 62.14%\n",
      "Epoch 840/2000, Loss: 0.6444, Train Accuracy: 62.20%\n",
      "Epoch 841/2000, Loss: 0.6444, Train Accuracy: 62.17%\n",
      "Epoch 842/2000, Loss: 0.6445, Train Accuracy: 62.28%\n",
      "Epoch 843/2000, Loss: 0.6446, Train Accuracy: 62.23%\n",
      "Epoch 844/2000, Loss: 0.6447, Train Accuracy: 62.27%\n",
      "Epoch 845/2000, Loss: 0.6449, Train Accuracy: 62.13%\n",
      "Epoch 846/2000, Loss: 0.6449, Train Accuracy: 62.12%\n",
      "Epoch 847/2000, Loss: 0.6448, Train Accuracy: 62.13%\n",
      "Epoch 848/2000, Loss: 0.6446, Train Accuracy: 62.40%\n",
      "Epoch 849/2000, Loss: 0.6443, Train Accuracy: 62.17%\n",
      "Epoch 850/2000, Loss: 0.6443, Train Accuracy: 62.22%\n",
      "Epoch 851/2000, Loss: 0.6444, Train Accuracy: 62.33%\n",
      "Epoch 852/2000, Loss: 0.6445, Train Accuracy: 62.25%\n",
      "Epoch 853/2000, Loss: 0.6445, Train Accuracy: 62.31%\n",
      "Epoch 854/2000, Loss: 0.6444, Train Accuracy: 62.25%\n",
      "Epoch 855/2000, Loss: 0.6442, Train Accuracy: 62.25%\n",
      "Epoch 856/2000, Loss: 0.6442, Train Accuracy: 62.23%\n",
      "Epoch 857/2000, Loss: 0.6443, Train Accuracy: 62.16%\n",
      "Epoch 858/2000, Loss: 0.6444, Train Accuracy: 62.31%\n",
      "Epoch 859/2000, Loss: 0.6443, Train Accuracy: 62.22%\n",
      "Epoch 860/2000, Loss: 0.6442, Train Accuracy: 62.26%\n",
      "Epoch 861/2000, Loss: 0.6442, Train Accuracy: 62.22%\n",
      "Epoch 862/2000, Loss: 0.6442, Train Accuracy: 62.21%\n",
      "Epoch 863/2000, Loss: 0.6442, Train Accuracy: 62.22%\n",
      "Epoch 864/2000, Loss: 0.6442, Train Accuracy: 62.14%\n",
      "Epoch 865/2000, Loss: 0.6442, Train Accuracy: 62.28%\n",
      "Epoch 866/2000, Loss: 0.6442, Train Accuracy: 62.13%\n",
      "Epoch 867/2000, Loss: 0.6441, Train Accuracy: 62.30%\n",
      "Epoch 868/2000, Loss: 0.6441, Train Accuracy: 62.28%\n",
      "Epoch 869/2000, Loss: 0.6441, Train Accuracy: 62.15%\n",
      "Epoch 870/2000, Loss: 0.6441, Train Accuracy: 62.22%\n",
      "Epoch 871/2000, Loss: 0.6441, Train Accuracy: 62.15%\n",
      "Epoch 872/2000, Loss: 0.6441, Train Accuracy: 62.23%\n",
      "Epoch 873/2000, Loss: 0.6441, Train Accuracy: 62.30%\n",
      "Epoch 874/2000, Loss: 0.6441, Train Accuracy: 62.25%\n",
      "Epoch 875/2000, Loss: 0.6441, Train Accuracy: 62.24%\n",
      "Epoch 876/2000, Loss: 0.6441, Train Accuracy: 62.16%\n",
      "Epoch 877/2000, Loss: 0.6441, Train Accuracy: 62.21%\n",
      "Epoch 878/2000, Loss: 0.6440, Train Accuracy: 62.18%\n",
      "Epoch 879/2000, Loss: 0.6440, Train Accuracy: 62.27%\n",
      "Epoch 880/2000, Loss: 0.6440, Train Accuracy: 62.33%\n",
      "Epoch 881/2000, Loss: 0.6440, Train Accuracy: 62.25%\n",
      "Epoch 882/2000, Loss: 0.6440, Train Accuracy: 62.28%\n",
      "Epoch 883/2000, Loss: 0.6440, Train Accuracy: 62.18%\n",
      "Epoch 884/2000, Loss: 0.6440, Train Accuracy: 62.29%\n",
      "Epoch 885/2000, Loss: 0.6440, Train Accuracy: 62.22%\n",
      "Epoch 886/2000, Loss: 0.6440, Train Accuracy: 62.28%\n",
      "Epoch 887/2000, Loss: 0.6439, Train Accuracy: 62.33%\n",
      "Epoch 888/2000, Loss: 0.6439, Train Accuracy: 62.23%\n",
      "Epoch 889/2000, Loss: 0.6439, Train Accuracy: 62.27%\n",
      "Epoch 890/2000, Loss: 0.6439, Train Accuracy: 62.22%\n",
      "Epoch 891/2000, Loss: 0.6439, Train Accuracy: 62.27%\n",
      "Epoch 892/2000, Loss: 0.6439, Train Accuracy: 62.25%\n",
      "Epoch 893/2000, Loss: 0.6439, Train Accuracy: 62.27%\n",
      "Epoch 894/2000, Loss: 0.6439, Train Accuracy: 62.32%\n",
      "Epoch 895/2000, Loss: 0.6439, Train Accuracy: 62.34%\n",
      "Epoch 896/2000, Loss: 0.6439, Train Accuracy: 62.25%\n",
      "Epoch 897/2000, Loss: 0.6439, Train Accuracy: 62.26%\n",
      "Epoch 898/2000, Loss: 0.6439, Train Accuracy: 62.25%\n",
      "Epoch 899/2000, Loss: 0.6439, Train Accuracy: 62.25%\n",
      "Epoch 900/2000, Loss: 0.6438, Train Accuracy: 62.26%\n",
      "Epoch 901/2000, Loss: 0.6438, Train Accuracy: 62.30%\n",
      "Epoch 902/2000, Loss: 0.6438, Train Accuracy: 62.31%\n",
      "Epoch 903/2000, Loss: 0.6438, Train Accuracy: 62.30%\n",
      "Epoch 904/2000, Loss: 0.6438, Train Accuracy: 62.25%\n",
      "Epoch 905/2000, Loss: 0.6438, Train Accuracy: 62.27%\n",
      "Epoch 906/2000, Loss: 0.6438, Train Accuracy: 62.33%\n",
      "Epoch 907/2000, Loss: 0.6438, Train Accuracy: 62.27%\n",
      "Epoch 908/2000, Loss: 0.6438, Train Accuracy: 62.31%\n",
      "Epoch 909/2000, Loss: 0.6438, Train Accuracy: 62.25%\n",
      "Epoch 910/2000, Loss: 0.6438, Train Accuracy: 62.27%\n",
      "Epoch 911/2000, Loss: 0.6438, Train Accuracy: 62.25%\n",
      "Epoch 912/2000, Loss: 0.6437, Train Accuracy: 62.29%\n",
      "Epoch 913/2000, Loss: 0.6437, Train Accuracy: 62.25%\n",
      "Epoch 914/2000, Loss: 0.6437, Train Accuracy: 62.30%\n",
      "Epoch 915/2000, Loss: 0.6437, Train Accuracy: 62.27%\n",
      "Epoch 916/2000, Loss: 0.6437, Train Accuracy: 62.27%\n",
      "Epoch 917/2000, Loss: 0.6437, Train Accuracy: 62.26%\n",
      "Epoch 918/2000, Loss: 0.6437, Train Accuracy: 62.27%\n",
      "Epoch 919/2000, Loss: 0.6437, Train Accuracy: 62.29%\n",
      "Epoch 920/2000, Loss: 0.6437, Train Accuracy: 62.24%\n",
      "Epoch 921/2000, Loss: 0.6437, Train Accuracy: 62.30%\n",
      "Epoch 922/2000, Loss: 0.6437, Train Accuracy: 62.22%\n",
      "Epoch 923/2000, Loss: 0.6437, Train Accuracy: 62.28%\n",
      "Epoch 924/2000, Loss: 0.6437, Train Accuracy: 62.23%\n",
      "Epoch 925/2000, Loss: 0.6437, Train Accuracy: 62.27%\n",
      "Epoch 926/2000, Loss: 0.6437, Train Accuracy: 62.23%\n",
      "Epoch 927/2000, Loss: 0.6437, Train Accuracy: 62.29%\n",
      "Epoch 928/2000, Loss: 0.6437, Train Accuracy: 62.23%\n",
      "Epoch 929/2000, Loss: 0.6437, Train Accuracy: 62.34%\n",
      "Epoch 930/2000, Loss: 0.6437, Train Accuracy: 62.25%\n",
      "Epoch 931/2000, Loss: 0.6437, Train Accuracy: 62.33%\n",
      "Epoch 932/2000, Loss: 0.6438, Train Accuracy: 62.22%\n",
      "Epoch 933/2000, Loss: 0.6438, Train Accuracy: 62.33%\n",
      "Epoch 934/2000, Loss: 0.6438, Train Accuracy: 62.25%\n",
      "Epoch 935/2000, Loss: 0.6438, Train Accuracy: 62.34%\n",
      "Epoch 936/2000, Loss: 0.6438, Train Accuracy: 62.27%\n",
      "Epoch 937/2000, Loss: 0.6437, Train Accuracy: 62.33%\n",
      "Epoch 938/2000, Loss: 0.6436, Train Accuracy: 62.25%\n",
      "Epoch 939/2000, Loss: 0.6436, Train Accuracy: 62.26%\n",
      "Epoch 940/2000, Loss: 0.6435, Train Accuracy: 62.30%\n",
      "Epoch 941/2000, Loss: 0.6435, Train Accuracy: 62.26%\n",
      "Epoch 942/2000, Loss: 0.6435, Train Accuracy: 62.30%\n",
      "Epoch 943/2000, Loss: 0.6436, Train Accuracy: 62.27%\n",
      "Epoch 944/2000, Loss: 0.6436, Train Accuracy: 62.37%\n",
      "Epoch 945/2000, Loss: 0.6436, Train Accuracy: 62.27%\n",
      "Epoch 946/2000, Loss: 0.6436, Train Accuracy: 62.36%\n",
      "Epoch 947/2000, Loss: 0.6436, Train Accuracy: 62.30%\n",
      "Epoch 948/2000, Loss: 0.6435, Train Accuracy: 62.29%\n",
      "Epoch 949/2000, Loss: 0.6435, Train Accuracy: 62.22%\n",
      "Epoch 950/2000, Loss: 0.6434, Train Accuracy: 62.32%\n",
      "Epoch 951/2000, Loss: 0.6434, Train Accuracy: 62.32%\n",
      "Epoch 952/2000, Loss: 0.6434, Train Accuracy: 62.22%\n",
      "Epoch 953/2000, Loss: 0.6435, Train Accuracy: 62.28%\n",
      "Epoch 954/2000, Loss: 0.6435, Train Accuracy: 62.29%\n",
      "Epoch 955/2000, Loss: 0.6435, Train Accuracy: 62.27%\n",
      "Epoch 956/2000, Loss: 0.6434, Train Accuracy: 62.32%\n",
      "Epoch 957/2000, Loss: 0.6434, Train Accuracy: 62.28%\n",
      "Epoch 958/2000, Loss: 0.6434, Train Accuracy: 62.25%\n",
      "Epoch 959/2000, Loss: 0.6434, Train Accuracy: 62.28%\n",
      "Epoch 960/2000, Loss: 0.6434, Train Accuracy: 62.33%\n",
      "Epoch 961/2000, Loss: 0.6433, Train Accuracy: 62.28%\n",
      "Epoch 962/2000, Loss: 0.6433, Train Accuracy: 62.35%\n",
      "Epoch 963/2000, Loss: 0.6433, Train Accuracy: 62.28%\n",
      "Epoch 964/2000, Loss: 0.6433, Train Accuracy: 62.30%\n",
      "Epoch 965/2000, Loss: 0.6433, Train Accuracy: 62.27%\n",
      "Epoch 966/2000, Loss: 0.6433, Train Accuracy: 62.26%\n",
      "Epoch 967/2000, Loss: 0.6433, Train Accuracy: 62.32%\n",
      "Epoch 968/2000, Loss: 0.6433, Train Accuracy: 62.28%\n",
      "Epoch 969/2000, Loss: 0.6433, Train Accuracy: 62.29%\n",
      "Epoch 970/2000, Loss: 0.6433, Train Accuracy: 62.26%\n",
      "Epoch 971/2000, Loss: 0.6433, Train Accuracy: 62.29%\n",
      "Epoch 972/2000, Loss: 0.6433, Train Accuracy: 62.26%\n",
      "Epoch 973/2000, Loss: 0.6433, Train Accuracy: 62.30%\n",
      "Epoch 974/2000, Loss: 0.6433, Train Accuracy: 62.31%\n",
      "Epoch 975/2000, Loss: 0.6433, Train Accuracy: 62.28%\n",
      "Epoch 976/2000, Loss: 0.6432, Train Accuracy: 62.28%\n",
      "Epoch 977/2000, Loss: 0.6432, Train Accuracy: 62.37%\n",
      "Epoch 978/2000, Loss: 0.6432, Train Accuracy: 62.32%\n",
      "Epoch 979/2000, Loss: 0.6432, Train Accuracy: 62.32%\n",
      "Epoch 980/2000, Loss: 0.6432, Train Accuracy: 62.33%\n",
      "Epoch 981/2000, Loss: 0.6432, Train Accuracy: 62.33%\n",
      "Epoch 982/2000, Loss: 0.6432, Train Accuracy: 62.30%\n",
      "Epoch 983/2000, Loss: 0.6432, Train Accuracy: 62.30%\n",
      "Epoch 984/2000, Loss: 0.6432, Train Accuracy: 62.30%\n",
      "Epoch 985/2000, Loss: 0.6432, Train Accuracy: 62.32%\n",
      "Epoch 986/2000, Loss: 0.6432, Train Accuracy: 62.32%\n",
      "Epoch 987/2000, Loss: 0.6431, Train Accuracy: 62.32%\n",
      "Epoch 988/2000, Loss: 0.6431, Train Accuracy: 62.33%\n",
      "Epoch 989/2000, Loss: 0.6431, Train Accuracy: 62.37%\n",
      "Epoch 990/2000, Loss: 0.6431, Train Accuracy: 62.33%\n",
      "Epoch 991/2000, Loss: 0.6431, Train Accuracy: 62.28%\n",
      "Epoch 992/2000, Loss: 0.6431, Train Accuracy: 62.32%\n",
      "Epoch 993/2000, Loss: 0.6431, Train Accuracy: 62.33%\n",
      "Epoch 994/2000, Loss: 0.6431, Train Accuracy: 62.30%\n",
      "Epoch 995/2000, Loss: 0.6432, Train Accuracy: 62.33%\n",
      "Epoch 996/2000, Loss: 0.6432, Train Accuracy: 62.24%\n",
      "Epoch 997/2000, Loss: 0.6433, Train Accuracy: 62.43%\n",
      "Epoch 998/2000, Loss: 0.6434, Train Accuracy: 62.30%\n",
      "Epoch 999/2000, Loss: 0.6435, Train Accuracy: 62.26%\n",
      "Epoch 1000/2000, Loss: 0.6436, Train Accuracy: 62.31%\n",
      "Epoch 1001/2000, Loss: 0.6436, Train Accuracy: 62.26%\n",
      "Epoch 1002/2000, Loss: 0.6435, Train Accuracy: 62.36%\n",
      "Epoch 1003/2000, Loss: 0.6433, Train Accuracy: 62.42%\n",
      "Epoch 1004/2000, Loss: 0.6431, Train Accuracy: 62.30%\n",
      "Epoch 1005/2000, Loss: 0.6430, Train Accuracy: 62.30%\n",
      "Epoch 1006/2000, Loss: 0.6431, Train Accuracy: 62.34%\n",
      "Epoch 1007/2000, Loss: 0.6432, Train Accuracy: 62.32%\n",
      "Epoch 1008/2000, Loss: 0.6433, Train Accuracy: 62.38%\n",
      "Epoch 1009/2000, Loss: 0.6432, Train Accuracy: 62.33%\n",
      "Epoch 1010/2000, Loss: 0.6431, Train Accuracy: 62.36%\n",
      "Epoch 1011/2000, Loss: 0.6430, Train Accuracy: 62.36%\n",
      "Epoch 1012/2000, Loss: 0.6430, Train Accuracy: 62.35%\n",
      "Epoch 1013/2000, Loss: 0.6430, Train Accuracy: 62.38%\n",
      "Epoch 1014/2000, Loss: 0.6431, Train Accuracy: 62.22%\n",
      "Epoch 1015/2000, Loss: 0.6431, Train Accuracy: 62.44%\n",
      "Epoch 1016/2000, Loss: 0.6430, Train Accuracy: 62.30%\n",
      "Epoch 1017/2000, Loss: 0.6430, Train Accuracy: 62.34%\n",
      "Epoch 1018/2000, Loss: 0.6429, Train Accuracy: 62.34%\n",
      "Epoch 1019/2000, Loss: 0.6429, Train Accuracy: 62.35%\n",
      "Epoch 1020/2000, Loss: 0.6430, Train Accuracy: 62.37%\n",
      "Epoch 1021/2000, Loss: 0.6430, Train Accuracy: 62.27%\n",
      "Epoch 1022/2000, Loss: 0.6430, Train Accuracy: 62.35%\n",
      "Epoch 1023/2000, Loss: 0.6429, Train Accuracy: 62.28%\n",
      "Epoch 1024/2000, Loss: 0.6429, Train Accuracy: 62.36%\n",
      "Epoch 1025/2000, Loss: 0.6429, Train Accuracy: 62.35%\n",
      "Epoch 1026/2000, Loss: 0.6429, Train Accuracy: 62.36%\n",
      "Epoch 1027/2000, Loss: 0.6429, Train Accuracy: 62.38%\n",
      "Epoch 1028/2000, Loss: 0.6429, Train Accuracy: 62.32%\n",
      "Epoch 1029/2000, Loss: 0.6429, Train Accuracy: 62.37%\n",
      "Epoch 1030/2000, Loss: 0.6429, Train Accuracy: 62.36%\n",
      "Epoch 1031/2000, Loss: 0.6428, Train Accuracy: 62.38%\n",
      "Epoch 1032/2000, Loss: 0.6428, Train Accuracy: 62.35%\n",
      "Epoch 1033/2000, Loss: 0.6428, Train Accuracy: 62.39%\n",
      "Epoch 1034/2000, Loss: 0.6428, Train Accuracy: 62.38%\n",
      "Epoch 1035/2000, Loss: 0.6428, Train Accuracy: 62.40%\n",
      "Epoch 1036/2000, Loss: 0.6428, Train Accuracy: 62.39%\n",
      "Epoch 1037/2000, Loss: 0.6428, Train Accuracy: 62.43%\n",
      "Epoch 1038/2000, Loss: 0.6428, Train Accuracy: 62.42%\n",
      "Epoch 1039/2000, Loss: 0.6428, Train Accuracy: 62.40%\n",
      "Epoch 1040/2000, Loss: 0.6428, Train Accuracy: 62.38%\n",
      "Epoch 1041/2000, Loss: 0.6428, Train Accuracy: 62.40%\n",
      "Epoch 1042/2000, Loss: 0.6427, Train Accuracy: 62.39%\n",
      "Epoch 1043/2000, Loss: 0.6427, Train Accuracy: 62.38%\n",
      "Epoch 1044/2000, Loss: 0.6427, Train Accuracy: 62.40%\n",
      "Epoch 1045/2000, Loss: 0.6427, Train Accuracy: 62.40%\n",
      "Epoch 1046/2000, Loss: 0.6427, Train Accuracy: 62.41%\n",
      "Epoch 1047/2000, Loss: 0.6427, Train Accuracy: 62.40%\n",
      "Epoch 1048/2000, Loss: 0.6427, Train Accuracy: 62.43%\n",
      "Epoch 1049/2000, Loss: 0.6427, Train Accuracy: 62.36%\n",
      "Epoch 1050/2000, Loss: 0.6427, Train Accuracy: 62.35%\n",
      "Epoch 1051/2000, Loss: 0.6427, Train Accuracy: 62.41%\n",
      "Epoch 1052/2000, Loss: 0.6427, Train Accuracy: 62.42%\n",
      "Epoch 1053/2000, Loss: 0.6427, Train Accuracy: 62.44%\n",
      "Epoch 1054/2000, Loss: 0.6427, Train Accuracy: 62.40%\n",
      "Epoch 1055/2000, Loss: 0.6427, Train Accuracy: 62.39%\n",
      "Epoch 1056/2000, Loss: 0.6427, Train Accuracy: 62.38%\n",
      "Epoch 1057/2000, Loss: 0.6426, Train Accuracy: 62.37%\n",
      "Epoch 1058/2000, Loss: 0.6426, Train Accuracy: 62.36%\n",
      "Epoch 1059/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1060/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1061/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1062/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1063/2000, Loss: 0.6426, Train Accuracy: 62.41%\n",
      "Epoch 1064/2000, Loss: 0.6426, Train Accuracy: 62.42%\n",
      "Epoch 1065/2000, Loss: 0.6426, Train Accuracy: 62.42%\n",
      "Epoch 1066/2000, Loss: 0.6426, Train Accuracy: 62.38%\n",
      "Epoch 1067/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1068/2000, Loss: 0.6426, Train Accuracy: 62.39%\n",
      "Epoch 1069/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1070/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1071/2000, Loss: 0.6426, Train Accuracy: 62.36%\n",
      "Epoch 1072/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1073/2000, Loss: 0.6426, Train Accuracy: 62.32%\n",
      "Epoch 1074/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1075/2000, Loss: 0.6426, Train Accuracy: 62.33%\n",
      "Epoch 1076/2000, Loss: 0.6426, Train Accuracy: 62.40%\n",
      "Epoch 1077/2000, Loss: 0.6426, Train Accuracy: 62.34%\n",
      "Epoch 1078/2000, Loss: 0.6426, Train Accuracy: 62.45%\n",
      "Epoch 1079/2000, Loss: 0.6426, Train Accuracy: 62.37%\n",
      "Epoch 1080/2000, Loss: 0.6426, Train Accuracy: 62.45%\n",
      "Epoch 1081/2000, Loss: 0.6426, Train Accuracy: 62.35%\n",
      "Epoch 1082/2000, Loss: 0.6426, Train Accuracy: 62.45%\n",
      "Epoch 1083/2000, Loss: 0.6425, Train Accuracy: 62.35%\n",
      "Epoch 1084/2000, Loss: 0.6425, Train Accuracy: 62.43%\n",
      "Epoch 1085/2000, Loss: 0.6425, Train Accuracy: 62.42%\n",
      "Epoch 1086/2000, Loss: 0.6425, Train Accuracy: 62.45%\n",
      "Epoch 1087/2000, Loss: 0.6424, Train Accuracy: 62.44%\n",
      "Epoch 1088/2000, Loss: 0.6424, Train Accuracy: 62.46%\n",
      "Epoch 1089/2000, Loss: 0.6424, Train Accuracy: 62.42%\n",
      "Epoch 1090/2000, Loss: 0.6424, Train Accuracy: 62.42%\n",
      "Epoch 1091/2000, Loss: 0.6424, Train Accuracy: 62.43%\n",
      "Epoch 1092/2000, Loss: 0.6424, Train Accuracy: 62.40%\n",
      "Epoch 1093/2000, Loss: 0.6424, Train Accuracy: 62.40%\n",
      "Epoch 1094/2000, Loss: 0.6425, Train Accuracy: 62.37%\n",
      "Epoch 1095/2000, Loss: 0.6425, Train Accuracy: 62.43%\n",
      "Epoch 1096/2000, Loss: 0.6425, Train Accuracy: 62.38%\n",
      "Epoch 1097/2000, Loss: 0.6425, Train Accuracy: 62.45%\n",
      "Epoch 1098/2000, Loss: 0.6425, Train Accuracy: 62.38%\n",
      "Epoch 1099/2000, Loss: 0.6425, Train Accuracy: 62.43%\n",
      "Epoch 1100/2000, Loss: 0.6425, Train Accuracy: 62.40%\n",
      "Epoch 1101/2000, Loss: 0.6424, Train Accuracy: 62.46%\n",
      "Epoch 1102/2000, Loss: 0.6424, Train Accuracy: 62.36%\n",
      "Epoch 1103/2000, Loss: 0.6424, Train Accuracy: 62.43%\n",
      "Epoch 1104/2000, Loss: 0.6423, Train Accuracy: 62.43%\n",
      "Epoch 1105/2000, Loss: 0.6423, Train Accuracy: 62.45%\n",
      "Epoch 1106/2000, Loss: 0.6423, Train Accuracy: 62.48%\n",
      "Epoch 1107/2000, Loss: 0.6423, Train Accuracy: 62.43%\n",
      "Epoch 1108/2000, Loss: 0.6423, Train Accuracy: 62.50%\n",
      "Epoch 1109/2000, Loss: 0.6423, Train Accuracy: 62.42%\n",
      "Epoch 1110/2000, Loss: 0.6423, Train Accuracy: 62.43%\n",
      "Epoch 1111/2000, Loss: 0.6423, Train Accuracy: 62.43%\n",
      "Epoch 1112/2000, Loss: 0.6424, Train Accuracy: 62.42%\n",
      "Epoch 1113/2000, Loss: 0.6424, Train Accuracy: 62.38%\n",
      "Epoch 1114/2000, Loss: 0.6424, Train Accuracy: 62.45%\n",
      "Epoch 1115/2000, Loss: 0.6424, Train Accuracy: 62.40%\n",
      "Epoch 1116/2000, Loss: 0.6423, Train Accuracy: 62.45%\n",
      "Epoch 1117/2000, Loss: 0.6423, Train Accuracy: 62.40%\n",
      "Epoch 1118/2000, Loss: 0.6423, Train Accuracy: 62.45%\n",
      "Epoch 1119/2000, Loss: 0.6423, Train Accuracy: 62.41%\n",
      "Epoch 1120/2000, Loss: 0.6422, Train Accuracy: 62.46%\n",
      "Epoch 1121/2000, Loss: 0.6422, Train Accuracy: 62.47%\n",
      "Epoch 1122/2000, Loss: 0.6422, Train Accuracy: 62.47%\n",
      "Epoch 1123/2000, Loss: 0.6422, Train Accuracy: 62.47%\n",
      "Epoch 1124/2000, Loss: 0.6422, Train Accuracy: 62.47%\n",
      "Epoch 1125/2000, Loss: 0.6422, Train Accuracy: 62.48%\n",
      "Epoch 1126/2000, Loss: 0.6422, Train Accuracy: 62.50%\n",
      "Epoch 1127/2000, Loss: 0.6422, Train Accuracy: 62.52%\n",
      "Epoch 1128/2000, Loss: 0.6422, Train Accuracy: 62.48%\n",
      "Epoch 1129/2000, Loss: 0.6422, Train Accuracy: 62.47%\n",
      "Epoch 1130/2000, Loss: 0.6422, Train Accuracy: 62.42%\n",
      "Epoch 1131/2000, Loss: 0.6422, Train Accuracy: 62.50%\n",
      "Epoch 1132/2000, Loss: 0.6422, Train Accuracy: 62.43%\n",
      "Epoch 1133/2000, Loss: 0.6423, Train Accuracy: 62.47%\n",
      "Epoch 1134/2000, Loss: 0.6423, Train Accuracy: 62.41%\n",
      "Epoch 1135/2000, Loss: 0.6423, Train Accuracy: 62.44%\n",
      "Epoch 1136/2000, Loss: 0.6423, Train Accuracy: 62.43%\n",
      "Epoch 1137/2000, Loss: 0.6422, Train Accuracy: 62.41%\n",
      "Epoch 1138/2000, Loss: 0.6422, Train Accuracy: 62.37%\n",
      "Epoch 1139/2000, Loss: 0.6422, Train Accuracy: 62.50%\n",
      "Epoch 1140/2000, Loss: 0.6421, Train Accuracy: 62.45%\n",
      "Epoch 1141/2000, Loss: 0.6421, Train Accuracy: 62.47%\n",
      "Epoch 1142/2000, Loss: 0.6421, Train Accuracy: 62.55%\n",
      "Epoch 1143/2000, Loss: 0.6421, Train Accuracy: 62.47%\n",
      "Epoch 1144/2000, Loss: 0.6421, Train Accuracy: 62.52%\n",
      "Epoch 1145/2000, Loss: 0.6420, Train Accuracy: 62.52%\n",
      "Epoch 1146/2000, Loss: 0.6421, Train Accuracy: 62.50%\n",
      "Epoch 1147/2000, Loss: 0.6421, Train Accuracy: 62.54%\n",
      "Epoch 1148/2000, Loss: 0.6421, Train Accuracy: 62.46%\n",
      "Epoch 1149/2000, Loss: 0.6421, Train Accuracy: 62.43%\n",
      "Epoch 1150/2000, Loss: 0.6421, Train Accuracy: 62.48%\n",
      "Epoch 1151/2000, Loss: 0.6421, Train Accuracy: 62.42%\n",
      "Epoch 1152/2000, Loss: 0.6421, Train Accuracy: 62.47%\n",
      "Epoch 1153/2000, Loss: 0.6421, Train Accuracy: 62.45%\n",
      "Epoch 1154/2000, Loss: 0.6421, Train Accuracy: 62.48%\n",
      "Epoch 1155/2000, Loss: 0.6421, Train Accuracy: 62.43%\n",
      "Epoch 1156/2000, Loss: 0.6421, Train Accuracy: 62.47%\n",
      "Epoch 1157/2000, Loss: 0.6421, Train Accuracy: 62.44%\n",
      "Epoch 1158/2000, Loss: 0.6420, Train Accuracy: 62.46%\n",
      "Epoch 1159/2000, Loss: 0.6420, Train Accuracy: 62.59%\n",
      "Epoch 1160/2000, Loss: 0.6420, Train Accuracy: 62.48%\n",
      "Epoch 1161/2000, Loss: 0.6419, Train Accuracy: 62.52%\n",
      "Epoch 1162/2000, Loss: 0.6419, Train Accuracy: 62.54%\n",
      "Epoch 1163/2000, Loss: 0.6419, Train Accuracy: 62.47%\n",
      "Epoch 1164/2000, Loss: 0.6420, Train Accuracy: 62.58%\n",
      "Epoch 1165/2000, Loss: 0.6420, Train Accuracy: 62.48%\n",
      "Epoch 1166/2000, Loss: 0.6420, Train Accuracy: 62.43%\n",
      "Epoch 1167/2000, Loss: 0.6420, Train Accuracy: 62.48%\n",
      "Epoch 1168/2000, Loss: 0.6420, Train Accuracy: 62.45%\n",
      "Epoch 1169/2000, Loss: 0.6420, Train Accuracy: 62.50%\n",
      "Epoch 1170/2000, Loss: 0.6420, Train Accuracy: 62.43%\n",
      "Epoch 1171/2000, Loss: 0.6420, Train Accuracy: 62.50%\n",
      "Epoch 1172/2000, Loss: 0.6420, Train Accuracy: 62.48%\n",
      "Epoch 1173/2000, Loss: 0.6419, Train Accuracy: 62.52%\n",
      "Epoch 1174/2000, Loss: 0.6419, Train Accuracy: 62.52%\n",
      "Epoch 1175/2000, Loss: 0.6419, Train Accuracy: 62.54%\n",
      "Epoch 1176/2000, Loss: 0.6419, Train Accuracy: 62.60%\n",
      "Epoch 1177/2000, Loss: 0.6418, Train Accuracy: 62.53%\n",
      "Epoch 1178/2000, Loss: 0.6418, Train Accuracy: 62.54%\n",
      "Epoch 1179/2000, Loss: 0.6418, Train Accuracy: 62.53%\n",
      "Epoch 1180/2000, Loss: 0.6418, Train Accuracy: 62.54%\n",
      "Epoch 1181/2000, Loss: 0.6418, Train Accuracy: 62.58%\n",
      "Epoch 1182/2000, Loss: 0.6418, Train Accuracy: 62.46%\n",
      "Epoch 1183/2000, Loss: 0.6418, Train Accuracy: 62.61%\n",
      "Epoch 1184/2000, Loss: 0.6418, Train Accuracy: 62.52%\n",
      "Epoch 1185/2000, Loss: 0.6418, Train Accuracy: 62.55%\n",
      "Epoch 1186/2000, Loss: 0.6418, Train Accuracy: 62.54%\n",
      "Epoch 1187/2000, Loss: 0.6419, Train Accuracy: 62.47%\n",
      "Epoch 1188/2000, Loss: 0.6419, Train Accuracy: 62.55%\n",
      "Epoch 1189/2000, Loss: 0.6419, Train Accuracy: 62.49%\n",
      "Epoch 1190/2000, Loss: 0.6419, Train Accuracy: 62.43%\n",
      "Epoch 1191/2000, Loss: 0.6420, Train Accuracy: 62.39%\n",
      "Epoch 1192/2000, Loss: 0.6420, Train Accuracy: 62.48%\n",
      "Epoch 1193/2000, Loss: 0.6420, Train Accuracy: 62.35%\n",
      "Epoch 1194/2000, Loss: 0.6419, Train Accuracy: 62.47%\n",
      "Epoch 1195/2000, Loss: 0.6419, Train Accuracy: 62.47%\n",
      "Epoch 1196/2000, Loss: 0.6418, Train Accuracy: 62.55%\n",
      "Epoch 1197/2000, Loss: 0.6418, Train Accuracy: 62.54%\n",
      "Epoch 1198/2000, Loss: 0.6417, Train Accuracy: 62.53%\n",
      "Epoch 1199/2000, Loss: 0.6417, Train Accuracy: 62.55%\n",
      "Epoch 1200/2000, Loss: 0.6417, Train Accuracy: 62.57%\n",
      "Epoch 1201/2000, Loss: 0.6417, Train Accuracy: 62.52%\n",
      "Epoch 1202/2000, Loss: 0.6417, Train Accuracy: 62.57%\n",
      "Epoch 1203/2000, Loss: 0.6417, Train Accuracy: 62.52%\n",
      "Epoch 1204/2000, Loss: 0.6418, Train Accuracy: 62.50%\n",
      "Epoch 1205/2000, Loss: 0.6418, Train Accuracy: 62.55%\n",
      "Epoch 1206/2000, Loss: 0.6418, Train Accuracy: 62.48%\n",
      "Epoch 1207/2000, Loss: 0.6418, Train Accuracy: 62.54%\n",
      "Epoch 1208/2000, Loss: 0.6418, Train Accuracy: 62.50%\n",
      "Epoch 1209/2000, Loss: 0.6417, Train Accuracy: 62.53%\n",
      "Epoch 1210/2000, Loss: 0.6417, Train Accuracy: 62.57%\n",
      "Epoch 1211/2000, Loss: 0.6417, Train Accuracy: 62.52%\n",
      "Epoch 1212/2000, Loss: 0.6416, Train Accuracy: 62.57%\n",
      "Epoch 1213/2000, Loss: 0.6416, Train Accuracy: 62.56%\n",
      "Epoch 1214/2000, Loss: 0.6416, Train Accuracy: 62.55%\n",
      "Epoch 1215/2000, Loss: 0.6416, Train Accuracy: 62.57%\n",
      "Epoch 1216/2000, Loss: 0.6416, Train Accuracy: 62.54%\n",
      "Epoch 1217/2000, Loss: 0.6416, Train Accuracy: 62.59%\n",
      "Epoch 1218/2000, Loss: 0.6416, Train Accuracy: 62.52%\n",
      "Epoch 1219/2000, Loss: 0.6416, Train Accuracy: 62.63%\n",
      "Epoch 1220/2000, Loss: 0.6416, Train Accuracy: 62.52%\n",
      "Epoch 1221/2000, Loss: 0.6416, Train Accuracy: 62.55%\n",
      "Epoch 1222/2000, Loss: 0.6416, Train Accuracy: 62.54%\n",
      "Epoch 1223/2000, Loss: 0.6416, Train Accuracy: 62.53%\n",
      "Epoch 1224/2000, Loss: 0.6416, Train Accuracy: 62.52%\n",
      "Epoch 1225/2000, Loss: 0.6416, Train Accuracy: 62.53%\n",
      "Epoch 1226/2000, Loss: 0.6416, Train Accuracy: 62.51%\n",
      "Epoch 1227/2000, Loss: 0.6416, Train Accuracy: 62.54%\n",
      "Epoch 1228/2000, Loss: 0.6416, Train Accuracy: 62.53%\n",
      "Epoch 1229/2000, Loss: 0.6416, Train Accuracy: 62.57%\n",
      "Epoch 1230/2000, Loss: 0.6416, Train Accuracy: 62.55%\n",
      "Epoch 1231/2000, Loss: 0.6415, Train Accuracy: 62.62%\n",
      "Epoch 1232/2000, Loss: 0.6415, Train Accuracy: 62.52%\n",
      "Epoch 1233/2000, Loss: 0.6415, Train Accuracy: 62.62%\n",
      "Epoch 1234/2000, Loss: 0.6415, Train Accuracy: 62.59%\n",
      "Epoch 1235/2000, Loss: 0.6415, Train Accuracy: 62.61%\n",
      "Epoch 1236/2000, Loss: 0.6415, Train Accuracy: 62.58%\n",
      "Epoch 1237/2000, Loss: 0.6414, Train Accuracy: 62.59%\n",
      "Epoch 1238/2000, Loss: 0.6414, Train Accuracy: 62.58%\n",
      "Epoch 1239/2000, Loss: 0.6414, Train Accuracy: 62.58%\n",
      "Epoch 1240/2000, Loss: 0.6414, Train Accuracy: 62.62%\n",
      "Epoch 1241/2000, Loss: 0.6414, Train Accuracy: 62.60%\n",
      "Epoch 1242/2000, Loss: 0.6414, Train Accuracy: 62.65%\n",
      "Epoch 1243/2000, Loss: 0.6414, Train Accuracy: 62.54%\n",
      "Epoch 1244/2000, Loss: 0.6414, Train Accuracy: 62.62%\n",
      "Epoch 1245/2000, Loss: 0.6415, Train Accuracy: 62.52%\n",
      "Epoch 1246/2000, Loss: 0.6415, Train Accuracy: 62.53%\n",
      "Epoch 1247/2000, Loss: 0.6415, Train Accuracy: 62.53%\n",
      "Epoch 1248/2000, Loss: 0.6416, Train Accuracy: 62.40%\n",
      "Epoch 1249/2000, Loss: 0.6417, Train Accuracy: 62.50%\n",
      "Epoch 1250/2000, Loss: 0.6417, Train Accuracy: 62.40%\n",
      "Epoch 1251/2000, Loss: 0.6418, Train Accuracy: 62.45%\n",
      "Epoch 1252/2000, Loss: 0.6418, Train Accuracy: 62.38%\n",
      "Epoch 1253/2000, Loss: 0.6417, Train Accuracy: 62.47%\n",
      "Epoch 1254/2000, Loss: 0.6416, Train Accuracy: 62.38%\n",
      "Epoch 1255/2000, Loss: 0.6414, Train Accuracy: 62.52%\n",
      "Epoch 1256/2000, Loss: 0.6413, Train Accuracy: 62.65%\n",
      "Epoch 1257/2000, Loss: 0.6413, Train Accuracy: 62.62%\n",
      "Epoch 1258/2000, Loss: 0.6414, Train Accuracy: 62.62%\n",
      "Epoch 1259/2000, Loss: 0.6414, Train Accuracy: 62.57%\n",
      "Epoch 1260/2000, Loss: 0.6415, Train Accuracy: 62.51%\n",
      "Epoch 1261/2000, Loss: 0.6415, Train Accuracy: 62.39%\n",
      "Epoch 1262/2000, Loss: 0.6415, Train Accuracy: 62.55%\n",
      "Epoch 1263/2000, Loss: 0.6414, Train Accuracy: 62.50%\n",
      "Epoch 1264/2000, Loss: 0.6413, Train Accuracy: 62.55%\n",
      "Epoch 1265/2000, Loss: 0.6413, Train Accuracy: 62.61%\n",
      "Epoch 1266/2000, Loss: 0.6413, Train Accuracy: 62.58%\n",
      "Epoch 1267/2000, Loss: 0.6413, Train Accuracy: 62.59%\n",
      "Epoch 1268/2000, Loss: 0.6413, Train Accuracy: 62.65%\n",
      "Epoch 1269/2000, Loss: 0.6413, Train Accuracy: 62.58%\n",
      "Epoch 1270/2000, Loss: 0.6413, Train Accuracy: 62.62%\n",
      "Epoch 1271/2000, Loss: 0.6413, Train Accuracy: 62.55%\n",
      "Epoch 1272/2000, Loss: 0.6413, Train Accuracy: 62.62%\n",
      "Epoch 1273/2000, Loss: 0.6412, Train Accuracy: 62.57%\n",
      "Epoch 1274/2000, Loss: 0.6412, Train Accuracy: 62.60%\n",
      "Epoch 1275/2000, Loss: 0.6412, Train Accuracy: 62.58%\n",
      "Epoch 1276/2000, Loss: 0.6412, Train Accuracy: 62.57%\n",
      "Epoch 1277/2000, Loss: 0.6412, Train Accuracy: 62.64%\n",
      "Epoch 1278/2000, Loss: 0.6412, Train Accuracy: 62.54%\n",
      "Epoch 1279/2000, Loss: 0.6412, Train Accuracy: 62.63%\n",
      "Epoch 1280/2000, Loss: 0.6412, Train Accuracy: 62.52%\n",
      "Epoch 1281/2000, Loss: 0.6412, Train Accuracy: 62.65%\n",
      "Epoch 1282/2000, Loss: 0.6412, Train Accuracy: 62.55%\n",
      "Epoch 1283/2000, Loss: 0.6412, Train Accuracy: 62.67%\n",
      "Epoch 1284/2000, Loss: 0.6412, Train Accuracy: 62.57%\n",
      "Epoch 1285/2000, Loss: 0.6411, Train Accuracy: 62.60%\n",
      "Epoch 1286/2000, Loss: 0.6411, Train Accuracy: 62.62%\n",
      "Epoch 1287/2000, Loss: 0.6411, Train Accuracy: 62.65%\n",
      "Epoch 1288/2000, Loss: 0.6411, Train Accuracy: 62.59%\n",
      "Epoch 1289/2000, Loss: 0.6411, Train Accuracy: 62.60%\n",
      "Epoch 1290/2000, Loss: 0.6411, Train Accuracy: 62.64%\n",
      "Epoch 1291/2000, Loss: 0.6411, Train Accuracy: 62.60%\n",
      "Epoch 1292/2000, Loss: 0.6411, Train Accuracy: 62.66%\n",
      "Epoch 1293/2000, Loss: 0.6411, Train Accuracy: 62.63%\n",
      "Epoch 1294/2000, Loss: 0.6411, Train Accuracy: 62.63%\n",
      "Epoch 1295/2000, Loss: 0.6411, Train Accuracy: 62.57%\n",
      "Epoch 1296/2000, Loss: 0.6411, Train Accuracy: 62.66%\n",
      "Epoch 1297/2000, Loss: 0.6411, Train Accuracy: 62.52%\n",
      "Epoch 1298/2000, Loss: 0.6411, Train Accuracy: 62.66%\n",
      "Epoch 1299/2000, Loss: 0.6411, Train Accuracy: 62.53%\n",
      "Epoch 1300/2000, Loss: 0.6411, Train Accuracy: 62.63%\n",
      "Epoch 1301/2000, Loss: 0.6411, Train Accuracy: 62.58%\n",
      "Epoch 1302/2000, Loss: 0.6411, Train Accuracy: 62.63%\n",
      "Epoch 1303/2000, Loss: 0.6411, Train Accuracy: 62.52%\n",
      "Epoch 1304/2000, Loss: 0.6411, Train Accuracy: 62.60%\n",
      "Epoch 1305/2000, Loss: 0.6411, Train Accuracy: 62.50%\n",
      "Epoch 1306/2000, Loss: 0.6411, Train Accuracy: 62.60%\n",
      "Epoch 1307/2000, Loss: 0.6411, Train Accuracy: 62.52%\n",
      "Epoch 1308/2000, Loss: 0.6411, Train Accuracy: 62.61%\n",
      "Epoch 1309/2000, Loss: 0.6411, Train Accuracy: 62.53%\n",
      "Epoch 1310/2000, Loss: 0.6411, Train Accuracy: 62.62%\n",
      "Epoch 1311/2000, Loss: 0.6411, Train Accuracy: 62.50%\n",
      "Epoch 1312/2000, Loss: 0.6410, Train Accuracy: 62.61%\n",
      "Epoch 1313/2000, Loss: 0.6410, Train Accuracy: 62.50%\n",
      "Epoch 1314/2000, Loss: 0.6410, Train Accuracy: 62.63%\n",
      "Epoch 1315/2000, Loss: 0.6410, Train Accuracy: 62.56%\n",
      "Epoch 1316/2000, Loss: 0.6410, Train Accuracy: 62.65%\n",
      "Epoch 1317/2000, Loss: 0.6410, Train Accuracy: 62.55%\n",
      "Epoch 1318/2000, Loss: 0.6409, Train Accuracy: 62.66%\n",
      "Epoch 1319/2000, Loss: 0.6409, Train Accuracy: 62.60%\n",
      "Epoch 1320/2000, Loss: 0.6409, Train Accuracy: 62.64%\n",
      "Epoch 1321/2000, Loss: 0.6409, Train Accuracy: 62.60%\n",
      "Epoch 1322/2000, Loss: 0.6409, Train Accuracy: 62.65%\n",
      "Epoch 1323/2000, Loss: 0.6409, Train Accuracy: 62.63%\n",
      "Epoch 1324/2000, Loss: 0.6409, Train Accuracy: 62.67%\n",
      "Epoch 1325/2000, Loss: 0.6409, Train Accuracy: 62.64%\n",
      "Epoch 1326/2000, Loss: 0.6409, Train Accuracy: 62.62%\n",
      "Epoch 1327/2000, Loss: 0.6409, Train Accuracy: 62.63%\n",
      "Epoch 1328/2000, Loss: 0.6409, Train Accuracy: 62.63%\n",
      "Epoch 1329/2000, Loss: 0.6409, Train Accuracy: 62.65%\n",
      "Epoch 1330/2000, Loss: 0.6409, Train Accuracy: 62.61%\n",
      "Epoch 1331/2000, Loss: 0.6408, Train Accuracy: 62.65%\n",
      "Epoch 1332/2000, Loss: 0.6408, Train Accuracy: 62.61%\n",
      "Epoch 1333/2000, Loss: 0.6409, Train Accuracy: 62.68%\n",
      "Epoch 1334/2000, Loss: 0.6409, Train Accuracy: 62.53%\n",
      "Epoch 1335/2000, Loss: 0.6409, Train Accuracy: 62.63%\n",
      "Epoch 1336/2000, Loss: 0.6409, Train Accuracy: 62.61%\n",
      "Epoch 1337/2000, Loss: 0.6410, Train Accuracy: 62.55%\n",
      "Epoch 1338/2000, Loss: 0.6411, Train Accuracy: 62.65%\n",
      "Epoch 1339/2000, Loss: 0.6412, Train Accuracy: 62.38%\n",
      "Epoch 1340/2000, Loss: 0.6414, Train Accuracy: 62.63%\n",
      "Epoch 1341/2000, Loss: 0.6416, Train Accuracy: 62.33%\n",
      "Epoch 1342/2000, Loss: 0.6416, Train Accuracy: 62.64%\n",
      "Epoch 1343/2000, Loss: 0.6414, Train Accuracy: 62.33%\n",
      "Epoch 1344/2000, Loss: 0.6411, Train Accuracy: 62.58%\n",
      "Epoch 1345/2000, Loss: 0.6409, Train Accuracy: 62.64%\n",
      "Epoch 1346/2000, Loss: 0.6408, Train Accuracy: 62.62%\n",
      "Epoch 1347/2000, Loss: 0.6409, Train Accuracy: 62.60%\n",
      "Epoch 1348/2000, Loss: 0.6410, Train Accuracy: 62.47%\n",
      "Epoch 1349/2000, Loss: 0.6411, Train Accuracy: 62.58%\n",
      "Epoch 1350/2000, Loss: 0.6410, Train Accuracy: 62.42%\n",
      "Epoch 1351/2000, Loss: 0.6409, Train Accuracy: 62.61%\n",
      "Epoch 1352/2000, Loss: 0.6407, Train Accuracy: 62.65%\n",
      "Epoch 1353/2000, Loss: 0.6407, Train Accuracy: 62.65%\n",
      "Epoch 1354/2000, Loss: 0.6408, Train Accuracy: 62.58%\n",
      "Epoch 1355/2000, Loss: 0.6409, Train Accuracy: 62.52%\n",
      "Epoch 1356/2000, Loss: 0.6409, Train Accuracy: 62.70%\n",
      "Epoch 1357/2000, Loss: 0.6408, Train Accuracy: 62.56%\n",
      "Epoch 1358/2000, Loss: 0.6407, Train Accuracy: 62.58%\n",
      "Epoch 1359/2000, Loss: 0.6407, Train Accuracy: 62.63%\n",
      "Epoch 1360/2000, Loss: 0.6407, Train Accuracy: 62.62%\n",
      "Epoch 1361/2000, Loss: 0.6407, Train Accuracy: 62.60%\n",
      "Epoch 1362/2000, Loss: 0.6408, Train Accuracy: 62.61%\n",
      "Epoch 1363/2000, Loss: 0.6408, Train Accuracy: 62.61%\n",
      "Epoch 1364/2000, Loss: 0.6407, Train Accuracy: 62.65%\n",
      "Epoch 1365/2000, Loss: 0.6407, Train Accuracy: 62.57%\n",
      "Epoch 1366/2000, Loss: 0.6406, Train Accuracy: 62.59%\n",
      "Epoch 1367/2000, Loss: 0.6406, Train Accuracy: 62.63%\n",
      "Epoch 1368/2000, Loss: 0.6407, Train Accuracy: 62.59%\n",
      "Epoch 1369/2000, Loss: 0.6407, Train Accuracy: 62.67%\n",
      "Epoch 1370/2000, Loss: 0.6407, Train Accuracy: 62.58%\n",
      "Epoch 1371/2000, Loss: 0.6406, Train Accuracy: 62.66%\n",
      "Epoch 1372/2000, Loss: 0.6406, Train Accuracy: 62.56%\n",
      "Epoch 1373/2000, Loss: 0.6406, Train Accuracy: 62.57%\n",
      "Epoch 1374/2000, Loss: 0.6406, Train Accuracy: 62.62%\n",
      "Epoch 1375/2000, Loss: 0.6406, Train Accuracy: 62.56%\n",
      "Epoch 1376/2000, Loss: 0.6406, Train Accuracy: 62.63%\n",
      "Epoch 1377/2000, Loss: 0.6406, Train Accuracy: 62.61%\n",
      "Epoch 1378/2000, Loss: 0.6406, Train Accuracy: 62.63%\n",
      "Epoch 1379/2000, Loss: 0.6406, Train Accuracy: 62.60%\n",
      "Epoch 1380/2000, Loss: 0.6406, Train Accuracy: 62.59%\n",
      "Epoch 1381/2000, Loss: 0.6406, Train Accuracy: 62.58%\n",
      "Epoch 1382/2000, Loss: 0.6406, Train Accuracy: 62.58%\n",
      "Epoch 1383/2000, Loss: 0.6406, Train Accuracy: 62.65%\n",
      "Epoch 1384/2000, Loss: 0.6406, Train Accuracy: 62.59%\n",
      "Epoch 1385/2000, Loss: 0.6406, Train Accuracy: 62.63%\n",
      "Epoch 1386/2000, Loss: 0.6405, Train Accuracy: 62.57%\n",
      "Epoch 1387/2000, Loss: 0.6405, Train Accuracy: 62.64%\n",
      "Epoch 1388/2000, Loss: 0.6405, Train Accuracy: 62.60%\n",
      "Epoch 1389/2000, Loss: 0.6405, Train Accuracy: 62.63%\n",
      "Epoch 1390/2000, Loss: 0.6405, Train Accuracy: 62.56%\n",
      "Epoch 1391/2000, Loss: 0.6405, Train Accuracy: 62.58%\n",
      "Epoch 1392/2000, Loss: 0.6405, Train Accuracy: 62.57%\n",
      "Epoch 1393/2000, Loss: 0.6405, Train Accuracy: 62.58%\n",
      "Epoch 1394/2000, Loss: 0.6405, Train Accuracy: 62.62%\n",
      "Epoch 1395/2000, Loss: 0.6405, Train Accuracy: 62.61%\n",
      "Epoch 1396/2000, Loss: 0.6405, Train Accuracy: 62.65%\n",
      "Epoch 1397/2000, Loss: 0.6405, Train Accuracy: 62.57%\n",
      "Epoch 1398/2000, Loss: 0.6405, Train Accuracy: 62.67%\n",
      "Epoch 1399/2000, Loss: 0.6405, Train Accuracy: 62.58%\n",
      "Epoch 1400/2000, Loss: 0.6405, Train Accuracy: 62.68%\n",
      "Epoch 1401/2000, Loss: 0.6404, Train Accuracy: 62.61%\n",
      "Epoch 1402/2000, Loss: 0.6404, Train Accuracy: 62.67%\n",
      "Epoch 1403/2000, Loss: 0.6404, Train Accuracy: 62.57%\n",
      "Epoch 1404/2000, Loss: 0.6404, Train Accuracy: 62.62%\n",
      "Epoch 1405/2000, Loss: 0.6404, Train Accuracy: 62.61%\n",
      "Epoch 1406/2000, Loss: 0.6404, Train Accuracy: 62.62%\n",
      "Epoch 1407/2000, Loss: 0.6404, Train Accuracy: 62.58%\n",
      "Epoch 1408/2000, Loss: 0.6404, Train Accuracy: 62.63%\n",
      "Epoch 1409/2000, Loss: 0.6404, Train Accuracy: 62.63%\n",
      "Epoch 1410/2000, Loss: 0.6404, Train Accuracy: 62.63%\n",
      "Epoch 1411/2000, Loss: 0.6404, Train Accuracy: 62.62%\n",
      "Epoch 1412/2000, Loss: 0.6404, Train Accuracy: 62.67%\n",
      "Epoch 1413/2000, Loss: 0.6404, Train Accuracy: 62.59%\n",
      "Epoch 1414/2000, Loss: 0.6404, Train Accuracy: 62.67%\n",
      "Epoch 1415/2000, Loss: 0.6404, Train Accuracy: 62.62%\n",
      "Epoch 1416/2000, Loss: 0.6404, Train Accuracy: 62.62%\n",
      "Epoch 1417/2000, Loss: 0.6404, Train Accuracy: 62.65%\n",
      "Epoch 1418/2000, Loss: 0.6404, Train Accuracy: 62.59%\n",
      "Epoch 1419/2000, Loss: 0.6404, Train Accuracy: 62.68%\n",
      "Epoch 1420/2000, Loss: 0.6404, Train Accuracy: 62.65%\n",
      "Epoch 1421/2000, Loss: 0.6404, Train Accuracy: 62.59%\n",
      "Epoch 1422/2000, Loss: 0.6405, Train Accuracy: 62.50%\n",
      "Epoch 1423/2000, Loss: 0.6405, Train Accuracy: 62.65%\n",
      "Epoch 1424/2000, Loss: 0.6405, Train Accuracy: 62.53%\n",
      "Epoch 1425/2000, Loss: 0.6406, Train Accuracy: 62.75%\n",
      "Epoch 1426/2000, Loss: 0.6406, Train Accuracy: 62.47%\n",
      "Epoch 1427/2000, Loss: 0.6406, Train Accuracy: 62.73%\n",
      "Epoch 1428/2000, Loss: 0.6406, Train Accuracy: 62.51%\n",
      "Epoch 1429/2000, Loss: 0.6405, Train Accuracy: 62.70%\n",
      "Epoch 1430/2000, Loss: 0.6404, Train Accuracy: 62.52%\n",
      "Epoch 1431/2000, Loss: 0.6403, Train Accuracy: 62.65%\n",
      "Epoch 1432/2000, Loss: 0.6403, Train Accuracy: 62.63%\n",
      "Epoch 1433/2000, Loss: 0.6403, Train Accuracy: 62.61%\n",
      "Epoch 1434/2000, Loss: 0.6403, Train Accuracy: 62.62%\n",
      "Epoch 1435/2000, Loss: 0.6403, Train Accuracy: 62.62%\n",
      "Epoch 1436/2000, Loss: 0.6403, Train Accuracy: 62.63%\n",
      "Epoch 1437/2000, Loss: 0.6404, Train Accuracy: 62.55%\n",
      "Epoch 1438/2000, Loss: 0.6404, Train Accuracy: 62.68%\n",
      "Epoch 1439/2000, Loss: 0.6404, Train Accuracy: 62.52%\n",
      "Epoch 1440/2000, Loss: 0.6403, Train Accuracy: 62.67%\n",
      "Epoch 1441/2000, Loss: 0.6403, Train Accuracy: 62.55%\n",
      "Epoch 1442/2000, Loss: 0.6403, Train Accuracy: 62.67%\n",
      "Epoch 1443/2000, Loss: 0.6402, Train Accuracy: 62.56%\n",
      "Epoch 1444/2000, Loss: 0.6402, Train Accuracy: 62.62%\n",
      "Epoch 1445/2000, Loss: 0.6402, Train Accuracy: 62.64%\n",
      "Epoch 1446/2000, Loss: 0.6402, Train Accuracy: 62.63%\n",
      "Epoch 1447/2000, Loss: 0.6402, Train Accuracy: 62.63%\n",
      "Epoch 1448/2000, Loss: 0.6402, Train Accuracy: 62.60%\n",
      "Epoch 1449/2000, Loss: 0.6402, Train Accuracy: 62.67%\n",
      "Epoch 1450/2000, Loss: 0.6402, Train Accuracy: 62.63%\n",
      "Epoch 1451/2000, Loss: 0.6402, Train Accuracy: 62.66%\n",
      "Epoch 1452/2000, Loss: 0.6402, Train Accuracy: 62.63%\n",
      "Epoch 1453/2000, Loss: 0.6402, Train Accuracy: 62.65%\n",
      "Epoch 1454/2000, Loss: 0.6402, Train Accuracy: 62.62%\n",
      "Epoch 1455/2000, Loss: 0.6402, Train Accuracy: 62.66%\n",
      "Epoch 1456/2000, Loss: 0.6402, Train Accuracy: 62.63%\n",
      "Epoch 1457/2000, Loss: 0.6402, Train Accuracy: 62.72%\n",
      "Epoch 1458/2000, Loss: 0.6401, Train Accuracy: 62.61%\n",
      "Epoch 1459/2000, Loss: 0.6401, Train Accuracy: 62.64%\n",
      "Epoch 1460/2000, Loss: 0.6401, Train Accuracy: 62.70%\n",
      "Epoch 1461/2000, Loss: 0.6401, Train Accuracy: 62.62%\n",
      "Epoch 1462/2000, Loss: 0.6401, Train Accuracy: 62.61%\n",
      "Epoch 1463/2000, Loss: 0.6401, Train Accuracy: 62.65%\n",
      "Epoch 1464/2000, Loss: 0.6401, Train Accuracy: 62.61%\n",
      "Epoch 1465/2000, Loss: 0.6401, Train Accuracy: 62.67%\n",
      "Epoch 1466/2000, Loss: 0.6401, Train Accuracy: 62.65%\n",
      "Epoch 1467/2000, Loss: 0.6401, Train Accuracy: 62.62%\n",
      "Epoch 1468/2000, Loss: 0.6401, Train Accuracy: 62.80%\n",
      "Epoch 1469/2000, Loss: 0.6401, Train Accuracy: 62.64%\n",
      "Epoch 1470/2000, Loss: 0.6401, Train Accuracy: 62.67%\n",
      "Epoch 1471/2000, Loss: 0.6401, Train Accuracy: 62.58%\n",
      "Epoch 1472/2000, Loss: 0.6401, Train Accuracy: 62.64%\n",
      "Epoch 1473/2000, Loss: 0.6402, Train Accuracy: 62.56%\n",
      "Epoch 1474/2000, Loss: 0.6402, Train Accuracy: 62.75%\n",
      "Epoch 1475/2000, Loss: 0.6402, Train Accuracy: 62.54%\n",
      "Epoch 1476/2000, Loss: 0.6402, Train Accuracy: 62.75%\n",
      "Epoch 1477/2000, Loss: 0.6402, Train Accuracy: 62.61%\n",
      "Epoch 1478/2000, Loss: 0.6402, Train Accuracy: 62.78%\n",
      "Epoch 1479/2000, Loss: 0.6402, Train Accuracy: 62.58%\n",
      "Epoch 1480/2000, Loss: 0.6402, Train Accuracy: 62.73%\n",
      "Epoch 1481/2000, Loss: 0.6401, Train Accuracy: 62.56%\n",
      "Epoch 1482/2000, Loss: 0.6401, Train Accuracy: 62.62%\n",
      "Epoch 1483/2000, Loss: 0.6400, Train Accuracy: 62.60%\n",
      "Epoch 1484/2000, Loss: 0.6400, Train Accuracy: 62.67%\n",
      "Epoch 1485/2000, Loss: 0.6400, Train Accuracy: 62.66%\n",
      "Epoch 1486/2000, Loss: 0.6400, Train Accuracy: 62.68%\n",
      "Epoch 1487/2000, Loss: 0.6400, Train Accuracy: 62.73%\n",
      "Epoch 1488/2000, Loss: 0.6400, Train Accuracy: 62.61%\n",
      "Epoch 1489/2000, Loss: 0.6400, Train Accuracy: 62.67%\n",
      "Epoch 1490/2000, Loss: 0.6400, Train Accuracy: 62.61%\n",
      "Epoch 1491/2000, Loss: 0.6400, Train Accuracy: 62.63%\n",
      "Epoch 1492/2000, Loss: 0.6400, Train Accuracy: 62.57%\n",
      "Epoch 1493/2000, Loss: 0.6401, Train Accuracy: 62.68%\n",
      "Epoch 1494/2000, Loss: 0.6401, Train Accuracy: 62.53%\n",
      "Epoch 1495/2000, Loss: 0.6400, Train Accuracy: 62.68%\n",
      "Epoch 1496/2000, Loss: 0.6400, Train Accuracy: 62.55%\n",
      "Epoch 1497/2000, Loss: 0.6400, Train Accuracy: 62.62%\n",
      "Epoch 1498/2000, Loss: 0.6400, Train Accuracy: 62.58%\n",
      "Epoch 1499/2000, Loss: 0.6399, Train Accuracy: 62.71%\n",
      "Epoch 1500/2000, Loss: 0.6399, Train Accuracy: 62.63%\n",
      "Epoch 1501/2000, Loss: 0.6399, Train Accuracy: 62.68%\n",
      "Epoch 1502/2000, Loss: 0.6399, Train Accuracy: 62.67%\n",
      "Epoch 1503/2000, Loss: 0.6399, Train Accuracy: 62.67%\n",
      "Epoch 1504/2000, Loss: 0.6399, Train Accuracy: 62.73%\n",
      "Epoch 1505/2000, Loss: 0.6399, Train Accuracy: 62.67%\n",
      "Epoch 1506/2000, Loss: 0.6399, Train Accuracy: 62.71%\n",
      "Epoch 1507/2000, Loss: 0.6399, Train Accuracy: 62.60%\n",
      "Epoch 1508/2000, Loss: 0.6399, Train Accuracy: 62.65%\n",
      "Epoch 1509/2000, Loss: 0.6399, Train Accuracy: 62.60%\n",
      "Epoch 1510/2000, Loss: 0.6399, Train Accuracy: 62.62%\n",
      "Epoch 1511/2000, Loss: 0.6399, Train Accuracy: 62.61%\n",
      "Epoch 1512/2000, Loss: 0.6399, Train Accuracy: 62.62%\n",
      "Epoch 1513/2000, Loss: 0.6399, Train Accuracy: 62.57%\n",
      "Epoch 1514/2000, Loss: 0.6399, Train Accuracy: 62.64%\n",
      "Epoch 1515/2000, Loss: 0.6399, Train Accuracy: 62.54%\n",
      "Epoch 1516/2000, Loss: 0.6399, Train Accuracy: 62.65%\n",
      "Epoch 1517/2000, Loss: 0.6399, Train Accuracy: 62.58%\n",
      "Epoch 1518/2000, Loss: 0.6399, Train Accuracy: 62.60%\n",
      "Epoch 1519/2000, Loss: 0.6399, Train Accuracy: 62.64%\n",
      "Epoch 1520/2000, Loss: 0.6398, Train Accuracy: 62.65%\n",
      "Epoch 1521/2000, Loss: 0.6398, Train Accuracy: 62.63%\n",
      "Epoch 1522/2000, Loss: 0.6398, Train Accuracy: 62.70%\n",
      "Epoch 1523/2000, Loss: 0.6398, Train Accuracy: 62.65%\n",
      "Epoch 1524/2000, Loss: 0.6397, Train Accuracy: 62.73%\n",
      "Epoch 1525/2000, Loss: 0.6397, Train Accuracy: 62.68%\n",
      "Epoch 1526/2000, Loss: 0.6397, Train Accuracy: 62.71%\n",
      "Epoch 1527/2000, Loss: 0.6397, Train Accuracy: 62.71%\n",
      "Epoch 1528/2000, Loss: 0.6397, Train Accuracy: 62.65%\n",
      "Epoch 1529/2000, Loss: 0.6397, Train Accuracy: 62.74%\n",
      "Epoch 1530/2000, Loss: 0.6397, Train Accuracy: 62.65%\n",
      "Epoch 1531/2000, Loss: 0.6397, Train Accuracy: 62.73%\n",
      "Epoch 1532/2000, Loss: 0.6397, Train Accuracy: 62.60%\n",
      "Epoch 1533/2000, Loss: 0.6398, Train Accuracy: 62.64%\n",
      "Epoch 1534/2000, Loss: 0.6398, Train Accuracy: 62.63%\n",
      "Epoch 1535/2000, Loss: 0.6398, Train Accuracy: 62.67%\n",
      "Epoch 1536/2000, Loss: 0.6398, Train Accuracy: 62.60%\n",
      "Epoch 1537/2000, Loss: 0.6399, Train Accuracy: 62.85%\n",
      "Epoch 1538/2000, Loss: 0.6399, Train Accuracy: 62.60%\n",
      "Epoch 1539/2000, Loss: 0.6399, Train Accuracy: 62.80%\n",
      "Epoch 1540/2000, Loss: 0.6399, Train Accuracy: 62.60%\n",
      "Epoch 1541/2000, Loss: 0.6399, Train Accuracy: 62.85%\n",
      "Epoch 1542/2000, Loss: 0.6399, Train Accuracy: 62.58%\n",
      "Epoch 1543/2000, Loss: 0.6398, Train Accuracy: 62.83%\n",
      "Epoch 1544/2000, Loss: 0.6398, Train Accuracy: 62.65%\n",
      "Epoch 1545/2000, Loss: 0.6397, Train Accuracy: 62.64%\n",
      "Epoch 1546/2000, Loss: 0.6396, Train Accuracy: 62.64%\n",
      "Epoch 1547/2000, Loss: 0.6396, Train Accuracy: 62.79%\n",
      "Epoch 1548/2000, Loss: 0.6396, Train Accuracy: 62.77%\n",
      "Epoch 1549/2000, Loss: 0.6396, Train Accuracy: 62.64%\n",
      "Epoch 1550/2000, Loss: 0.6397, Train Accuracy: 62.67%\n",
      "Epoch 1551/2000, Loss: 0.6397, Train Accuracy: 62.62%\n",
      "Epoch 1552/2000, Loss: 0.6397, Train Accuracy: 62.70%\n",
      "Epoch 1553/2000, Loss: 0.6397, Train Accuracy: 62.62%\n",
      "Epoch 1554/2000, Loss: 0.6397, Train Accuracy: 62.72%\n",
      "Epoch 1555/2000, Loss: 0.6397, Train Accuracy: 62.60%\n",
      "Epoch 1556/2000, Loss: 0.6397, Train Accuracy: 62.65%\n",
      "Epoch 1557/2000, Loss: 0.6396, Train Accuracy: 62.62%\n",
      "Epoch 1558/2000, Loss: 0.6396, Train Accuracy: 62.71%\n",
      "Epoch 1559/2000, Loss: 0.6396, Train Accuracy: 62.65%\n",
      "Epoch 1560/2000, Loss: 0.6396, Train Accuracy: 62.77%\n",
      "Epoch 1561/2000, Loss: 0.6395, Train Accuracy: 62.77%\n",
      "Epoch 1562/2000, Loss: 0.6395, Train Accuracy: 62.73%\n",
      "Epoch 1563/2000, Loss: 0.6395, Train Accuracy: 62.72%\n",
      "Epoch 1564/2000, Loss: 0.6395, Train Accuracy: 62.65%\n",
      "Epoch 1565/2000, Loss: 0.6396, Train Accuracy: 62.70%\n",
      "Epoch 1566/2000, Loss: 0.6396, Train Accuracy: 62.64%\n",
      "Epoch 1567/2000, Loss: 0.6396, Train Accuracy: 62.68%\n",
      "Epoch 1568/2000, Loss: 0.6396, Train Accuracy: 62.65%\n",
      "Epoch 1569/2000, Loss: 0.6396, Train Accuracy: 62.70%\n",
      "Epoch 1570/2000, Loss: 0.6396, Train Accuracy: 62.67%\n",
      "Epoch 1571/2000, Loss: 0.6396, Train Accuracy: 62.72%\n",
      "Epoch 1572/2000, Loss: 0.6396, Train Accuracy: 62.63%\n",
      "Epoch 1573/2000, Loss: 0.6396, Train Accuracy: 62.73%\n",
      "Epoch 1574/2000, Loss: 0.6396, Train Accuracy: 62.59%\n",
      "Epoch 1575/2000, Loss: 0.6396, Train Accuracy: 62.67%\n",
      "Epoch 1576/2000, Loss: 0.6395, Train Accuracy: 62.66%\n",
      "Epoch 1577/2000, Loss: 0.6395, Train Accuracy: 62.71%\n",
      "Epoch 1578/2000, Loss: 0.6395, Train Accuracy: 62.65%\n",
      "Epoch 1579/2000, Loss: 0.6395, Train Accuracy: 62.71%\n",
      "Epoch 1580/2000, Loss: 0.6394, Train Accuracy: 62.67%\n",
      "Epoch 1581/2000, Loss: 0.6394, Train Accuracy: 62.79%\n",
      "Epoch 1582/2000, Loss: 0.6394, Train Accuracy: 62.77%\n",
      "Epoch 1583/2000, Loss: 0.6394, Train Accuracy: 62.76%\n",
      "Epoch 1584/2000, Loss: 0.6394, Train Accuracy: 62.77%\n",
      "Epoch 1585/2000, Loss: 0.6394, Train Accuracy: 62.73%\n",
      "Epoch 1586/2000, Loss: 0.6394, Train Accuracy: 62.77%\n",
      "Epoch 1587/2000, Loss: 0.6394, Train Accuracy: 62.66%\n",
      "Epoch 1588/2000, Loss: 0.6394, Train Accuracy: 62.71%\n",
      "Epoch 1589/2000, Loss: 0.6394, Train Accuracy: 62.67%\n",
      "Epoch 1590/2000, Loss: 0.6394, Train Accuracy: 62.68%\n",
      "Epoch 1591/2000, Loss: 0.6394, Train Accuracy: 62.65%\n",
      "Epoch 1592/2000, Loss: 0.6395, Train Accuracy: 62.72%\n",
      "Epoch 1593/2000, Loss: 0.6395, Train Accuracy: 62.60%\n",
      "Epoch 1594/2000, Loss: 0.6396, Train Accuracy: 62.82%\n",
      "Epoch 1595/2000, Loss: 0.6396, Train Accuracy: 62.57%\n",
      "Epoch 1596/2000, Loss: 0.6397, Train Accuracy: 62.87%\n",
      "Epoch 1597/2000, Loss: 0.6397, Train Accuracy: 62.57%\n",
      "Epoch 1598/2000, Loss: 0.6397, Train Accuracy: 62.87%\n",
      "Epoch 1599/2000, Loss: 0.6397, Train Accuracy: 62.55%\n",
      "Epoch 1600/2000, Loss: 0.6396, Train Accuracy: 62.87%\n",
      "Epoch 1601/2000, Loss: 0.6395, Train Accuracy: 62.57%\n",
      "Epoch 1602/2000, Loss: 0.6394, Train Accuracy: 62.73%\n",
      "Epoch 1603/2000, Loss: 0.6393, Train Accuracy: 62.64%\n",
      "Epoch 1604/2000, Loss: 0.6393, Train Accuracy: 62.81%\n",
      "Epoch 1605/2000, Loss: 0.6393, Train Accuracy: 62.68%\n",
      "Epoch 1606/2000, Loss: 0.6394, Train Accuracy: 62.64%\n",
      "Epoch 1607/2000, Loss: 0.6394, Train Accuracy: 62.77%\n",
      "Epoch 1608/2000, Loss: 0.6395, Train Accuracy: 62.63%\n",
      "Epoch 1609/2000, Loss: 0.6395, Train Accuracy: 62.88%\n",
      "Epoch 1610/2000, Loss: 0.6395, Train Accuracy: 62.65%\n",
      "Epoch 1611/2000, Loss: 0.6394, Train Accuracy: 62.79%\n",
      "Epoch 1612/2000, Loss: 0.6394, Train Accuracy: 62.63%\n",
      "Epoch 1613/2000, Loss: 0.6393, Train Accuracy: 62.70%\n",
      "Epoch 1614/2000, Loss: 0.6393, Train Accuracy: 62.67%\n",
      "Epoch 1615/2000, Loss: 0.6393, Train Accuracy: 62.82%\n",
      "Epoch 1616/2000, Loss: 0.6393, Train Accuracy: 62.80%\n",
      "Epoch 1617/2000, Loss: 0.6393, Train Accuracy: 62.70%\n",
      "Epoch 1618/2000, Loss: 0.6393, Train Accuracy: 62.77%\n",
      "Epoch 1619/2000, Loss: 0.6393, Train Accuracy: 62.65%\n",
      "Epoch 1620/2000, Loss: 0.6393, Train Accuracy: 62.75%\n",
      "Epoch 1621/2000, Loss: 0.6393, Train Accuracy: 62.63%\n",
      "Epoch 1622/2000, Loss: 0.6393, Train Accuracy: 62.76%\n",
      "Epoch 1623/2000, Loss: 0.6393, Train Accuracy: 62.58%\n",
      "Epoch 1624/2000, Loss: 0.6393, Train Accuracy: 62.75%\n",
      "Epoch 1625/2000, Loss: 0.6392, Train Accuracy: 62.70%\n",
      "Epoch 1626/2000, Loss: 0.6392, Train Accuracy: 62.71%\n",
      "Epoch 1627/2000, Loss: 0.6392, Train Accuracy: 62.73%\n",
      "Epoch 1628/2000, Loss: 0.6392, Train Accuracy: 62.78%\n",
      "Epoch 1629/2000, Loss: 0.6392, Train Accuracy: 62.82%\n",
      "Epoch 1630/2000, Loss: 0.6392, Train Accuracy: 62.72%\n",
      "Epoch 1631/2000, Loss: 0.6392, Train Accuracy: 62.73%\n",
      "Epoch 1632/2000, Loss: 0.6392, Train Accuracy: 62.71%\n",
      "Epoch 1633/2000, Loss: 0.6392, Train Accuracy: 62.77%\n",
      "Epoch 1634/2000, Loss: 0.6392, Train Accuracy: 62.64%\n",
      "Epoch 1635/2000, Loss: 0.6392, Train Accuracy: 62.78%\n",
      "Epoch 1636/2000, Loss: 0.6392, Train Accuracy: 62.58%\n",
      "Epoch 1637/2000, Loss: 0.6392, Train Accuracy: 62.74%\n",
      "Epoch 1638/2000, Loss: 0.6392, Train Accuracy: 62.61%\n",
      "Epoch 1639/2000, Loss: 0.6392, Train Accuracy: 62.78%\n",
      "Epoch 1640/2000, Loss: 0.6392, Train Accuracy: 62.58%\n",
      "Epoch 1641/2000, Loss: 0.6392, Train Accuracy: 62.73%\n",
      "Epoch 1642/2000, Loss: 0.6392, Train Accuracy: 62.60%\n",
      "Epoch 1643/2000, Loss: 0.6392, Train Accuracy: 62.79%\n",
      "Epoch 1644/2000, Loss: 0.6391, Train Accuracy: 62.68%\n",
      "Epoch 1645/2000, Loss: 0.6391, Train Accuracy: 62.78%\n",
      "Epoch 1646/2000, Loss: 0.6391, Train Accuracy: 62.73%\n",
      "Epoch 1647/2000, Loss: 0.6391, Train Accuracy: 62.75%\n",
      "Epoch 1648/2000, Loss: 0.6391, Train Accuracy: 62.73%\n",
      "Epoch 1649/2000, Loss: 0.6391, Train Accuracy: 62.80%\n",
      "Epoch 1650/2000, Loss: 0.6391, Train Accuracy: 62.73%\n",
      "Epoch 1651/2000, Loss: 0.6391, Train Accuracy: 62.77%\n",
      "Epoch 1652/2000, Loss: 0.6391, Train Accuracy: 62.76%\n",
      "Epoch 1653/2000, Loss: 0.6391, Train Accuracy: 62.83%\n",
      "Epoch 1654/2000, Loss: 0.6391, Train Accuracy: 62.77%\n",
      "Epoch 1655/2000, Loss: 0.6390, Train Accuracy: 62.82%\n",
      "Epoch 1656/2000, Loss: 0.6390, Train Accuracy: 62.74%\n",
      "Epoch 1657/2000, Loss: 0.6390, Train Accuracy: 62.78%\n",
      "Epoch 1658/2000, Loss: 0.6390, Train Accuracy: 62.77%\n",
      "Epoch 1659/2000, Loss: 0.6390, Train Accuracy: 62.73%\n",
      "Epoch 1660/2000, Loss: 0.6390, Train Accuracy: 62.73%\n",
      "Epoch 1661/2000, Loss: 0.6390, Train Accuracy: 62.79%\n",
      "Epoch 1662/2000, Loss: 0.6390, Train Accuracy: 62.73%\n",
      "Epoch 1663/2000, Loss: 0.6390, Train Accuracy: 62.75%\n",
      "Epoch 1664/2000, Loss: 0.6391, Train Accuracy: 62.61%\n",
      "Epoch 1665/2000, Loss: 0.6391, Train Accuracy: 62.80%\n",
      "Epoch 1666/2000, Loss: 0.6391, Train Accuracy: 62.66%\n",
      "Epoch 1667/2000, Loss: 0.6392, Train Accuracy: 62.87%\n",
      "Epoch 1668/2000, Loss: 0.6393, Train Accuracy: 62.57%\n",
      "Epoch 1669/2000, Loss: 0.6394, Train Accuracy: 62.83%\n",
      "Epoch 1670/2000, Loss: 0.6395, Train Accuracy: 62.52%\n",
      "Epoch 1671/2000, Loss: 0.6396, Train Accuracy: 62.78%\n",
      "Epoch 1672/2000, Loss: 0.6396, Train Accuracy: 62.52%\n",
      "Epoch 1673/2000, Loss: 0.6395, Train Accuracy: 62.86%\n",
      "Epoch 1674/2000, Loss: 0.6393, Train Accuracy: 62.56%\n",
      "Epoch 1675/2000, Loss: 0.6391, Train Accuracy: 62.78%\n",
      "Epoch 1676/2000, Loss: 0.6390, Train Accuracy: 62.76%\n",
      "Epoch 1677/2000, Loss: 0.6390, Train Accuracy: 62.73%\n",
      "Epoch 1678/2000, Loss: 0.6390, Train Accuracy: 62.83%\n",
      "Epoch 1679/2000, Loss: 0.6391, Train Accuracy: 62.55%\n",
      "Epoch 1680/2000, Loss: 0.6392, Train Accuracy: 62.89%\n",
      "Epoch 1681/2000, Loss: 0.6392, Train Accuracy: 62.55%\n",
      "Epoch 1682/2000, Loss: 0.6391, Train Accuracy: 62.87%\n",
      "Epoch 1683/2000, Loss: 0.6390, Train Accuracy: 62.60%\n",
      "Epoch 1684/2000, Loss: 0.6389, Train Accuracy: 62.80%\n",
      "Epoch 1685/2000, Loss: 0.6389, Train Accuracy: 62.78%\n",
      "Epoch 1686/2000, Loss: 0.6389, Train Accuracy: 62.69%\n",
      "Epoch 1687/2000, Loss: 0.6390, Train Accuracy: 62.80%\n",
      "Epoch 1688/2000, Loss: 0.6390, Train Accuracy: 62.65%\n",
      "Epoch 1689/2000, Loss: 0.6390, Train Accuracy: 62.87%\n",
      "Epoch 1690/2000, Loss: 0.6390, Train Accuracy: 62.67%\n",
      "Epoch 1691/2000, Loss: 0.6389, Train Accuracy: 62.80%\n",
      "Epoch 1692/2000, Loss: 0.6389, Train Accuracy: 62.72%\n",
      "Epoch 1693/2000, Loss: 0.6389, Train Accuracy: 62.75%\n",
      "Epoch 1694/2000, Loss: 0.6389, Train Accuracy: 62.80%\n",
      "Epoch 1695/2000, Loss: 0.6389, Train Accuracy: 62.81%\n",
      "Epoch 1696/2000, Loss: 0.6389, Train Accuracy: 62.77%\n",
      "Epoch 1697/2000, Loss: 0.6389, Train Accuracy: 62.65%\n",
      "Epoch 1698/2000, Loss: 0.6389, Train Accuracy: 62.79%\n",
      "Epoch 1699/2000, Loss: 0.6389, Train Accuracy: 62.63%\n",
      "Epoch 1700/2000, Loss: 0.6389, Train Accuracy: 62.78%\n",
      "Epoch 1701/2000, Loss: 0.6388, Train Accuracy: 62.68%\n",
      "Epoch 1702/2000, Loss: 0.6388, Train Accuracy: 62.77%\n",
      "Epoch 1703/2000, Loss: 0.6388, Train Accuracy: 62.83%\n",
      "Epoch 1704/2000, Loss: 0.6388, Train Accuracy: 62.82%\n",
      "Epoch 1705/2000, Loss: 0.6388, Train Accuracy: 62.76%\n",
      "Epoch 1706/2000, Loss: 0.6388, Train Accuracy: 62.75%\n",
      "Epoch 1707/2000, Loss: 0.6388, Train Accuracy: 62.77%\n",
      "Epoch 1708/2000, Loss: 0.6388, Train Accuracy: 62.68%\n",
      "Epoch 1709/2000, Loss: 0.6388, Train Accuracy: 62.79%\n",
      "Epoch 1710/2000, Loss: 0.6388, Train Accuracy: 62.68%\n",
      "Epoch 1711/2000, Loss: 0.6388, Train Accuracy: 62.79%\n",
      "Epoch 1712/2000, Loss: 0.6388, Train Accuracy: 62.68%\n",
      "Epoch 1713/2000, Loss: 0.6388, Train Accuracy: 62.77%\n",
      "Epoch 1714/2000, Loss: 0.6388, Train Accuracy: 62.75%\n",
      "Epoch 1715/2000, Loss: 0.6388, Train Accuracy: 62.76%\n",
      "Epoch 1716/2000, Loss: 0.6387, Train Accuracy: 62.80%\n",
      "Epoch 1717/2000, Loss: 0.6387, Train Accuracy: 62.76%\n",
      "Epoch 1718/2000, Loss: 0.6387, Train Accuracy: 62.82%\n",
      "Epoch 1719/2000, Loss: 0.6387, Train Accuracy: 62.82%\n",
      "Epoch 1720/2000, Loss: 0.6387, Train Accuracy: 62.82%\n",
      "Epoch 1721/2000, Loss: 0.6387, Train Accuracy: 62.83%\n",
      "Epoch 1722/2000, Loss: 0.6387, Train Accuracy: 62.77%\n",
      "Epoch 1723/2000, Loss: 0.6387, Train Accuracy: 62.83%\n",
      "Epoch 1724/2000, Loss: 0.6387, Train Accuracy: 62.78%\n",
      "Epoch 1725/2000, Loss: 0.6387, Train Accuracy: 62.75%\n",
      "Epoch 1726/2000, Loss: 0.6387, Train Accuracy: 62.77%\n",
      "Epoch 1727/2000, Loss: 0.6387, Train Accuracy: 62.73%\n",
      "Epoch 1728/2000, Loss: 0.6387, Train Accuracy: 62.78%\n",
      "Epoch 1729/2000, Loss: 0.6387, Train Accuracy: 62.70%\n",
      "Epoch 1730/2000, Loss: 0.6387, Train Accuracy: 62.82%\n",
      "Epoch 1731/2000, Loss: 0.6387, Train Accuracy: 62.64%\n",
      "Epoch 1732/2000, Loss: 0.6388, Train Accuracy: 62.87%\n",
      "Epoch 1733/2000, Loss: 0.6388, Train Accuracy: 62.63%\n",
      "Epoch 1734/2000, Loss: 0.6388, Train Accuracy: 62.95%\n",
      "Epoch 1735/2000, Loss: 0.6388, Train Accuracy: 62.55%\n",
      "Epoch 1736/2000, Loss: 0.6389, Train Accuracy: 62.90%\n",
      "Epoch 1737/2000, Loss: 0.6389, Train Accuracy: 62.53%\n",
      "Epoch 1738/2000, Loss: 0.6388, Train Accuracy: 62.90%\n",
      "Epoch 1739/2000, Loss: 0.6388, Train Accuracy: 62.53%\n",
      "Epoch 1740/2000, Loss: 0.6388, Train Accuracy: 62.91%\n",
      "Epoch 1741/2000, Loss: 0.6387, Train Accuracy: 62.68%\n",
      "Epoch 1742/2000, Loss: 0.6387, Train Accuracy: 62.88%\n",
      "Epoch 1743/2000, Loss: 0.6386, Train Accuracy: 62.71%\n",
      "Epoch 1744/2000, Loss: 0.6386, Train Accuracy: 62.79%\n",
      "Epoch 1745/2000, Loss: 0.6386, Train Accuracy: 62.80%\n",
      "Epoch 1746/2000, Loss: 0.6386, Train Accuracy: 62.78%\n",
      "Epoch 1747/2000, Loss: 0.6386, Train Accuracy: 62.76%\n",
      "Epoch 1748/2000, Loss: 0.6386, Train Accuracy: 62.72%\n",
      "Epoch 1749/2000, Loss: 0.6386, Train Accuracy: 62.89%\n",
      "Epoch 1750/2000, Loss: 0.6386, Train Accuracy: 62.67%\n",
      "Epoch 1751/2000, Loss: 0.6387, Train Accuracy: 62.84%\n",
      "Epoch 1752/2000, Loss: 0.6387, Train Accuracy: 62.65%\n",
      "Epoch 1753/2000, Loss: 0.6387, Train Accuracy: 62.90%\n",
      "Epoch 1754/2000, Loss: 0.6387, Train Accuracy: 62.62%\n",
      "Epoch 1755/2000, Loss: 0.6387, Train Accuracy: 62.90%\n",
      "Epoch 1756/2000, Loss: 0.6387, Train Accuracy: 62.66%\n",
      "Epoch 1757/2000, Loss: 0.6386, Train Accuracy: 62.90%\n",
      "Epoch 1758/2000, Loss: 0.6386, Train Accuracy: 62.70%\n",
      "Epoch 1759/2000, Loss: 0.6386, Train Accuracy: 62.90%\n",
      "Epoch 1760/2000, Loss: 0.6386, Train Accuracy: 62.76%\n",
      "Epoch 1761/2000, Loss: 0.6385, Train Accuracy: 62.75%\n",
      "Epoch 1762/2000, Loss: 0.6385, Train Accuracy: 62.77%\n",
      "Epoch 1763/2000, Loss: 0.6385, Train Accuracy: 62.77%\n",
      "Epoch 1764/2000, Loss: 0.6385, Train Accuracy: 62.77%\n",
      "Epoch 1765/2000, Loss: 0.6385, Train Accuracy: 62.78%\n",
      "Epoch 1766/2000, Loss: 0.6385, Train Accuracy: 62.72%\n",
      "Epoch 1767/2000, Loss: 0.6385, Train Accuracy: 62.74%\n",
      "Epoch 1768/2000, Loss: 0.6385, Train Accuracy: 62.79%\n",
      "Epoch 1769/2000, Loss: 0.6385, Train Accuracy: 62.77%\n",
      "Epoch 1770/2000, Loss: 0.6385, Train Accuracy: 62.95%\n",
      "Epoch 1771/2000, Loss: 0.6385, Train Accuracy: 62.76%\n",
      "Epoch 1772/2000, Loss: 0.6385, Train Accuracy: 62.85%\n",
      "Epoch 1773/2000, Loss: 0.6386, Train Accuracy: 62.70%\n",
      "Epoch 1774/2000, Loss: 0.6386, Train Accuracy: 62.90%\n",
      "Epoch 1775/2000, Loss: 0.6386, Train Accuracy: 62.60%\n",
      "Epoch 1776/2000, Loss: 0.6386, Train Accuracy: 62.91%\n",
      "Epoch 1777/2000, Loss: 0.6386, Train Accuracy: 62.62%\n",
      "Epoch 1778/2000, Loss: 0.6386, Train Accuracy: 62.92%\n",
      "Epoch 1779/2000, Loss: 0.6386, Train Accuracy: 62.60%\n",
      "Epoch 1780/2000, Loss: 0.6386, Train Accuracy: 62.90%\n",
      "Epoch 1781/2000, Loss: 0.6385, Train Accuracy: 62.65%\n",
      "Epoch 1782/2000, Loss: 0.6385, Train Accuracy: 62.92%\n",
      "Epoch 1783/2000, Loss: 0.6385, Train Accuracy: 62.75%\n",
      "Epoch 1784/2000, Loss: 0.6384, Train Accuracy: 62.92%\n",
      "Epoch 1785/2000, Loss: 0.6384, Train Accuracy: 62.75%\n",
      "Epoch 1786/2000, Loss: 0.6384, Train Accuracy: 62.73%\n",
      "Epoch 1787/2000, Loss: 0.6384, Train Accuracy: 62.72%\n",
      "Epoch 1788/2000, Loss: 0.6384, Train Accuracy: 62.77%\n",
      "Epoch 1789/2000, Loss: 0.6384, Train Accuracy: 62.79%\n",
      "Epoch 1790/2000, Loss: 0.6384, Train Accuracy: 62.75%\n",
      "Epoch 1791/2000, Loss: 0.6384, Train Accuracy: 62.74%\n",
      "Epoch 1792/2000, Loss: 0.6384, Train Accuracy: 62.77%\n",
      "Epoch 1793/2000, Loss: 0.6384, Train Accuracy: 62.83%\n",
      "Epoch 1794/2000, Loss: 0.6384, Train Accuracy: 62.80%\n",
      "Epoch 1795/2000, Loss: 0.6384, Train Accuracy: 62.95%\n",
      "Epoch 1796/2000, Loss: 0.6384, Train Accuracy: 62.77%\n",
      "Epoch 1797/2000, Loss: 0.6384, Train Accuracy: 62.91%\n",
      "Epoch 1798/2000, Loss: 0.6385, Train Accuracy: 62.69%\n",
      "Epoch 1799/2000, Loss: 0.6385, Train Accuracy: 62.89%\n",
      "Epoch 1800/2000, Loss: 0.6385, Train Accuracy: 62.62%\n",
      "Epoch 1801/2000, Loss: 0.6385, Train Accuracy: 62.90%\n",
      "Epoch 1802/2000, Loss: 0.6385, Train Accuracy: 62.58%\n",
      "Epoch 1803/2000, Loss: 0.6385, Train Accuracy: 62.91%\n",
      "Epoch 1804/2000, Loss: 0.6385, Train Accuracy: 62.62%\n",
      "Epoch 1805/2000, Loss: 0.6384, Train Accuracy: 62.92%\n",
      "Epoch 1806/2000, Loss: 0.6384, Train Accuracy: 62.67%\n",
      "Epoch 1807/2000, Loss: 0.6384, Train Accuracy: 62.90%\n",
      "Epoch 1808/2000, Loss: 0.6383, Train Accuracy: 62.80%\n",
      "Epoch 1809/2000, Loss: 0.6383, Train Accuracy: 62.77%\n",
      "Epoch 1810/2000, Loss: 0.6383, Train Accuracy: 62.72%\n",
      "Epoch 1811/2000, Loss: 0.6382, Train Accuracy: 62.77%\n",
      "Epoch 1812/2000, Loss: 0.6382, Train Accuracy: 62.77%\n",
      "Epoch 1813/2000, Loss: 0.6382, Train Accuracy: 62.75%\n",
      "Epoch 1814/2000, Loss: 0.6383, Train Accuracy: 62.78%\n",
      "Epoch 1815/2000, Loss: 0.6383, Train Accuracy: 62.80%\n",
      "Epoch 1816/2000, Loss: 0.6383, Train Accuracy: 62.93%\n",
      "Epoch 1817/2000, Loss: 0.6383, Train Accuracy: 62.77%\n",
      "Epoch 1818/2000, Loss: 0.6383, Train Accuracy: 62.90%\n",
      "Epoch 1819/2000, Loss: 0.6383, Train Accuracy: 62.74%\n",
      "Epoch 1820/2000, Loss: 0.6384, Train Accuracy: 62.93%\n",
      "Epoch 1821/2000, Loss: 0.6384, Train Accuracy: 62.63%\n",
      "Epoch 1822/2000, Loss: 0.6384, Train Accuracy: 62.90%\n",
      "Epoch 1823/2000, Loss: 0.6384, Train Accuracy: 62.63%\n",
      "Epoch 1824/2000, Loss: 0.6384, Train Accuracy: 62.94%\n",
      "Epoch 1825/2000, Loss: 0.6383, Train Accuracy: 62.66%\n",
      "Epoch 1826/2000, Loss: 0.6383, Train Accuracy: 62.90%\n",
      "Epoch 1827/2000, Loss: 0.6383, Train Accuracy: 62.74%\n",
      "Epoch 1828/2000, Loss: 0.6382, Train Accuracy: 62.87%\n",
      "Epoch 1829/2000, Loss: 0.6382, Train Accuracy: 62.85%\n",
      "Epoch 1830/2000, Loss: 0.6382, Train Accuracy: 62.83%\n",
      "Epoch 1831/2000, Loss: 0.6382, Train Accuracy: 62.75%\n",
      "Epoch 1832/2000, Loss: 0.6381, Train Accuracy: 62.75%\n",
      "Epoch 1833/2000, Loss: 0.6381, Train Accuracy: 62.78%\n",
      "Epoch 1834/2000, Loss: 0.6381, Train Accuracy: 62.72%\n",
      "Epoch 1835/2000, Loss: 0.6381, Train Accuracy: 62.78%\n",
      "Epoch 1836/2000, Loss: 0.6381, Train Accuracy: 62.77%\n",
      "Epoch 1837/2000, Loss: 0.6382, Train Accuracy: 62.86%\n",
      "Epoch 1838/2000, Loss: 0.6382, Train Accuracy: 62.83%\n",
      "Epoch 1839/2000, Loss: 0.6382, Train Accuracy: 62.89%\n",
      "Epoch 1840/2000, Loss: 0.6382, Train Accuracy: 62.72%\n",
      "Epoch 1841/2000, Loss: 0.6382, Train Accuracy: 62.87%\n",
      "Epoch 1842/2000, Loss: 0.6383, Train Accuracy: 62.64%\n",
      "Epoch 1843/2000, Loss: 0.6383, Train Accuracy: 62.87%\n",
      "Epoch 1844/2000, Loss: 0.6383, Train Accuracy: 62.64%\n",
      "Epoch 1845/2000, Loss: 0.6383, Train Accuracy: 62.91%\n",
      "Epoch 1846/2000, Loss: 0.6383, Train Accuracy: 62.60%\n",
      "Epoch 1847/2000, Loss: 0.6383, Train Accuracy: 62.92%\n",
      "Epoch 1848/2000, Loss: 0.6382, Train Accuracy: 62.65%\n",
      "Epoch 1849/2000, Loss: 0.6382, Train Accuracy: 62.83%\n",
      "Epoch 1850/2000, Loss: 0.6381, Train Accuracy: 62.77%\n",
      "Epoch 1851/2000, Loss: 0.6381, Train Accuracy: 62.89%\n",
      "Epoch 1852/2000, Loss: 0.6381, Train Accuracy: 62.78%\n",
      "Epoch 1853/2000, Loss: 0.6380, Train Accuracy: 62.75%\n",
      "Epoch 1854/2000, Loss: 0.6380, Train Accuracy: 62.74%\n",
      "Epoch 1855/2000, Loss: 0.6380, Train Accuracy: 62.79%\n",
      "Epoch 1856/2000, Loss: 0.6380, Train Accuracy: 62.87%\n",
      "Epoch 1857/2000, Loss: 0.6381, Train Accuracy: 62.85%\n",
      "Epoch 1858/2000, Loss: 0.6381, Train Accuracy: 62.87%\n",
      "Epoch 1859/2000, Loss: 0.6381, Train Accuracy: 62.72%\n",
      "Epoch 1860/2000, Loss: 0.6381, Train Accuracy: 62.87%\n",
      "Epoch 1861/2000, Loss: 0.6381, Train Accuracy: 62.66%\n",
      "Epoch 1862/2000, Loss: 0.6381, Train Accuracy: 62.82%\n",
      "Epoch 1863/2000, Loss: 0.6381, Train Accuracy: 62.67%\n",
      "Epoch 1864/2000, Loss: 0.6381, Train Accuracy: 62.86%\n",
      "Epoch 1865/2000, Loss: 0.6381, Train Accuracy: 62.72%\n",
      "Epoch 1866/2000, Loss: 0.6381, Train Accuracy: 62.89%\n",
      "Epoch 1867/2000, Loss: 0.6380, Train Accuracy: 62.78%\n",
      "Epoch 1868/2000, Loss: 0.6380, Train Accuracy: 62.86%\n",
      "Epoch 1869/2000, Loss: 0.6380, Train Accuracy: 62.77%\n",
      "Epoch 1870/2000, Loss: 0.6380, Train Accuracy: 62.78%\n",
      "Epoch 1871/2000, Loss: 0.6379, Train Accuracy: 62.76%\n",
      "Epoch 1872/2000, Loss: 0.6379, Train Accuracy: 62.73%\n",
      "Epoch 1873/2000, Loss: 0.6379, Train Accuracy: 62.71%\n",
      "Epoch 1874/2000, Loss: 0.6379, Train Accuracy: 62.76%\n",
      "Epoch 1875/2000, Loss: 0.6379, Train Accuracy: 62.77%\n",
      "Epoch 1876/2000, Loss: 0.6379, Train Accuracy: 62.77%\n",
      "Epoch 1877/2000, Loss: 0.6379, Train Accuracy: 62.82%\n",
      "Epoch 1878/2000, Loss: 0.6380, Train Accuracy: 62.78%\n",
      "Epoch 1879/2000, Loss: 0.6380, Train Accuracy: 62.85%\n",
      "Epoch 1880/2000, Loss: 0.6380, Train Accuracy: 62.80%\n",
      "Epoch 1881/2000, Loss: 0.6380, Train Accuracy: 62.85%\n",
      "Epoch 1882/2000, Loss: 0.6380, Train Accuracy: 62.72%\n",
      "Epoch 1883/2000, Loss: 0.6380, Train Accuracy: 62.87%\n",
      "Epoch 1884/2000, Loss: 0.6381, Train Accuracy: 62.67%\n",
      "Epoch 1885/2000, Loss: 0.6381, Train Accuracy: 62.92%\n",
      "Epoch 1886/2000, Loss: 0.6381, Train Accuracy: 62.67%\n",
      "Epoch 1887/2000, Loss: 0.6381, Train Accuracy: 62.90%\n",
      "Epoch 1888/2000, Loss: 0.6380, Train Accuracy: 62.68%\n",
      "Epoch 1889/2000, Loss: 0.6380, Train Accuracy: 62.90%\n",
      "Epoch 1890/2000, Loss: 0.6380, Train Accuracy: 62.70%\n",
      "Epoch 1891/2000, Loss: 0.6379, Train Accuracy: 62.84%\n",
      "Epoch 1892/2000, Loss: 0.6379, Train Accuracy: 62.82%\n",
      "Epoch 1893/2000, Loss: 0.6379, Train Accuracy: 62.82%\n",
      "Epoch 1894/2000, Loss: 0.6378, Train Accuracy: 62.78%\n",
      "Epoch 1895/2000, Loss: 0.6378, Train Accuracy: 62.75%\n",
      "Epoch 1896/2000, Loss: 0.6378, Train Accuracy: 62.78%\n",
      "Epoch 1897/2000, Loss: 0.6378, Train Accuracy: 62.78%\n",
      "Epoch 1898/2000, Loss: 0.6378, Train Accuracy: 62.78%\n",
      "Epoch 1899/2000, Loss: 0.6378, Train Accuracy: 62.76%\n",
      "Epoch 1900/2000, Loss: 0.6378, Train Accuracy: 62.85%\n",
      "Epoch 1901/2000, Loss: 0.6378, Train Accuracy: 62.78%\n",
      "Epoch 1902/2000, Loss: 0.6378, Train Accuracy: 62.83%\n",
      "Epoch 1903/2000, Loss: 0.6379, Train Accuracy: 62.78%\n",
      "Epoch 1904/2000, Loss: 0.6379, Train Accuracy: 62.86%\n",
      "Epoch 1905/2000, Loss: 0.6379, Train Accuracy: 62.73%\n",
      "Epoch 1906/2000, Loss: 0.6379, Train Accuracy: 62.89%\n",
      "Epoch 1907/2000, Loss: 0.6380, Train Accuracy: 62.70%\n",
      "Epoch 1908/2000, Loss: 0.6380, Train Accuracy: 62.96%\n",
      "Epoch 1909/2000, Loss: 0.6380, Train Accuracy: 62.67%\n",
      "Epoch 1910/2000, Loss: 0.6380, Train Accuracy: 62.93%\n",
      "Epoch 1911/2000, Loss: 0.6380, Train Accuracy: 62.68%\n",
      "Epoch 1912/2000, Loss: 0.6380, Train Accuracy: 62.95%\n",
      "Epoch 1913/2000, Loss: 0.6379, Train Accuracy: 62.75%\n",
      "Epoch 1914/2000, Loss: 0.6378, Train Accuracy: 62.87%\n",
      "Epoch 1915/2000, Loss: 0.6378, Train Accuracy: 62.80%\n",
      "Epoch 1916/2000, Loss: 0.6377, Train Accuracy: 62.83%\n",
      "Epoch 1917/2000, Loss: 0.6377, Train Accuracy: 62.77%\n",
      "Epoch 1918/2000, Loss: 0.6377, Train Accuracy: 62.77%\n",
      "Epoch 1919/2000, Loss: 0.6377, Train Accuracy: 62.78%\n",
      "Epoch 1920/2000, Loss: 0.6377, Train Accuracy: 62.80%\n",
      "Epoch 1921/2000, Loss: 0.6377, Train Accuracy: 62.82%\n",
      "Epoch 1922/2000, Loss: 0.6378, Train Accuracy: 62.82%\n",
      "Epoch 1923/2000, Loss: 0.6378, Train Accuracy: 62.87%\n",
      "Epoch 1924/2000, Loss: 0.6378, Train Accuracy: 62.74%\n",
      "Epoch 1925/2000, Loss: 0.6378, Train Accuracy: 62.89%\n",
      "Epoch 1926/2000, Loss: 0.6378, Train Accuracy: 62.77%\n",
      "Epoch 1927/2000, Loss: 0.6378, Train Accuracy: 62.90%\n",
      "Epoch 1928/2000, Loss: 0.6378, Train Accuracy: 62.77%\n",
      "Epoch 1929/2000, Loss: 0.6377, Train Accuracy: 62.88%\n",
      "Epoch 1930/2000, Loss: 0.6377, Train Accuracy: 62.82%\n",
      "Epoch 1931/2000, Loss: 0.6377, Train Accuracy: 62.81%\n",
      "Epoch 1932/2000, Loss: 0.6377, Train Accuracy: 62.76%\n",
      "Epoch 1933/2000, Loss: 0.6376, Train Accuracy: 62.77%\n",
      "Epoch 1934/2000, Loss: 0.6376, Train Accuracy: 62.77%\n",
      "Epoch 1935/2000, Loss: 0.6376, Train Accuracy: 62.81%\n",
      "Epoch 1936/2000, Loss: 0.6376, Train Accuracy: 62.82%\n",
      "Epoch 1937/2000, Loss: 0.6376, Train Accuracy: 62.77%\n",
      "Epoch 1938/2000, Loss: 0.6376, Train Accuracy: 62.81%\n",
      "Epoch 1939/2000, Loss: 0.6376, Train Accuracy: 62.78%\n",
      "Epoch 1940/2000, Loss: 0.6376, Train Accuracy: 62.85%\n",
      "Epoch 1941/2000, Loss: 0.6376, Train Accuracy: 62.80%\n",
      "Epoch 1942/2000, Loss: 0.6377, Train Accuracy: 62.84%\n",
      "Epoch 1943/2000, Loss: 0.6377, Train Accuracy: 62.78%\n",
      "Epoch 1944/2000, Loss: 0.6377, Train Accuracy: 62.90%\n",
      "Epoch 1945/2000, Loss: 0.6377, Train Accuracy: 62.74%\n",
      "Epoch 1946/2000, Loss: 0.6377, Train Accuracy: 62.84%\n",
      "Epoch 1947/2000, Loss: 0.6377, Train Accuracy: 62.77%\n",
      "Epoch 1948/2000, Loss: 0.6378, Train Accuracy: 62.96%\n",
      "Epoch 1949/2000, Loss: 0.6378, Train Accuracy: 62.69%\n",
      "Epoch 1950/2000, Loss: 0.6378, Train Accuracy: 62.98%\n",
      "Epoch 1951/2000, Loss: 0.6377, Train Accuracy: 62.73%\n",
      "Epoch 1952/2000, Loss: 0.6377, Train Accuracy: 62.92%\n",
      "Epoch 1953/2000, Loss: 0.6377, Train Accuracy: 62.72%\n",
      "Epoch 1954/2000, Loss: 0.6376, Train Accuracy: 62.82%\n",
      "Epoch 1955/2000, Loss: 0.6376, Train Accuracy: 62.80%\n",
      "Epoch 1956/2000, Loss: 0.6375, Train Accuracy: 62.85%\n",
      "Epoch 1957/2000, Loss: 0.6375, Train Accuracy: 62.76%\n",
      "Epoch 1958/2000, Loss: 0.6375, Train Accuracy: 62.83%\n",
      "Epoch 1959/2000, Loss: 0.6375, Train Accuracy: 62.83%\n",
      "Epoch 1960/2000, Loss: 0.6375, Train Accuracy: 62.78%\n",
      "Epoch 1961/2000, Loss: 0.6375, Train Accuracy: 62.81%\n",
      "Epoch 1962/2000, Loss: 0.6375, Train Accuracy: 62.80%\n",
      "Epoch 1963/2000, Loss: 0.6375, Train Accuracy: 62.84%\n",
      "Epoch 1964/2000, Loss: 0.6376, Train Accuracy: 62.85%\n",
      "Epoch 1965/2000, Loss: 0.6376, Train Accuracy: 62.82%\n",
      "Epoch 1966/2000, Loss: 0.6376, Train Accuracy: 62.73%\n",
      "Epoch 1967/2000, Loss: 0.6376, Train Accuracy: 62.91%\n",
      "Epoch 1968/2000, Loss: 0.6376, Train Accuracy: 62.77%\n",
      "Epoch 1969/2000, Loss: 0.6376, Train Accuracy: 62.93%\n",
      "Epoch 1970/2000, Loss: 0.6376, Train Accuracy: 62.74%\n",
      "Epoch 1971/2000, Loss: 0.6376, Train Accuracy: 62.92%\n",
      "Epoch 1972/2000, Loss: 0.6376, Train Accuracy: 62.74%\n",
      "Epoch 1973/2000, Loss: 0.6376, Train Accuracy: 62.87%\n",
      "Epoch 1974/2000, Loss: 0.6375, Train Accuracy: 62.80%\n",
      "Epoch 1975/2000, Loss: 0.6375, Train Accuracy: 62.87%\n",
      "Epoch 1976/2000, Loss: 0.6374, Train Accuracy: 62.84%\n",
      "Epoch 1977/2000, Loss: 0.6374, Train Accuracy: 62.78%\n",
      "Epoch 1978/2000, Loss: 0.6374, Train Accuracy: 62.79%\n",
      "Epoch 1979/2000, Loss: 0.6374, Train Accuracy: 62.80%\n",
      "Epoch 1980/2000, Loss: 0.6374, Train Accuracy: 62.81%\n",
      "Epoch 1981/2000, Loss: 0.6374, Train Accuracy: 62.73%\n",
      "Epoch 1982/2000, Loss: 0.6374, Train Accuracy: 62.80%\n",
      "Epoch 1983/2000, Loss: 0.6374, Train Accuracy: 62.82%\n",
      "Epoch 1984/2000, Loss: 0.6374, Train Accuracy: 62.90%\n",
      "Epoch 1985/2000, Loss: 0.6374, Train Accuracy: 62.83%\n",
      "Epoch 1986/2000, Loss: 0.6375, Train Accuracy: 62.85%\n",
      "Epoch 1987/2000, Loss: 0.6375, Train Accuracy: 62.78%\n",
      "Epoch 1988/2000, Loss: 0.6375, Train Accuracy: 62.92%\n",
      "Epoch 1989/2000, Loss: 0.6375, Train Accuracy: 62.74%\n",
      "Epoch 1990/2000, Loss: 0.6376, Train Accuracy: 62.90%\n",
      "Epoch 1991/2000, Loss: 0.6376, Train Accuracy: 62.72%\n",
      "Epoch 1992/2000, Loss: 0.6376, Train Accuracy: 62.94%\n",
      "Epoch 1993/2000, Loss: 0.6375, Train Accuracy: 62.74%\n",
      "Epoch 1994/2000, Loss: 0.6375, Train Accuracy: 62.93%\n",
      "Epoch 1995/2000, Loss: 0.6375, Train Accuracy: 62.75%\n",
      "Epoch 1996/2000, Loss: 0.6374, Train Accuracy: 62.89%\n",
      "Epoch 1997/2000, Loss: 0.6374, Train Accuracy: 62.83%\n",
      "Epoch 1998/2000, Loss: 0.6373, Train Accuracy: 62.85%\n",
      "Epoch 1999/2000, Loss: 0.6373, Train Accuracy: 62.77%\n",
      "Epoch 2000/2000, Loss: 0.6373, Train Accuracy: 62.81%\n",
      "\n",
      "✅ Training completed.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Train the Model\n",
    "epochs = 2000\n",
    "print(\"\\n✅ Training Started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6445abc5-3e8f-479f-aaa1-cf9de04a2e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model saved as 'trojan_gnn_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Save Model\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model.pth'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5678193-4292-4655-8048-a88f68917eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Model Performance:\n",
      "✅ Accuracy: 60.30%\n",
      "✅ Precision: 60.93%\n",
      "✅ Recall: 57.21%\n",
      "✅ F1-score: 59.01%\n"
     ]
    }
   ],
   "source": [
    "# ✅ Evaluate the Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# ✅ Compute Metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "# ✅ Print Results\n",
    "print(\"\\n🎯 Model Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall: {recall:.2f}%\")\n",
    "print(f\"✅ F1-score: {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b1cdc35-c980-4813-9552-9779c552acaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Sample Predictions:\n",
      "Predicted: [0 0 1 0 0]\n",
      "Actual   : [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check sample predictions\n",
    "print(\"\\n✅ Sample Predictions:\")\n",
    "with torch.no_grad():\n",
    "    sample_output = model(data)\n",
    "    sample_pred = sample_output[:5].argmax(dim=1)\n",
    "\n",
    "print(\"Predicted:\", sample_pred.cpu().numpy())\n",
    "print(\"Actual   :\", data.y[:5].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d969e0-33c9-45c0-be5c-884349a81d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Loss: 0.0146 | Train Acc: 99.75%\n",
      "Epoch 20/100 | Loss: 0.0042 | Train Acc: 100.00%\n",
      "Epoch 30/100 | Loss: 0.0021 | Train Acc: 100.00%\n",
      "Epoch 40/100 | Loss: 0.0012 | Train Acc: 100.00%\n",
      "Epoch 50/100 | Loss: 0.0008 | Train Acc: 100.00%\n",
      "Epoch 60/100 | Loss: 0.0005 | Train Acc: 100.00%\n",
      "Epoch 70/100 | Loss: 0.0004 | Train Acc: 100.00%\n",
      "Epoch 80/100 | Loss: 0.0003 | Train Acc: 100.00%\n",
      "Epoch 90/100 | Loss: 0.0002 | Train Acc: 100.00%\n",
      "Epoch 100/100 | Loss: 0.0002 | Train Acc: 100.00%\n",
      "\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Generate Synthetic Time Series Data\n",
    "# ------------------------------\n",
    "# Let's assume we have T=500 time steps, each with D=3 features.\n",
    "T = 500\n",
    "D = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create some synthetic time series with a trend + noise\n",
    "time = np.arange(T)\n",
    "series = 0.5 * np.sin(0.1 * time) + 0.01 * time + 0.2 * np.random.randn(T)\n",
    "# We'll make this multivariate by stacking 2 more channels\n",
    "series_multi = np.stack([series,\n",
    "                         series + 0.1 * np.random.randn(T),\n",
    "                         0.5 * np.cos(0.1 * time) + 0.1 * np.random.randn(T)],\n",
    "                        axis=1)  # shape: (T, 3)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Create Rolling Window Features\n",
    "# ------------------------------\n",
    "def create_rolling_windows(data, window_size=10, step=1):\n",
    "    \"\"\"\n",
    "    data: shape (T, D)\n",
    "    window_size: length of each rolling window\n",
    "    step: stride between windows\n",
    "    Returns: X_rolled (num_windows, window_size*D), a flattened window approach\n",
    "    \"\"\"\n",
    "    X_rolled = []\n",
    "    for start_idx in range(0, len(data) - window_size + 1, step):\n",
    "        window = data[start_idx:start_idx + window_size]  # shape (window_size, D)\n",
    "        X_rolled.append(window.flatten())  # flatten to 1D\n",
    "    return np.array(X_rolled)\n",
    "\n",
    "WINDOW_SIZE = 10\n",
    "X_rolled = create_rolling_windows(series_multi, window_size=WINDOW_SIZE)\n",
    "num_windows = X_rolled.shape[0]  # T - window_size + 1\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Build Graph Edges\n",
    "# ------------------------------\n",
    "# (A) Temporal adjacency: connect each window i --> i+1\n",
    "# We'll create edges for consecutive windows.\n",
    "temporal_edges = []\n",
    "for i in range(num_windows - 1):\n",
    "    temporal_edges.append([i, i+1])  # forward edge\n",
    "    temporal_edges.append([i+1, i])  # backward edge (if you want undirected)\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, ?)\n",
    "\n",
    "# (B) KNN adjacency: connect each window to k similar windows based on feature space\n",
    "k_neighbors = 5  # can adjust\n",
    "knn_graph = kneighbors_graph(X_rolled, k_neighbors, mode='connectivity', include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, n_edges)\n",
    "\n",
    "# Combine edges (temporal + KNN)\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "\n",
    "# Convert to a PyTorch long tensor\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create Labels for Supervised Learning\n",
    "# ------------------------------\n",
    "# Suppose we want to classify each window as \"1\" if the *average* in that window > 0,\n",
    "# else \"0\". This is just a synthetic classification target.\n",
    "\n",
    "window_averages = X_rolled.mean(axis=1)\n",
    "labels = (window_averages > 0).astype(int)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Build PyTorch Geometric Data Object\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_rolled, dtype=torch.float32)  # Node features\n",
    "y_tensor = torch.tensor(labels, dtype=torch.long)        # Node labels\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Train/Test Split\n",
    "# ------------------------------\n",
    "# Let's do a simple random split of the windows\n",
    "train_mask = torch.rand(num_windows) < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Define GCN Model\n",
    "# ------------------------------\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN(input_dim=X_rolled.shape[1], hidden_dim=64, output_dim=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Training\n",
    "# ------------------------------\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)   # shape [num_windows, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Train Acc: {train_acc:.2%}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Evaluation\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "    test_acc = (test_pred == data.y[test_mask]).float().mean().item()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dbc9f47-6028-4be9-ba44-7937f68e71ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 15947\n",
      "✅ Testing samples: 4053\n",
      "✅ GNN model initialized.\n",
      "\n",
      "✅ Training Started...\n",
      "Epoch 100/2000, Loss: 0.6697, Train Accuracy: 59.65%\n",
      "Epoch 200/2000, Loss: 0.6480, Train Accuracy: 62.04%\n",
      "Epoch 300/2000, Loss: 0.6417, Train Accuracy: 62.85%\n",
      "Epoch 400/2000, Loss: 0.6382, Train Accuracy: 63.07%\n",
      "Epoch 500/2000, Loss: 0.6358, Train Accuracy: 63.62%\n",
      "Epoch 600/2000, Loss: 0.6335, Train Accuracy: 63.63%\n",
      "Epoch 700/2000, Loss: 0.6316, Train Accuracy: 63.97%\n",
      "Epoch 800/2000, Loss: 0.6299, Train Accuracy: 64.19%\n",
      "Epoch 900/2000, Loss: 0.6284, Train Accuracy: 64.36%\n",
      "Epoch 1000/2000, Loss: 0.6268, Train Accuracy: 64.34%\n",
      "Epoch 1100/2000, Loss: 0.6252, Train Accuracy: 64.65%\n",
      "Epoch 1200/2000, Loss: 0.6237, Train Accuracy: 64.87%\n",
      "Epoch 1300/2000, Loss: 0.6224, Train Accuracy: 65.13%\n",
      "Epoch 1400/2000, Loss: 0.6209, Train Accuracy: 65.22%\n",
      "Epoch 1500/2000, Loss: 0.6196, Train Accuracy: 65.30%\n",
      "Epoch 1600/2000, Loss: 0.6187, Train Accuracy: 65.38%\n",
      "Epoch 1700/2000, Loss: 0.6171, Train Accuracy: 65.47%\n",
      "Epoch 1800/2000, Loss: 0.6159, Train Accuracy: 65.75%\n",
      "Epoch 1900/2000, Loss: 0.6150, Train Accuracy: 65.79%\n",
      "Epoch 2000/2000, Loss: 0.6141, Train Accuracy: 65.96%\n",
      "\n",
      "✅ Training completed.\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Nodes:\n",
      "✅ Accuracy:  64.32%\n",
      "✅ Precision: 64.63%\n",
      "✅ Recall:    61.67%\n",
      "✅ F1-score:  63.11%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load the Dataset\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    exit(1)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Handle Missing Values\n",
    "# ------------------------------\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN values with column mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Optional: Drop rows with any remaining NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Separate Features & Labels\n",
    "# ------------------------------\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan (assuming binary)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Normalize Features\n",
    "# ------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Build the Graph Edges\n",
    "# ------------------------------\n",
    "# (A) Temporal adjacency: connect each time step i -> i+1 (and i+1 -> i for undirected)\n",
    "temporal_edges = []\n",
    "num_samples = len(X_scaled)\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])  # comment out if you prefer a directed chain\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples-1))\n",
    "\n",
    "# (B) KNN adjacency\n",
    "k_neighbors = 5  # Adjust based on dataset size\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, number_of_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Train/Test Split\n",
    "# ------------------------------\n",
    "# Randomly assign ~80% of nodes to training, 20% to testing\n",
    "mask = torch.rand(num_samples).to(device)\n",
    "train_mask = mask < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Define GNN Model\n",
    "# ------------------------------\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = GNN(input_dim=X.shape[1], hidden_dim=256, output_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"✅ GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Train the Model\n",
    "# ------------------------------\n",
    "epochs = 2000\n",
    "print(\"\\n✅ Training Started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)  # shape: [num_samples, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Save the Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12. Evaluate the Model\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# Compute Metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n🎯 Model Performance on Test Nodes:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f14e0d-863c-4ee9-9829-6a7c71288753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 16000\n",
      "✅ Testing samples: 4000\n",
      "✅ Deeper GNN model initialized.\n",
      "\n",
      "✅ Training started...\n",
      "Epoch 50/500, Loss: 0.6558, Train Accuracy: 62.51%\n",
      "Epoch 100/500, Loss: 0.6177, Train Accuracy: 66.84%\n",
      "Epoch 150/500, Loss: 0.5756, Train Accuracy: 70.78%\n",
      "Epoch 200/500, Loss: 0.5703, Train Accuracy: 70.96%\n",
      "Epoch 250/500, Loss: 0.5676, Train Accuracy: 71.15%\n",
      "Epoch 300/500, Loss: 0.5617, Train Accuracy: 71.55%\n",
      "Epoch 350/500, Loss: 0.5615, Train Accuracy: 71.51%\n",
      "Epoch 400/500, Loss: 0.5568, Train Accuracy: 72.14%\n",
      "Epoch 450/500, Loss: 0.5563, Train Accuracy: 71.96%\n",
      "Epoch 500/500, Loss: 0.5536, Train Accuracy: 72.46%\n",
      "\n",
      "✅ Training completed.\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model_deeper.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Set:\n",
      "✅ Accuracy:  10.62%\n",
      "✅ Precision: 100.00%\n",
      "✅ Recall:    10.62%\n",
      "✅ F1-score:  19.21%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load and Preprocess the Dataset\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    raise FileNotFoundError(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN values with mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# (Optional) Drop any rows still containing NaN\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Ensure the 'label' column exists\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan (assuming binary classification)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Build Graph Edges\n",
    "# ------------------------------\n",
    "num_samples = len(X_scaled)\n",
    "\n",
    "# (A) Temporal edges: connect i -> i+1 (and i+1 -> i to make it undirected)\n",
    "temporal_edges = []\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape: (2, 2*(num_samples - 1))\n",
    "\n",
    "# (B) KNN edges: connect each sample to its k nearest neighbors in feature space\n",
    "k_neighbors = 5  # Adjust based on dataset size/needs\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape: (2, num_knn_edges)\n",
    "\n",
    "# Combine temporal and KNN edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Chronological Split (Train = first 80%, Test = last 20%)\n",
    "# ------------------------------\n",
    "split_point = int(0.8 * num_samples)  # 80% cutoff\n",
    "# Create boolean masks for each node (row)\n",
    "train_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "train_mask[:split_point] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Define a Deeper GNN Model (3 GCN Layers + Dropout)\n",
    "# ------------------------------\n",
    "class DeeperGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, dropout_p=0.2):\n",
    "        super(DeeperGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer (logits)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = DeeperGNN(input_dim=X.shape[1], hidden_dim=128, output_dim=2, dropout_p=0.3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✅ Deeper GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Train the Model\n",
    "# ------------------------------\n",
    "epochs = 500\n",
    "print(\"\\n✅ Training started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)  # shape: [num_samples, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:  # Print every 50 epochs\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Save the Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model_deeper.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model_deeper.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Evaluate the Model\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# Compute Metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "print(\"\\n🎯 Model Performance on Test Set:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c242e2-6362-4f4f-a7f8-a9bf050b0628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 16000\n",
      "✅ Testing samples: 4000\n",
      "✅ Deeper GNN model initialized.\n",
      "\n",
      "✅ Training started...\n",
      "Epoch 50/500, Loss: 0.6546, Train Accuracy: 62.50%\n",
      "Epoch 100/500, Loss: 0.6088, Train Accuracy: 68.07%\n",
      "Epoch 150/500, Loss: 0.5743, Train Accuracy: 70.78%\n",
      "Epoch 200/500, Loss: 0.5671, Train Accuracy: 71.19%\n",
      "Epoch 250/500, Loss: 0.5657, Train Accuracy: 71.17%\n",
      "Epoch 300/500, Loss: 0.5622, Train Accuracy: 71.57%\n",
      "Epoch 350/500, Loss: 0.5597, Train Accuracy: 71.93%\n",
      "Epoch 400/500, Loss: 0.5563, Train Accuracy: 72.18%\n",
      "Epoch 450/500, Loss: 0.5555, Train Accuracy: 72.15%\n",
      "Epoch 500/500, Loss: 0.5549, Train Accuracy: 72.24%\n",
      "\n",
      "✅ Training completed.\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model_deeper.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Set:\n",
      "✅ Accuracy:  13.50%\n",
      "✅ Precision: 100.00%\n",
      "✅ Recall:    13.50%\n",
      "✅ F1-score:  23.79%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load & Clean the CSV\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    raise FileNotFoundError(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN with column mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# (Optional) Drop rows with any remaining NaN\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Separate Features (X) and Labels (y)\n",
    "# ------------------------------\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # e.g. 0 = Normal, 1 = Trojan\n",
    "\n",
    "# Normalize Features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "num_samples = len(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Build Graph Edges: Temporal + KNN\n",
    "# ------------------------------\n",
    "# A) Temporal Edges: connect each row i->i+1 and i+1->i\n",
    "temporal_edges = []\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])  # Undirected\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples - 1))\n",
    "\n",
    "# B) KNN Edges\n",
    "k_neighbors = 5\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, num_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Chronological Split (80% Train, 20% Test)\n",
    "# ------------------------------\n",
    "split_point = int(0.8 * num_samples)  # 80% cutoff\n",
    "train_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "train_mask[:split_point] = True\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Define a Deeper GNN (3 GCN Layers + Dropout)\n",
    "# ------------------------------\n",
    "class DeeperGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, dropout_p=0.3):\n",
    "        super(DeeperGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer (logits)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = DeeperGNN(input_dim=X.shape[1], hidden_dim=128, output_dim=2, dropout_p=0.3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✅ Deeper GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Train the Model\n",
    "# ------------------------------\n",
    "epochs = 500\n",
    "print(\"\\n✅ Training started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)  # shape: [num_samples, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    with torch.no_grad():\n",
    "        train_pred = out[train_mask].argmax(dim=1)\n",
    "        train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Save the Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model_deeper.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model_deeper.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Evaluate the Model\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# Convert to NumPy for metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "print(\"\\n🎯 Model Performance on Test Set:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8921185a-9cd0-40aa-9382-4c3a53df4586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 14000\n",
      "✅ Validation samples: 2000\n",
      "✅ Testing samples: 4000\n",
      "Class Distribution -> Normal: 10000, Trojan: 10000, Ratio: 1.00\n",
      "\n",
      "✅ Deeper GNN model initialized with class weighting and Adam optimizer.\n",
      "\n",
      "✅ Training started...\n",
      "Epoch 50/500, Train Loss: 0.6013, Val Loss: 1.2586, Train Acc: 71.43%\n",
      "\n",
      "Early stopping at epoch 51, no improvement since epoch 1.\n",
      "\n",
      "✅ Training completed, best model from epoch: 1\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model_deeper.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Set:\n",
      "✅ Accuracy:  0.00%\n",
      "✅ Precision: 0.00%\n",
      "✅ Recall:    0.00%\n",
      "✅ F1-score:  0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load & Clean the CSV\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    raise FileNotFoundError(file_path)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# Basic missing-value handling\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.dropna(inplace=True)  # optional final drop of any row with remaining NaN\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Separate Features (X) & Labels (y)\n",
    "# ------------------------------\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan (assuming binary classification)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "num_samples = len(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Build Graph Edges: Temporal + KNN\n",
    "# ------------------------------\n",
    "# (A) Temporal edges: connect each row i->i+1 and i+1->i\n",
    "temporal_edges = []\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])  # Undirected\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples - 1))\n",
    "\n",
    "# (B) KNN edges\n",
    "k_neighbors = 5\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, num_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Chronological Split\n",
    "#     - Train: first 70%\n",
    "#     - Val: next 10%\n",
    "#     - Test: final 20%\n",
    "# ------------------------------\n",
    "train_cut = int(0.7 * num_samples)\n",
    "val_cut = int(0.8 * num_samples)\n",
    "\n",
    "train_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "val_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "test_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "\n",
    "train_mask[:train_cut] = True\n",
    "val_mask[train_cut:val_cut] = True\n",
    "test_mask[val_cut:] = True\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Validation samples: {val_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Define a Deeper GNN (3 GCN Layers + Dropout)\n",
    "# ------------------------------\n",
    "class DeeperGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, dropout_p=0.3):\n",
    "        super(DeeperGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer (logits)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = DeeperGNN(input_dim=X.shape[1], hidden_dim=128, output_dim=2, dropout_p=0.3).to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# 8. (Optional) Weighted Loss for Class Imbalance\n",
    "#    If Trojan is rare, try weighting the classes.\n",
    "#    For example, weight= [1.0, 5.0] if Trojan is 5x less common than Normal.\n",
    "# ------------------------------\n",
    "# Let's do a quick check of class distribution:\n",
    "trojan_count = (y == 1).sum()\n",
    "normal_count = (y == 0).sum()\n",
    "ratio = normal_count / float(trojan_count) if trojan_count != 0 else 1.0\n",
    "print(f\"Class Distribution -> Normal: {normal_count}, Trojan: {trojan_count}, Ratio: {ratio:.2f}\")\n",
    "\n",
    "# Adjust the weighting factor as needed:\n",
    "weight_tensor = torch.tensor([1.0, max(1.0, ratio)], dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "# If you're sure you don't want weighting, comment above lines and do:\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "print(\"\\n✅ Deeper GNN model initialized with class weighting and Adam optimizer.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Train the Model (with Early Validation)\n",
    "# ------------------------------\n",
    "epochs = 500\n",
    "print(\"\\n✅ Training started...\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "patience = 50  # If val_loss doesn't improve for 50 epochs, stop early\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # ---- Train Step ----\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss_train = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ---- Validation Step ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_val = model(data)\n",
    "        loss_val = criterion(out_val[val_mask], data.y[val_mask])\n",
    "    \n",
    "    # Print status\n",
    "    if epoch % 50 == 0:\n",
    "        train_pred = out[train_mask].argmax(dim=1)\n",
    "        train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "        print(f\"Epoch {epoch}/{epochs}, \"\n",
    "              f\"Train Loss: {loss_train.item():.4f}, \"\n",
    "              f\"Val Loss: {loss_val.item():.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2%}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if loss_val.item() < best_val_loss:\n",
    "        best_val_loss = loss_val.item()\n",
    "        best_epoch = epoch\n",
    "        best_state_dict = model.state_dict()  # Save model weights\n",
    "    else:\n",
    "        # If no improvement for 'patience' epochs, stop\n",
    "        if epoch - best_epoch >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}, \"\n",
    "                  f\"no improvement since epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(best_state_dict)\n",
    "print(\"\\n✅ Training completed, best model from epoch:\", best_epoch)\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Save the Best Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model_deeper.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model_deeper.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Evaluate on Test Set\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_test = model(data)\n",
    "    test_pred = out_test[test_mask].argmax(dim=1)\n",
    "\n",
    "# Convert to NumPy for metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "print(\"\\n🎯 Model Performance on Test Set:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec61c199-2f39-4890-ab99-51d4339027e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 14000\n",
      "✅ Validation samples: 2000\n",
      "✅ Testing samples: 4000\n",
      "Class Distribution -> Normal: 10000, Trojan: 10000\n",
      "\n",
      "✅ Deeper GNN model initialized.\n",
      "\n",
      "✅ Training started...\n",
      "Epoch 20/300, Train Loss: 0.6001, Val Loss: 1.1626, Train Acc: 71.43%, Val Acc: 0.00%\n",
      "Epoch 40/300, Train Loss: 0.5925, Val Loss: 1.2424, Train Acc: 71.43%, Val Acc: 0.00%\n",
      "Epoch 60/300, Train Loss: 0.5818, Val Loss: 1.2212, Train Acc: 71.43%, Val Acc: 0.05%\n",
      "Epoch 80/300, Train Loss: 0.5686, Val Loss: 1.2081, Train Acc: 72.37%, Val Acc: 3.30%\n",
      "Epoch 100/300, Train Loss: 0.5524, Val Loss: 1.1404, Train Acc: 73.31%, Val Acc: 11.60%\n",
      "Epoch 120/300, Train Loss: 0.5334, Val Loss: 1.0911, Train Acc: 74.27%, Val Acc: 17.15%\n",
      "Epoch 140/300, Train Loss: 0.5198, Val Loss: 1.0503, Train Acc: 74.99%, Val Acc: 24.65%\n",
      "Epoch 160/300, Train Loss: 0.5137, Val Loss: 1.0753, Train Acc: 75.24%, Val Acc: 28.10%\n",
      "Epoch 180/300, Train Loss: 0.5096, Val Loss: 1.1014, Train Acc: 75.54%, Val Acc: 29.20%\n",
      "Epoch 200/300, Train Loss: 0.5082, Val Loss: 1.0857, Train Acc: 75.64%, Val Acc: 30.05%\n",
      "Epoch 220/300, Train Loss: 0.5074, Val Loss: 1.0320, Train Acc: 75.73%, Val Acc: 32.65%\n",
      "Epoch 240/300, Train Loss: 0.5050, Val Loss: 1.0332, Train Acc: 75.69%, Val Acc: 32.85%\n",
      "Epoch 260/300, Train Loss: 0.5039, Val Loss: 1.0946, Train Acc: 75.79%, Val Acc: 30.90%\n",
      "Epoch 280/300, Train Loss: 0.5021, Val Loss: 1.0396, Train Acc: 75.89%, Val Acc: 32.95%\n",
      "Epoch 300/300, Train Loss: 0.5021, Val Loss: 1.0263, Train Acc: 75.82%, Val Acc: 33.90%\n",
      "\n",
      "✅ Training completed. Best validation accuracy = 36.75% at epoch 292.\n",
      "\n",
      "✅ Best model saved as 'trojan_gnn_model_deeper.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Set (Best Model):\n",
      "✅ Accuracy:  5.30%\n",
      "✅ Precision: 100.00%\n",
      "✅ Recall:    5.30%\n",
      "✅ F1-score:  10.07%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load & Clean the CSV\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"❌ ERROR: File '{file_path}' not found!\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# Basic missing-value handling\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.dropna(inplace=True)  # optional final drop of any row with remaining NaN\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Separate Features (X) & Labels (y)\n",
    "# ------------------------------\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "num_samples = len(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Build Graph Edges: Temporal + KNN\n",
    "# ------------------------------\n",
    "# (A) Temporal edges (undirected): connect i->i+1 and i+1->i\n",
    "temporal_edges = []\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples - 1))\n",
    "\n",
    "# (B) KNN edges (smaller k to avoid over-connection)\n",
    "k_neighbors = 3\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape: (2, num_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Chronological Split (70% / 10% / 20%)\n",
    "# ------------------------------\n",
    "train_cut = int(0.7 * num_samples)\n",
    "val_cut = int(0.8 * num_samples)\n",
    "\n",
    "train_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "val_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "test_mask = torch.zeros(num_samples, dtype=torch.bool).to(device)\n",
    "\n",
    "train_mask[:train_cut] = True\n",
    "val_mask[train_cut:val_cut] = True\n",
    "test_mask[val_cut:] = True\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Validation samples: {val_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# Optional: check distribution\n",
    "trojan_count = (y == 1).sum()\n",
    "normal_count = (y == 0).sum()\n",
    "print(f\"Class Distribution -> Normal: {normal_count}, Trojan: {trojan_count}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Define a Deeper GNN (3 GCN Layers + Smaller Dropout)\n",
    "# ------------------------------\n",
    "class DeeperGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2, dropout_p=0.1):\n",
    "        super(DeeperGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Third layer (logits)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = DeeperGNN(input_dim=X.shape[1], hidden_dim=128, output_dim=2, dropout_p=0.1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()  # Balanced dataset? Then no weighting needed\n",
    "\n",
    "print(\"\\n✅ Deeper GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Train the Model (No Hard Early Stopping)\n",
    "#    We'll do a fixed number of epochs (e.g., 300)\n",
    "#    and track \"best model\" based on validation accuracy\n",
    "# ------------------------------\n",
    "epochs = 300\n",
    "print(\"\\n✅ Training started...\")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---- Train Step ----\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)\n",
    "    loss_train = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ---- Validation Step ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_val = model(data)[val_mask]\n",
    "        val_labels = data.y[val_mask]\n",
    "        loss_val = criterion(out_val, val_labels)\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        val_pred = out_val.argmax(dim=1)\n",
    "        val_acc = (val_pred == val_labels).float().mean().item()\n",
    "\n",
    "    # Print progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        # Train accuracy (optional to check)\n",
    "        with torch.no_grad():\n",
    "            train_pred = out[train_mask].argmax(dim=1)\n",
    "            train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "        print(f\"Epoch {epoch}/{epochs}, \"\n",
    "              f\"Train Loss: {loss_train.item():.4f}, \"\n",
    "              f\"Val Loss: {loss_val.item():.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2%}, \"\n",
    "              f\"Val Acc: {val_acc:.2%}\")\n",
    "\n",
    "    # Save best model (based on val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(f\"\\n✅ Training completed. Best validation accuracy = {best_val_acc:.2%} at epoch {best_epoch}.\")\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Save the Best Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model_deeper.pth\")\n",
    "print(\"\\n✅ Best model saved as 'trojan_gnn_model_deeper.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Evaluate on Test Set\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out_test = model(data)[test_mask]\n",
    "    test_pred = out_test.argmax(dim=1)\n",
    "\n",
    "# Convert to NumPy for metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "print(\"\\n🎯 Model Performance on Test Set (Best Model):\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03628b3c-2d69-4c00-a64c-413b6f033eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 16002\n",
      "✅ Testing samples: 3998\n",
      "✅ GNN model initialized.\n",
      "\n",
      "✅ Training Started...\n",
      "Epoch 100/2000, Loss: 0.6779, Train Accuracy: 57.98%\n",
      "Epoch 200/2000, Loss: 0.6586, Train Accuracy: 61.04%\n",
      "Epoch 300/2000, Loss: 0.6471, Train Accuracy: 62.04%\n",
      "Epoch 400/2000, Loss: 0.6423, Train Accuracy: 62.59%\n",
      "Epoch 500/2000, Loss: 0.6389, Train Accuracy: 62.88%\n",
      "Epoch 600/2000, Loss: 0.6360, Train Accuracy: 63.32%\n",
      "Epoch 700/2000, Loss: 0.6335, Train Accuracy: 63.81%\n",
      "Epoch 800/2000, Loss: 0.6313, Train Accuracy: 64.09%\n",
      "Epoch 900/2000, Loss: 0.6295, Train Accuracy: 64.11%\n",
      "Epoch 1000/2000, Loss: 0.6279, Train Accuracy: 64.37%\n",
      "Epoch 1100/2000, Loss: 0.6262, Train Accuracy: 64.46%\n",
      "Epoch 1200/2000, Loss: 0.6249, Train Accuracy: 64.61%\n",
      "Epoch 1300/2000, Loss: 0.6237, Train Accuracy: 64.79%\n",
      "Epoch 1400/2000, Loss: 0.6225, Train Accuracy: 64.85%\n",
      "Epoch 1500/2000, Loss: 0.6215, Train Accuracy: 64.82%\n",
      "Epoch 1600/2000, Loss: 0.6206, Train Accuracy: 65.11%\n",
      "Epoch 1700/2000, Loss: 0.6198, Train Accuracy: 65.19%\n",
      "Epoch 1800/2000, Loss: 0.6193, Train Accuracy: 65.15%\n",
      "Epoch 1900/2000, Loss: 0.6185, Train Accuracy: 65.45%\n",
      "Epoch 2000/2000, Loss: 0.6178, Train Accuracy: 65.60%\n",
      "\n",
      "✅ Training completed.\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Nodes:\n",
      "✅ Accuracy:  65.66%\n",
      "✅ Precision: 67.57%\n",
      "✅ Recall:    62.62%\n",
      "✅ F1-score:  65.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load the Dataset\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    exit(1)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Handle Missing Values\n",
    "# ------------------------------\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN values with column mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Optional: Drop rows with any remaining NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Separate Features & Labels\n",
    "# ------------------------------\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan (assuming binary)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Normalize Features\n",
    "# ------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Build the Graph Edges\n",
    "# ------------------------------\n",
    "# (A) Temporal adjacency: connect each time step i -> i+1 (and i+1 -> i for undirected)\n",
    "temporal_edges = []\n",
    "num_samples = len(X_scaled)\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])  # comment out if you prefer a directed chain\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples-1))\n",
    "\n",
    "# (B) KNN adjacency\n",
    "k_neighbors = 5  # Adjust based on dataset size\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, number_of_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Train/Test Split\n",
    "# ------------------------------\n",
    "# Randomly assign ~80% of nodes to training, 20% to testing\n",
    "mask = torch.rand(num_samples).to(device)\n",
    "train_mask = mask < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Define GNN Model\n",
    "# ------------------------------\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = GNN(input_dim=X.shape[1], hidden_dim=256, output_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"✅ GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Train the Model\n",
    "# ------------------------------\n",
    "epochs = 2000\n",
    "print(\"\\n✅ Training Started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)  # shape: [num_samples, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Save the Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12. Evaluate the Model\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# Compute Metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n🎯 Model Performance on Test Nodes:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888ac1ee-0a3c-4dce-ae04-e46fb7506f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "\n",
      "✅ Training Isolation Forest...\n",
      "\n",
      "🎯 Isolation Forest Performance:\n",
      "✅ Accuracy:  53.50%\n",
      "✅ Precision: 67.50%\n",
      "✅ Recall:    13.50%\n",
      "✅ F1-score:  22.50%\n",
      "\n",
      "📊 Confusion Matrix:\n",
      "[[9350  650]\n",
      " [8650 1350]]\n",
      "\n",
      "📝 Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Clean       0.52      0.94      0.67     10000\n",
      "      Trojan       0.68      0.14      0.23     10000\n",
      "\n",
      "    accuracy                           0.54     20000\n",
      "   macro avg       0.60      0.54      0.45     20000\n",
      "weighted avg       0.60      0.54      0.45     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and Validate Dataset\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    exit(1)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Handle Missing Values\n",
    "# ------------------------------\n",
    "print(\"Missing values before handling:\\n\", df.isnull().sum())\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"Missing values after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Separate Features and Labels\n",
    "# ------------------------------\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Clean, 1 = Trojan\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Normalize Features\n",
    "# ------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Isolation Forest Model\n",
    "# ------------------------------\n",
    "print(\"\\n✅ Training Isolation Forest...\")\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "iso_forest.fit(X_scaled)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Predict and Evaluate\n",
    "# ------------------------------\n",
    "y_pred = iso_forest.predict(X_scaled)\n",
    "# Isolation Forest output: -1 = anomaly → Trojan (1), 1 = normal → Clean (0)\n",
    "y_pred_mapped = np.array([1 if val == -1 else 0 for val in y_pred])\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y, y_pred_mapped) * 100\n",
    "precision = precision_score(y, y_pred_mapped, zero_division=0) * 100\n",
    "recall = recall_score(y, y_pred_mapped, zero_division=0) * 100\n",
    "f1 = f1_score(y, y_pred_mapped, zero_division=0) * 100\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Print Results\n",
    "# ------------------------------\n",
    "print(\"\\n🎯 Isolation Forest Performance:\")\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n",
    "\n",
    "print(\"\\n📊 Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred_mapped))\n",
    "\n",
    "print(\"\\n📝 Detailed Classification Report:\")\n",
    "print(classification_report(y, y_pred_mapped, target_names=[\"Clean\", \"Trojan\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b86c69b-10ba-4aef-99e4-b303b35e2b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Using device: cuda\n",
      "Missing values per column before handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     1\n",
      "dtype: int64\n",
      "Missing values per column after handling:\n",
      " label                 0\n",
      "Mean                  0\n",
      "RMS                   0\n",
      "Variance              0\n",
      "Standard Deviation    0\n",
      "Peak-to-Peak          0\n",
      "Crest Factor          0\n",
      "Skewness              0\n",
      "Kurtosis              0\n",
      "Energy                0\n",
      "Entropy               0\n",
      "Max                   0\n",
      "Min                   0\n",
      "peak_magnitude        0\n",
      "spectral_centroid     0\n",
      "spectral_bandwidth    0\n",
      "spectral_flatness     0\n",
      "spectral_rolloff      0\n",
      "spectral_entropy      0\n",
      "spectral_contrast     0\n",
      "dtype: int64\n",
      "✅ Features normalized.\n",
      "✅ Graph dataset created with temporal + KNN edges.\n",
      "\n",
      "✅ Training samples: 15986\n",
      "✅ Testing samples: 4014\n",
      "✅ GNN model initialized.\n",
      "\n",
      "✅ Training Started...\n",
      "Epoch 100/2000, Loss: 0.6690, Train Accuracy: 59.64%\n",
      "Epoch 200/2000, Loss: 0.6488, Train Accuracy: 62.15%\n",
      "Epoch 300/2000, Loss: 0.6424, Train Accuracy: 62.67%\n",
      "Epoch 400/2000, Loss: 0.6388, Train Accuracy: 62.91%\n",
      "Epoch 500/2000, Loss: 0.6357, Train Accuracy: 63.32%\n",
      "Epoch 600/2000, Loss: 0.6331, Train Accuracy: 63.74%\n",
      "Epoch 700/2000, Loss: 0.6310, Train Accuracy: 63.91%\n",
      "Epoch 800/2000, Loss: 0.6288, Train Accuracy: 64.07%\n",
      "Epoch 900/2000, Loss: 0.6271, Train Accuracy: 64.45%\n",
      "Epoch 1000/2000, Loss: 0.6251, Train Accuracy: 64.55%\n",
      "Epoch 1100/2000, Loss: 0.6237, Train Accuracy: 64.71%\n",
      "Epoch 1200/2000, Loss: 0.6224, Train Accuracy: 64.70%\n",
      "Epoch 1300/2000, Loss: 0.6214, Train Accuracy: 65.03%\n",
      "Epoch 1400/2000, Loss: 0.6204, Train Accuracy: 65.14%\n",
      "Epoch 1500/2000, Loss: 0.6195, Train Accuracy: 65.23%\n",
      "Epoch 1600/2000, Loss: 0.6188, Train Accuracy: 65.33%\n",
      "Epoch 1700/2000, Loss: 0.6187, Train Accuracy: 65.38%\n",
      "Epoch 1800/2000, Loss: 0.6179, Train Accuracy: 65.46%\n",
      "Epoch 1900/2000, Loss: 0.6170, Train Accuracy: 65.50%\n",
      "Epoch 2000/2000, Loss: 0.6164, Train Accuracy: 65.60%\n",
      "\n",
      "✅ Training completed.\n",
      "\n",
      "✅ Model saved as 'trojan_gnn_model.pth'.\n",
      "\n",
      "🎯 Model Performance on Test Nodes:\n",
      "✅ AUC:       71.58%\n",
      "✅ Accuracy:  65.10%\n",
      "✅ Precision: 66.28%\n",
      "✅ Recall:    62.38%\n",
      "✅ F1-score:  64.27%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Check for GPU\n",
    "# ------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n✅ Using device: {device}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Load the Dataset\n",
    "# ------------------------------\n",
    "file_path = \"IL_T600_cleaned.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"❌ ERROR: File '{file_path}' not found! Place it in the same directory.\")\n",
    "    exit(1)\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rename 'Label' to 'label' if needed\n",
    "df.rename(columns={'Label': 'label'}, inplace=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Handle Missing Values\n",
    "# ------------------------------\n",
    "print(\"Missing values per column before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill NaN values with column mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Optional: Drop rows with any remaining NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Missing values per column after handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Separate Features & Labels\n",
    "# ------------------------------\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"❌ 'label' column is missing from the CSV. Please check your data.\")\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].values  # 0 = Normal, 1 = Trojan (assuming binary)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Normalize Features\n",
    "# ------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"✅ Features normalized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Build the Graph Edges\n",
    "# ------------------------------\n",
    "# (A) Temporal adjacency: connect each time step i -> i+1 (and i+1 -> i for undirected)\n",
    "temporal_edges = []\n",
    "num_samples = len(X_scaled)\n",
    "for i in range(num_samples - 1):\n",
    "    temporal_edges.append([i, i+1])\n",
    "    temporal_edges.append([i+1, i])  # comment out if you prefer a directed chain\n",
    "\n",
    "temporal_edges = np.array(temporal_edges).T  # shape (2, 2*(num_samples-1))\n",
    "\n",
    "# (B) KNN adjacency\n",
    "k_neighbors = 5  # Adjust based on dataset size\n",
    "knn_graph = kneighbors_graph(X_scaled, k_neighbors, mode=\"connectivity\", include_self=False)\n",
    "knn_coo = knn_graph.tocoo()\n",
    "knn_edges = np.vstack([knn_coo.row, knn_coo.col])  # shape (2, number_of_knn_edges)\n",
    "\n",
    "# Combine edges\n",
    "all_edges = np.concatenate([temporal_edges, knn_edges], axis=1)\n",
    "edge_index = torch.tensor(all_edges, dtype=torch.long)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Create PyTorch Geometric Data\n",
    "# ------------------------------\n",
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(\"✅ Graph dataset created with temporal + KNN edges.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Train/Test Split\n",
    "# ------------------------------\n",
    "# Randomly assign ~80% of nodes to training, 20% to testing\n",
    "mask = torch.rand(num_samples).to(device)\n",
    "train_mask = mask < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"\\n✅ Training samples: {train_mask.sum().item()}\")\n",
    "print(f\"✅ Testing samples: {test_mask.sum().item()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Define GNN Model\n",
    "# ------------------------------\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = GNN(input_dim=X.shape[1], hidden_dim=256, output_dim=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"✅ GNN model initialized.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Train the Model\n",
    "# ------------------------------\n",
    "epochs = 2000\n",
    "print(\"\\n✅ Training Started...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data)  # shape: [num_samples, 2]\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_pred = out[train_mask].argmax(dim=1)\n",
    "    train_acc = (train_pred == data.y[train_mask]).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.2%}\")\n",
    "\n",
    "print(\"\\n✅ Training completed.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Save the Model\n",
    "# ------------------------------\n",
    "torch.save(model.state_dict(), \"trojan_gnn_model.pth\")\n",
    "print(\"\\n✅ Model saved as 'trojan_gnn_model.pth'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 12. Evaluate the Model\n",
    "# ------------------------------\n",
    "from sklearn.metrics import roc_auc_score               # <-- add this import\n",
    "\n",
    "# ------------------------------\n",
    "# 12. Evaluate the Model\n",
    "# ------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)                                   # logits, shape [N, 2]\n",
    "\n",
    "    # ─── NEW: probability that each node is Trojan (class-1) ───\n",
    "    y_score = torch.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    test_pred = out[test_mask].argmax(dim=1)\n",
    "\n",
    "# Compute Metrics\n",
    "y_true = data.y[test_mask].cpu().numpy()\n",
    "y_pred = test_pred.cpu().numpy()\n",
    "\n",
    "auc      = roc_auc_score(y_true, y_score) * 100         # <-- NEW\n",
    "accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
    "recall    = recall_score(y_true, y_pred, zero_division=0) * 100\n",
    "f1        = f1_score(y_true, y_pred, zero_division=0) * 100\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n🎯 Model Performance on Test Nodes:\")\n",
    "print(f\"✅ AUC:       {auc:.2f}%\")                      # <-- NEW\n",
    "print(f\"✅ Accuracy:  {accuracy:.2f}%\")\n",
    "print(f\"✅ Precision: {precision:.2f}%\")\n",
    "print(f\"✅ Recall:    {recall:.2f}%\")\n",
    "print(f\"✅ F1-score:  {f1:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2d942-c262-4e04-a228-6dcae7fd4a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
